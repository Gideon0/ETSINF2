Apuntes de Estad√≠stica
para Ingenieros
Versi√≥n 1.3, junio de 2012

Prof. Dr. Antonio Jos√© S√°ez Castillo
Dpto de Estad√≠stica e Investigaci√≥n Operativa
Universidad de Ja√©n

Esta obra est√° bajo una licencia Reconocimiento-No
comercial-Sin obras derivadas 3.0 Espa√±a de Creative
Commons. Para ver una copia de esta licencia, visite
http://creativecommons.org/licenses/by-nc-nd/3.0/es/
o
envie una carta a Creative Commons, 171 Second Street,
Suite 300, San Francisco, California 94105, USA.

Apuntes de
Estad√≠stica para Ingenieros
Prof. Dr. Antonio Jos√© S√°ez Castillo
Departamento de Estad√≠stica e Investigaci√≥n Operativa
Universidad de Ja√©n

Versi√≥n 1.3
Junio de 2012

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

2

Prof. Dr. Antonio Jos√© S√°ez Castillo

√çndice general

1. Introducci√≥n

11

1.1. ¬æQu√© signica Estad√≠stica? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.2. La Estad√≠stica en el √°mbito de la Ciencia y la Ingenier√≠a . . . . . . . . . . . . . . . . . . . . .

12

1.2.1. Ejemplo de las capas de √≥xido de silicio . . . . . . . . . . . . . . . . . . . . . . . . . .

12

1.2.2. Ejemplo de la bombilla de bajo consumo . . . . . . . . . . . . . . . . . . . . . . . . . .

12

1.2.3. Ejemplo de los niveles de plomo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

1.2.4. Ejemplo de los cojinetes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

1.2.5. Ejemplo de la absorci√≥n de un compuesto a distintas dosis y en distintos tiempos de
absorci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

1.2.6. Ejemplo de los accidentes laborales . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

1.2.7. Ejemplo de la cobertura de la antena de telefon√≠a m√≥vil . . . . . . . . . . . . . . . . .

15

1.2.8. Ejemplo de la se√±al aleatoria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

1.3. Deniciones b√°sicas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

I Estad√≠stica descriptiva

17

2. El tratamiento de los datos. Estad√≠stica descriptiva

19

2.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

2.2. Tipos de datos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

2.3. M√©todos gr√°cos y num√©ricos para describir datos cualitativos . . . . . . . . . . . . . . . . . .

20

2.4. M√©todos gr√°cos para describir datos cuantitativos . . . . . . . . . . . . . . . . . . . . . . . .

21

2.5. M√©todos num√©ricos para describir datos cuantitativos

. . . . . . . . . . . . . . . . . . . . . .

25

2.5.1. Medidas de tendencia central . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

2.5.1.1. Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

2.5.1.2. Mediana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

2.5.1.3. Moda o intervalo modal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

2.5.2. Cuantiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

2.5.3. Medidas de variaci√≥n o dispersi√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

3

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

2.5.3.1. Varianza muestral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

2.5.3.2. Desviaci√≥n t√≠pica o estandar muestral . . . . . . . . . . . . . . . . . . . . . .

29

2.5.3.3. Coeciente de variaci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

2.5.4. Medidas de forma. Coeciente de asimetr√≠a . . . . . . . . . . . . . . . . . . . . . . . .

31

2.5.5. Par√°metros muestrales y par√°metros poblacionales . . . . . . . . . . . . . . . . . . . .

32

2.6. M√©todos para detectar datos cuantitativos at√≠picos o fuera de rango . . . . . . . . . . . . . .

33

2.6.1. Mediante la regla emp√≠rica

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

2.6.2. Mediante los percentiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

2.7. Sobre el ejemplo de las capas de di√≥xido de silicio . . . . . . . . . . . . . . . . . . . . . . . . .

34

II C√°lculo de Probabilidades

37

3. Probabilidad

39

3.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

3.2. Experimentos aleatorios y experimentos determin√≠sticos . . . . . . . . . . . . . . . . . . . . .

40

3.3. Denici√≥n de probabilidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

3.3.1. √Ålgebra de conjuntos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

3.3.2. Espacio muestral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

3.3.3. Funci√≥n de probabilidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

3.4. Interpretaci√≥n frecuentista de la probabilidad . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

3.5. Interpretaci√≥n subjetiva de la probabilidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

3.6. Espacio muestral con resultados equiprobables. F√≥rmula de Laplace . . . . . . . . . . . . . .

46

3.7. Probabilidad condicionada. Independencia de sucesos . . . . . . . . . . . . . . . . . . . . . . .

46

3.8. Teorema de la probabilidad total y Teorema de Bayes . . . . . . . . . . . . . . . . . . . . . .

51

3.9. M√°s sobre el Teorema de Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

3.9.1. Ejemplo del juez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

3.9.2. Ejemplo de la m√°quina de detecci√≥n de fallos . . . . . . . . . . . . . . . . . . . . . . .

57

4. Variable aleatoria. Modelos de distribuciones de probabilidad

4

61

4.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

61

4.2. Variable aleatoria discreta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62

4.2.1. Denici√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62

4.2.2. Funci√≥n masa de probabilidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62

4.2.3. Funci√≥n masa de probabilidad emp√≠rica . . . . . . . . . . . . . . . . . . . . . . . . . .

63

4.2.4. Media y varianza de una variable aleatoria discreta . . . . . . . . . . . . . . . . . . . .

63

4.3. Modelos de distribuciones de probabilidad para variables discretas . . . . . . . . . . . . . . .

64

4.3.1. Distribuci√≥n binomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

4.3.2. Distribuci√≥n de Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

68

4.3.3. Distribuci√≥n geom√©trica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

70

4.3.4. Distribuci√≥n binomial negativa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71

4.4. Variable aleatoria continua

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

4.4.1. Denici√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

4.4.2. Histograma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

4.4.3. Funci√≥n de densidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

4.4.4. Funci√≥n de distribuci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

76

4.4.5. Funci√≥n de distribuci√≥n emp√≠rica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

4.4.6. Media y varianza de una v.a. continua . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

4.5. Modelos de distribuciones de probabilidad para variables continuas . . . . . . . . . . . . . . .

82

4.5.1. Distribuci√≥n uniforme (continua) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

82

4.5.2. Distribuci√≥n exponencial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

82

4.5.3. Distribuci√≥n Gamma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

84

4.5.4. Distribuci√≥n normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86

4.6. Cuantiles de una distribuci√≥n. Aplicaciones . . . . . . . . . . . . . . . . . . . . . . . . . . . .

92

4.6.1. La bombilla de bajo consumo marca ANTE . . . . . . . . . . . . . . . . . . . . . . . .

93

4.6.2. Las visitas al pediatra de los padres preocupados . . . . . . . . . . . . . . . . . . . . .

94

5. Variables aleatorias con distribuci√≥n conjunta

97

5.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

97

5.2. Distribuciones conjunta, marginal y condicionada . . . . . . . . . . . . . . . . . . . . . . . . .

99

5.2.1. Distribuci√≥n conjunta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

99

5.2.2. Distribuciones marginales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.2.3. Distribuciones condicionadas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.3. Independencia estad√≠stica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.4. Medias, varianzas y covarianzas asociadas a un vector aleatorio . . . . . . . . . . . . . . . . . 111
5.4.1. Covarianza y coeciente de correlaci√≥n lineal . . . . . . . . . . . . . . . . . . . . . . . 111
5.4.2. Vector de medias y matriz de varianzas-covarianzas de un vector . . . . . . . . . . . . 118
5.5. Distribuci√≥n normal multivariante . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

III Inferencia estad√≠stica

125

6. Distribuciones en el muestreo

127

6.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.2. Muestreo aleatorio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
6.3. Distribuciones en el muestreo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
6.4. Distribuciones en el muestreo relacionadas con la distribuci√≥n normal . . . . . . . . . . . . . . 129
Prof. Dr. Antonio Jos√© S√°ez Castillo

5

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

7. Estimaci√≥n de par√°metros de una distribuci√≥n

133

7.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.2. Estimaci√≥n puntual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
7.2.1. Denici√≥n y propiedades deseables de los estimadores puntuales . . . . . . . . . . . . . 134
7.2.2. Estimaci√≥n de la media de una v.a. La media muestral . . . . . . . . . . . . . . . . . . 135
7.2.3. Estimaci√≥n de la varianza de una v.a. Varianza muestral . . . . . . . . . . . . . . . . . 135
7.2.4. Estimaci√≥n de una proporci√≥n poblacional . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.2.5. Obtenci√≥n de estimadores puntuales. M√©todos de estimaci√≥n . . . . . . . . . . . . . . . 138
7.2.5.1. M√©todo de los momentos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
7.2.5.2. M√©todo de m√°xima verosimilitud . . . . . . . . . . . . . . . . . . . . . . . . . 139
7.2.6. Tabla resumen de los estimadores de los par√°metros de las distribuciones m√°s comunes 142
7.3. Estimaci√≥n por intervalos de conanza . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.3.1. Intervalos de conanza para la media . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.3.2. Intervalos de conanza para una proporci√≥n . . . . . . . . . . . . . . . . . . . . . . . . 146
7.3.3. Intervalos de conanza para la varianza . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.3.4. Otros intervalos de conanza . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.4. Resoluci√≥n del ejemplo de los niveles de plomo . . . . . . . . . . . . . . . . . . . . . . . . . . 148

8. Contrastes de hip√≥tesis param√©tricas

149

8.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
8.2. Errores en un contraste de hip√≥tesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.3. p-valor de un contraste de hip√≥tesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.3.1. Denici√≥n de p-valor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.3.2. C√°lculo del p-valor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.4. Contraste para la media de una poblaci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
8.4.1. Con muestras grandes (n ‚â• 30) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
8.4.2. Con muestras peque√±as (n < 30) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
8.5. Contraste para la diferencia de medias de poblaciones independientes . . . . . . . . . . . . . . 159
8.5.1. Con muestras grandes (n1 , n2 ‚â• 30) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
8.5.2. Con muestras peque√±as (n1 < 30 o n2 < 30) y varianzas iguales . . . . . . . . . . . . . 160
8.5.3. Con muestras peque√±as, varianzas distintas y mismo tama√±o muestral . . . . . . . . . 161
8.5.4. Con muestras peque√±as, varianzas distintas y distinto tama√±o muestral . . . . . . . . 161
8.6. Contraste para la diferencia de medias de poblaciones apareadas . . . . . . . . . . . . . . . . 162
8.6.1. Con muestras grandes (n ‚â• 30) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.6.2. Con muestras peque√±as (n < 30) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.7. Contraste para la proporci√≥n en una poblaci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . 164
8.8. Contraste para la diferencia de proporciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

6

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

8.9. Contraste para la varianza de una poblaci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8.10. Contraste para el cociente de varianzas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8.11. Contraste para las medias de m√°s de dos poblaciones independientes. ANOVA . . . . . . . . . 168
8.12. El problemas de las pruebas m√∫ltiples. M√©todo de Bonferroni . . . . . . . . . . . . . . . . . . 171
8.13. Resoluci√≥n del ejemplo del del di√°metro de los cojinetes . . . . . . . . . . . . . . . . . . . . . 172

9. Contrastes de hip√≥tesis no param√©tricas

173

9.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.2. Contrastes de bondad de ajuste . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.2.1. Test œá2 de bondad de ajuste . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.2.2. Test de Kolmogorov-Smirno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
9.3. Contraste de independencia œá2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
9.4. Resoluci√≥n del ejemplo de los accidentes laborales . . . . . . . . . . . . . . . . . . . . . . . . . 183

10.Regresi√≥n lineal simple

185

10.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.2. Estimaci√≥n de los coecientes del modelo por m√≠nimos cuadrados . . . . . . . . . . . . . . . . 188
10.3. Supuestos adicionales para los estimadores de m√≠nimos cuadrados

. . . . . . . . . . . . . . . 192

10.4. Inferencias sobre el modelo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
10.4.1. Inferencia sobre la pendiente . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
10.4.2. Inferencia sobre la ordenada en el origen . . . . . . . . . . . . . . . . . . . . . . . . . . 197
10.5. El coeciente de correlaci√≥n lineal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
10.6. Fiabilidad de la recta de regresi√≥n. El coeciente de determinaci√≥n lineal . . . . . . . . . . . . 202
10.7. Predicci√≥n y estimaci√≥n a partir del modelo . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.8. Diagnosis del modelo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
10.8.1. Normalidad de los residuos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
10.8.2. Gr√°ca de residuos frente a valores ajustados . . . . . . . . . . . . . . . . . . . . . . . 206

IV Procesos aleatorios

209

11.Procesos aleatorios

211

11.1. Introducci√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
11.1.1. Denici√≥n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
11.1.2. Tipos de procesos aleatorios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
11.2. Descripci√≥n de un proceso aleatorio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
11.2.1. Descripci√≥n estad√≠stica mediante distribuciones multidimensionales . . . . . . . . . . . 215
11.2.2. Funci√≥n media y funciones de autocorrelaci√≥n y autocovarianza . . . . . . . . . . . . . 215
11.3. Tipos m√°s comunes de procesos aleatorios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
Prof. Dr. Antonio Jos√© S√°ez Castillo

7

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

11.3.1. Procesos independientes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
11.3.2. Procesos con incrementos independientes . . . . . . . . . . . . . . . . . . . . . . . . . 218
11.3.3. Procesos de Markov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
11.3.4. Procesos d√©bilmente estacionarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
11.3.5. Procesos erg√≥dicos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.4. Ejemplos de procesos aleatorios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
11.4.1. Ruidos blancos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
11.4.2. Procesos gaussianos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
11.4.3. Procesos de Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

8

Prof. Dr. Antonio Jos√© S√°ez Castillo

Pr√≥logo

El objeto fundamental de la edici√≥n de este documento es facilitar a los alumnos de ingenier√≠a de la Escuela
Polit√©cnica Superior de Linares el desarrollo de los contenidos te√≥ricos de la asignatura Estad√≠stica. Desde un
punto de vista menos local, espero que sea √∫til, en alguna medida, a todo aquel que necesite conocimientos
b√°sicos de las t√©cnicas estad√≠sticas m√°s usuales en el ambiente cient√≠co-tecnol√≥gico.
A todos ellos, alumnos y lectores en general, quiero facilitarles el privilegio de aprender de quienes yo he
aprendido, sugiri√©ndoles cuatro manuales que para m√≠ han sido referencias fundamentales. Se trata, en primer
lugar, del magn√≠co libro de Sheldon M. Ross,

Introducci√≥n a la Estad√≠stica.

En √©l puede encontrarse la

mayor parte de lo que vamos a estudiar aqu√≠, explicado de forma sencilla y clara, pero tambi√©n comentarios
hist√≥ricos, rese√±as bibliogr√°cas sobre matem√°ticos y estad√≠sticos relevantes y ejemplos muy apropiados.
En segundo lugar, recomiendo los trabajos de William Navidi,
Jay Devore,

Estad√≠stica para ingenieros y cient√≠cos,

Probabilidad y estad√≠stica para ingenier√≠a y ciencias,

y

sobre todo por la actualidad de muchos

de sus ejemplos y por c√≥mo enfatizan el car√°cter aplicado, pr√°ctico, de la Estad√≠stica en el √°mbito de la
Ciencia y la Tecnolog√≠a. Finalmente, debo mencionar tambi√©n el libro de Mendenhal & Sincich,
y Estad√≠stica para Ingenier√≠a y Ciencias,

Probabilidad

que incluye, como los dos anteriores, unos ejemplos y ejercicios

propuestos magn√≠cos.
En el actual contexto del Espacio Europeo de Educaci√≥n Superior, la asignatura Estad√≠stica tiene, en la mayor
parte de los grados en ingenier√≠a, un car√°cter b√°sico y una dotaci√≥n de 6 cr√©ditos ECTS. As√≠ ocurre, por
ejemplo, en las ramas de industriales o telecomunicaciones que se imparten en la Universidad de Ja√©n. Otras
ramas, como la de ingenier√≠a civil/minera, han optado por incluirla como asignatura obligatoria, compartida
con una asignatura de ampliaci√≥n de matem√°ticas en la que se proponen 3 cr√©ditos ECTS de estad√≠stica. Con
todo, creo que estos apuntes pueden adaptarse a esos distintos contextos, aclarando qu√© temas pueden ser
m√°s adecuados para cada titulaci√≥n. En concreto:
1. Para las distintas especialidades de la rama de industriales ser√≠an oportunos los cap√≠tulos 1, 2, 3, 4, 6,
7, 8, 9 y 10. El cap√≠tulo 9, sobre contrastes no param√©tricos puede darse a modo de seminario, si el
desarrollo de la docencia as√≠ lo sugiere. Sin embargo, el cap√≠tulo 10, sobre regresi√≥n lineal simple, me
parece imprescindible en la formaci√≥n de un futuro ingeniero industrial.
2. En los grados de la rama de telecomunicaciones, creo que son necesarios los cap√≠tulos 1, 2, 3, 4, 5, 6,
7, 8 y 11. Resulta as√≠ el temario quiz√° m√°s exigente, debido a la necesidad de introducir un cap√≠tulo
sobre vectores aleatorios previo a otro sobre procesos estoc√°sticos. Queda a iniciativa del docente la
posibilidad de recortar algunos aspectos en los temas tratados en aras a hacer m√°s ligera la carga
docente.
3. Finalmente, en los grados de la rama civil y minera, donde la dotaci√≥n de cr√©ditos es menor, creo que
9

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

son adecuados los cap√≠tulos 1, 2, 3, 4, 6, 7, 8 y 10, si bien eliminando algunos de sus apartados, cuesti√≥n
√©sta que dejo, de nuevo, a juicio del docente. Tambi√©n sugiero que se trabajen los problemas sobre estos
cap√≠tulos directamente en el contexto de unas pr√°cticas con ordenador.
S√≥lo me queda pedir disculpas de antemano por las erratas que, probablemente, contienen estas p√°ginas. Os
ruego que me las hag√°is llegar para corregirlas en posteriores ediciones.
Linares, junio de 2012.

10

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 1
Introducci√≥n

Llegar√° un d√≠a en el que el razonamiento estad√≠stico ser√° tan necesario para el ciudadano como
ahora lo es la habilidad de leer y escribir
H.G. Wells (1866-1946)

Resumen. El cap√≠tulo incluye una introducci√≥n del t√©rmino Estad√≠stica y presenta los conceptos m√°s b√°sicos
relativos a poblaciones y muestras.

Palabras clave: estad√≠stica, poblaci√≥n, poblaci√≥n tangible, poblaci√≥n conceptual, variable, muestra, muestra
aleatoria simple.

1.1. ¬æQu√© signica Estad√≠stica?
Si buscamos en el Diccionario de la Real Academia Espa√±ola de la Lengua (DRAE) el vocablo
aparecen tres acepciones de dicha
1.

Estad√≠stica

palabra1 :

Estudio de los datos cuantitativos de la poblaci√≥n, de los recursos naturales e industriales, del tr√°co o
de cualquier otra manifestaci√≥n de las sociedades humanas.

2.

Conjunto de estos datos.

3.

Rama de la matem√°tica que utiliza grandes conjuntos de datos num√©ricos para obtener inferencias
basadas en el c√°lculo de probabilidades.

Probablemente el m√°s com√∫n de los signicados conocidos de la palabra sea el segundo, y por ello solemos
ver en los medios de comunicaci√≥n que cualquier recopilaci√≥n de cifras referentes a alg√∫n asunto es llamado
(de forma muy reduccionista)

estad√≠stica

Sin embargo, el valor real de la

o

Estad√≠stica

estad√≠sticas.

como ciencia tiene que ver mucho m√°s con la primera y la tercera

acepci√≥n del DRAE. Concretamente, el primero de los signicados se corresponde con lo que vamos a estudiar
como

Estad√≠stica Descriptiva,

donde la Estad√≠stica se utiliza para resumir, describir y explorar datos, y el

tercero con lo que denominaremos

Inferencia Estad√≠stica,

donde lo que se pretende mediante la Estad√≠stica

1 http://buscon.rae.es/draeI/SrvltGUIBusUsual?LEMA=estad %C3 %ADstica

11

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

es utilizar datos de un conjunto reducido de casos para inferir caracter√≠sticas de √©stos al conjunto de todos
ellos.

1.2. La Estad√≠stica en el √°mbito de la Ciencia y la Ingenier√≠a
El papel de la Estad√≠stica en la Ciencia y la Ingenier√≠a hoy en d√≠a es crucial, fundamentalmente porque
al analizar datos recopilados en experimentos de cualquier tipo, se observa en la mayor√≠a de las ocasiones
que dichos datos est√°n sujetos a alg√∫n tipo de incertidumbre. El investigador o el profesional debe tomar
decisiones respecto de su objeto de an√°lisis bas√°ndose en esos datos, para lo cual debe dotarse de herramientas
adecuadas.
A continuaci√≥n vamos a describir una serie de problemas pr√°cticos en los que se plantean situaciones de este
tipo. Vamos a ponerle un nombre espec√≠co porque iremos mencion√°ndolos a lo largo del curso, conforme
seamos capaces de responder a las cuestiones que cada uno de ellos dejan abiertas.

1.2.1. Ejemplo de las capas de √≥xido de silicio
El art√≠culo Virgin Versus Recycled Wafers for Furnace Qualication: Is the Expense Justied? (V. Czitrom y
J. Reece, en Statistical

Case Studies for Industrial Process Improvement,

ASA y SIAM, 1997:87-104) describe

un proceso para el crecimiento de una capa delgada de di√≥xido de silicio sobre placas de silicio que se usan en
la fabricaci√≥n de semiconductores. En √©l aparecen datos relativos a las mediciones del espesor, en angstroms
‚ó¶

(A), de la capa de √≥xido para pruebas realizadas en 24 placas: en concreto, se realizaron 9 mediciones en cada
una de las 24 placas. Las placas se fabricaron en dos series distintas, 12 placas en cada serie. Estas placas
eran de distintos tipos y se procesaron en distintas posiciones en el horno, ya que entre otros aspectos, el
prop√≥sito de la recopilaci√≥n de los datos era determinar si el espesor de la capa de √≥xido estaba afectado por
el tipo de placa y por la posici√≥n en el horno. Por el contrario, el experimento se dise√±√≥ de tal manera que
no se esperaba ninguna diferencia sistem√°tica entre las dos series. Los datos se muestran en la Tabla 1.1.
Lo primero que salta a la vista al mirar esos datos es que es muy complicado hacerse una idea global de los
‚ó¶

resultados. Parecen estar en torno a 90 A, pero con variaciones importantes respecto de ese valor. Algunas de
esas variaciones son especialmente llamativas (77.5, 106.7, ...): ¬æqu√© pas√≥ en esas placas? En suma, es evidente
que se hace necesaria una manera sistem√°tica de analizar los datos, tratando de describirlos de forma precisa
y objetiva, respondiendo a las preguntas que subyacen en el dise√±o del experimento: ¬æson las dos series de
experimentos homog√©neas? ¬æafecta el tipo de placa? ¬æafecta la posici√≥n en el horno? ...

1.2.2. Ejemplo de la bombilla de bajo consumo
En el envoltorio de la bombilla marca ANTE de 14W se arma literalmente L√°mpara

ahorradora de energ√≠a.

Duraci√≥n 8 a√±os .

Debo reconocer de que tengo mis dudas. Para empezar, ¬æes que a los 8 a√±os, de repente, la l√°mpara se
rompe? Por otra parte, creo que todos nosotros hemos experimentado el hecho de que √©stas l√°mparas que
supuestamente tienen una duraci√≥n mayor que las tradicionales l√°mparas incandescentes (seg√∫n el envoltorio,
8 veces mayor), sin embargo, se rompen con facilidad. Luego, ¬æqu√© quiere decir exactamente el envoltorio al
armar que su duraci√≥n es de 8 a√±os?

12

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Serie
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2

Placa
1
2
3
4
5
6
7
8
9
10
11
12
1
2
3
4
5
6
7
8
9
10
11
12

‚ó¶

90.00
91.80
90.30
92.60
91.10
76.10
92.40
91.30
96.70
92.00
94.10
91.70
93.00
91.40
91.90
90.60
93.10
90.80
88.00
88.30
94.20
101.50
92.80
92.10

92.20
94.50
91.10
90.30
89.80
90.20
91.70
90.10
93.70
94.60
91.50
97.40
89.90
90.60
91.80
91.30
91.80
91.50
91.80
96.00
92.20
103.10
90.80
93.40

94.90
93.90
93.30
92.80
91.50
96.80
91.60
95.40
93.90
93.70
95.30
95.10
93.60
92.20
92.80
94.90
94.60
91.50
90.50
92.80
95.80
103.20
92.20
94.00

92.70
77.30
93.50
91.60
91.50
84.60
91.10
89.60
87.90
94.00
92.80
96.70
89.00
91.90
96.40
88.30
88.90
91.50
90.40
93.70
92.50
103.50
91.70
94.70

A
91.6
92.0
87.2
92.7
90.6
93.3
88.0
90.7
90.4
89.3
93.4
77.5
93.6
92.4
93.8
87.9
90.0
94.0
90.3
89.6
91.0
96.1
89.0
90.8

88.20
89.90
88.10
91.70
93.10
95.70
92.40
95.80
92.00
90.10
92.20
91.40
90.90
87.60
86.50
92.20
97.90
91.00
91.50
89.60
91.40
102.50
88.50
92.10

92.00
87.90
90.10
89.30
88.90
90.90
88.70
91.70
90.50
91.30
89.40
90.50
89.80
88.90
92.70
90.70
92.10
92.10
89.40
90.20
92.80
102.00
87.50
91.20

98.20
92.80
91.90
95.50
92.50
100.30
92.90
97.90
95.20
92.70
94.50
95.20
92.40
90.90
90.90
91.30
91.60
91.80
93.20
95.30
93.60
106.70
93.80
92.30

96.00
93.30
94.50
93.60
92.40
95.20
92.60
95.70
94.30
94.50
95.40
93.10
93.00
92.80
92.80
93.60
98.40
94.00
93.90
93.00
91.00
105.40
91.40
91.10

Cuadro 1.1: Datos del espesor de las capas de √≥xido de silicio

Prof. Dr. Antonio Jos√© S√°ez Castillo

13

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

En realidad, nosotros deberemos aprender a analizar este problema, asumiendo que la duraci√≥n de esta
bombilla no es un valor jo y conocido, sino que est√° sujeto a incertidumbre. Lo que haremos ser√° dotarnos
de un modelo matem√°tico que nos permita valorar si es probable o no que una l√°mpara ANTE se rompa
antes de un a√±o, despu√©s de tres a√±os, etc.

1.2.3. Ejemplo de los niveles de plomo
Un art√≠culo publicado en

Journal of Environmental Engineering

en 2002, titulado Leachate from Land Dis-

posed Residential Construction Waste, presenta un estudio de la contaminaci√≥n en basureros que contienen
desechos de construcci√≥n y desperdicios de demoliciones. De un sitio de prueba se tomaron 42 muestras de
lixiado, de las cuales 26 contienen niveles detectables de plomo. Se pone as√≠ de maniesto que s√≥lo una parte
de los basureros est√° contaminada por plomo. La cuesti√≥n es ¬æqu√© proporci√≥n supone esta parte contaminada
de la supercie total de los basureros?
Si una ingeniera desea obtener a partir de esos datos una estimaci√≥n de la proporci√≥n de los basureros que
contiene niveles detectables de plomo debe ser consciente de dos cuestiones:
1. Es imposible analizar todos los rincones de todos los basureros.
2. Si se basa s√≥lo en los datos del art√≠culo, esa estimaci√≥n ser√° s√≥lo eso, una estimaci√≥n basada en esa
muestra, que es de s√≥lo 42 datos. Deber√≠a, por tanto obtener tambi√©n una estimaci√≥n del error que est√°
cometiendo al hacer la estimaci√≥n. Con ambos resultados, la estimaci√≥n en s√≠ y una cuanticaci√≥n del
error que podr√≠a cometer con ella, incluso podr√° obtener un rango donde la verdadera proporci√≥n se
encuentra, con un alto nivel de conanza.

1.2.4. Ejemplo de los cojinetes
Un ingeniero industrial es responsable de la producci√≥n de cojinetes de bolas y tiene dos m√°quinas distintas
para ello. Le interesa que los cojinetes producidos tengan di√°metros similares, independientemente de la
m√°quina que los produce, pero tiene sospechas de que est√° produciendo alg√∫n problema de falta de calibraci√≥n
entre ellas. Para analizar esta cuesti√≥n, extrae una muestra de 120 cojinetes que se fabricaron en la m√°quina
A, y encuentra que la media del di√°metro es de 5.068 mm y que su desviaci√≥n est√°ndar es de 0.011 mm. Realiza
el mismo experimento con la m√°quina B sobre 65 cojinetes y encuentra que la media y la desviaci√≥n est√°ndar
son, respectivamente, 5.072 mm y 0.007 mm. ¬æPuede el ingeniero concluir que los cojinetes producidos por
las m√°quinas tienen di√°metros medios signicativamente diferentes?

1.2.5. Ejemplo de la absorci√≥n de un compuesto a distintas dosis y en distintos
tiempos de absorci√≥n
Un equipo de investigadores que trabajan en seguridad en el trabajo est√° tratando de analizar c√≥mo la
piel absorbe un cierto componente qu√≠mico peligroso. Para ello, coloca diferentes vol√∫menes del compuesto
qu√≠mico sobre diferentes segmentos de piel durante distintos intervalos de tiempo, midiendo al cabo de ese
tiempo el porcentaje de volumen absorbido del compuesto. El dise√±o del experimento se ha realizado para que
la interacci√≥n esperable entre el tiempo y el volumen no inuya sobre los resultados. Los datos se mostrar√°n
en el √∫ltimo tema.

14

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Lo que los investigadores se cuestionan es si la cantidad de compuesto por un lado y el tiempo de exposici√≥n
al que se somete por otro, inuyen en el porcentaje que se absorbe. De ser as√≠, ser√≠a interesante estimar
el porcentaje de absorci√≥n de personas que se sometan a una exposici√≥n de una determinada cantidad, por
ejemplo, durante 8 horas.

1.2.6. Ejemplo de los accidentes laborales
En una empresa se sospecha que hay franjas horarias donde los accidentes laborales son m√°s frecuentes.
Para estudiar este fen√≥meno, contabilizan los accidentes laborales que sufren los trabajadores seg√∫n franjas
horarias, durante un a√±o. Los resultados aparecen en la tabla.
Horas del d√≠a
8-10 h.
10-12 h.
13-15 h.
15-17 h.

N√∫mero de accidentes
47
52
57
63

Con esa informaci√≥n, los responsables de seguridad de la empresa deben decidir si hay franjas horarias donde
los accidentes son m√°s probables o si, por el contrario, √©stos ocurren absolutamente al azar.

1.2.7. Ejemplo de la cobertura de la antena de telefon√≠a m√≥vil
Reduciendo mucho el problema, supongamos que una antena de telefon√≠a m√≥vil tiene una cobertura que
abarca a cualquier m√≥vil dentro de un c√≠rculo de radio r. Un ingeniero puede suponer que un tel√©fono
concreto puede estar situado en

cualquier punto al azar

de ese c√≠rculo, pero ¬æc√≥mo plasmar eso? Por ejemplo,

si nos centramos en la distancia a la antena, ¬æcualquier distancia es

igualmente probable ?

¬æY qu√© podemos

decir de las coordenadas en un momento concreto del m√≥vil?

1.2.8. Ejemplo de la se√±al aleatoria
En el contexto de las telecomunicaciones, cualquier se√±al debe considerarse aleatoria, es decir, debe tenerse en
cuenta que cuando la observamos, parte de ella es debida a la incertidumbre inherente a cualquier proceso de
comunicaci√≥n. Y es que, por multitud de razones, nadie tiene garant√≠as que la se√±al enviada sea exactamente
igual a la se√±al recibida.
Un ingeniero debe tener en cuenta eso y, a pesar de todo, ser capaz de analizar las propiedades m√°s relevantes
de cualquier se√±al y de estudiar su comportamiento en cualquier momento del proceso de comunicaci√≥n.
Por ejemplo, hoy en d√≠a una se√±al sufre multitud de transformaciones en el proceso de comunicaci√≥n. Cada
una de esas transformaciones se considera el resultado del paso de la se√±al por un sistema. El ingeniero debe
ser capaz de conocer las caracter√≠sticas m√°s relevantes de la se√±al a lo largo de todas esas transformaciones.

1.3. Deniciones b√°sicas
Para nalizar este primer tema de introducci√≥n, vamos a ir jando las deniciones m√°s elementales que
utilizaremos a lo largo del curso y que ya han sido motivadas en la introducci√≥n de los ejemplos anteriores.
Prof. Dr. Antonio Jos√© S√°ez Castillo

15

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Se denomina

poblaci√≥n a un conjunto de individuos o casos, objetivo de nuestro inter√©s.

Podemos distinguir entre poblaciones tangibles y poblaciones conceptuales.
Una poblaci√≥n es

tangible si consta de elementos f√≠sicos reales que forman un conjunto nito.

Por ejemplo, si estamos considerando el estudio de la altura de los alumnos de la Escuela, el conjunto de
estos alumnos es una poblaci√≥n tangible.
Una poblaci√≥n

conceptual no tiene elementos reales, sino que sus casos se obtienen por la repetici√≥n de un

experimento.
Por ejemplo, cuando plante√°bamos las pruebas sobre placas de silicio, vemos que hay tantos casos como pruebas puedan hacerse, lo que supone un conjunto innito de casos. En poblaciones conceptuales es imposible,
por tanto, conocer todos los casos, y tenemos que conformarnos con muestras de los mismos.
Una

variable o dato es una caracter√≠stica concreta de una poblaci√≥n.

Por ejemplo:
Si consideramos la poblaci√≥n de todos los alumnos de la Escuela, podemos jarnos en la variable altura.
Si consideramos el supuesto de las pruebas sobre placas de silicio, podemos considerar la variable espesor
de la capa de √≥xido de silicio generada.

Se denomina

muestra a cualquier subconjunto de datos seleccionados de una poblaci√≥n.

El objetivo de una muestra, ya sea en una poblaci√≥n tangible o en una poblaci√≥n conceptual es que los
elementos de la muestra

representen al conjunto de todos los elementos de la poblaci√≥n. Esta cuesti√≥n, la

construcci√≥n de muestras adecuadas, representativas, es uno de los aspectos m√°s delicados de la Estad√≠stica.
Nosotros vamos a considerar en esta asignatura s√≥lo un tipo de muestras, denominadas muestras

simples.

aleatorias

En una muestra aleatoria simple, todos los elementos de la poblaci√≥n deben tener las mismas

posibilidades de salir en la muestra y, adem√°s, los elementos de la muestra deben ser independientes: el que
salga un resultado en la muestra no debe afectar a que ning√∫n otro resultado salga en la muestra.
Por ejemplo, podr√≠amos estar interesados en la poblaci√≥n de todos los espa√±oles con derecho a voto (poblaci√≥n
tangible, pero enorme), de los que querr√≠amos conocer un dato o variable, su intenci√≥n de voto en las pr√≥ximas
elecciones generales. Dado que estamos hablando de millones de personas, probablemente deberemos escoger
una muestra, es decir, un subconjunto de espa√±oles a los que se les realizar√≠a una encuesta. Si queremos que
esa muestra sea aleatoria simple, deberemos tener cuidado de que todos los espa√±oles con derecho a voto
tengan las mismas posibilidades de caer en la muestra y de que la respuesta de un entrevistado no afecte a la
de ning√∫n otro. Como nota curiosa, sabed que la mayor√≠a de las encuestas nacionales se hacen v√≠a telef√≥nica,
lo cual es una peque√±a violaci√≥n de las hip√≥tesis de muestra aleatoria simple, ya que hay espa√±oles con
derecho a voto que no tienen tel√©fono, luego es imposible que salgan en la muestra.

16

Prof. Dr. Antonio Jos√© S√°ez Castillo

Parte I
Estad√≠stica descriptiva

17

Cap√≠tulo 2
El tratamiento de los datos. Estad√≠stica
descriptiva

Es un error capital el teorizar antes de poseer datos. Insensiblemente uno comienza a alterar
los hechos para encajarlos en las teor√≠as, en lugar encajar las teor√≠as en los hechos
Sherlock Holmes (A. C. Doyle), en

Un esc√°ndalo en Bohemia

Resumen. En este cap√≠tulo aprenderemos m√©todos para resumir y describir conjuntos de datos a trav√©s de
distintos tipos de tablas, gr√°cos y medidas estad√≠sticas.

Palabras clave:

datos cuantitativos, datos cualitativos, datos discretos, datos continuos, distribuci√≥n de

frecuencias, diagrama de barras, diagrama de sectores, histograma, media, mediana, moda, cuantiles, varianza,
desviaci√≥n t√≠pica, asimetr√≠a, datos at√≠picos.

2.1. Introducci√≥n
Obtenidos a trav√©s de encuestas, experimentos o cualquier otro conjunto de medidas, los datos estad√≠sticos
suelen ser tan numerosos que resultan pr√°cticamente in√∫tiles si no son resumidos de forma adecuada. Para
ello la Estad√≠stica utiliza tanto t√©cnicas gr√°cas como num√©ricas, algunas de las cuales describimos en este
cap√≠tulo.
Podemos decir que existe una clasicaci√≥n, un tanto articial, de los datos, seg√∫n se reeran a una poblaci√≥n
tangible, en cuyo caso se conocer√°n todos los casos, o a una poblaci√≥n conceptual, en cuyo caso s√≥lo se
conocer√° una muestra (aleatoria simple). Sin embargo, esta clasicaci√≥n no tiene ning√∫n efecto en lo relativo
a lo que vamos a estudiar en este cap√≠tulo.

2.2. Tipos de datos
Los datos (o variables) pueden ser de dos tipos:

cuantitativos y cualitativos.
19

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

cuantitativos son los que representan una cantidad reejada en una escala num√©rica. A su vez,
pueden clasicarse como datos cuantitativos discretos si se reeren al conteo de alguna caracter√≠stica, o
datos cuantitativos continuos si se reeren a una medida.
Los datos

Los datos

cualitativos o categ√≥ricos se reeren a caracter√≠sticas de la poblaci√≥n que no pueden asociarse

a cantidades con signicado num√©rico, sino a caracter√≠sticas que s√≥lo pueden clasicarse.

Ejemplo. Veamos algunos ejemplos de cada uno de estos tipos de variables:
En el ejemplo del √≥xido de silicio, la variable
En el ejemplo de los cojinetes, el

espesor

es cuantitativa continua.

di√°metro de los cojinetes

es una variable cuantitativa continua.

En el ejemplo de los niveles de plomo, se est√° analizando si una muestra contiene niveles detectables o no. Se trata, por tanto, de una variable cualitativa con dos categor√≠as:
detectables

o

s√≠ contiene niveles

no contiene niveles detectables.

En el ejemplo de los accidentes laborales, la variable n√∫mero

de accidentes laborales

es cuantitativa

discreta, mientras que las franjas horarias constituyen una variable cualitativa.

2.3. M√©todos gr√°cos y num√©ricos para describir datos cualitativos
La forma m√°s sencilla de describir de forma num√©rica una variable cualitativa es determinar su distribuci√≥n
de frecuencias. Por su parte, esta distribuci√≥n de frecuencias determina a su vez las representaciones gr√°cas
m√°s usuales.
Supongamos que tenemos una variable cualitativa, que toma una serie de posibles valores (categor√≠as). El
n√∫mero de veces que se da cada valor es la

distribuci√≥n de frecuencias de la variable. Si en vez de dar el
distribuci√≥n de frecuencias relativas.

n√∫mero de veces nos jamos en la proporci√≥n de veces, tenemos la

Las representaciones gr√°cas m√°s usuales son los diagramas de barras y los diagramas de sectores.
Los diagramas

de barras son una representaci√≥n de cada una de las categor√≠as de la variable mediante una

barra colocada sobre el eje X y cuya altura sea la frecuencia o la frecuencia relativa de dichas categor√≠as.
Los

diagramas de sectores son c√≠rculos divididos en tantos sectores como categor√≠as, sectores cuyo √°ngulo

debe ser proporcional a la frecuencia de cada categor√≠a.

20

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Categor√≠a
Pa√≠s
B√©lgica
Francia
Finlandia
Alemania
Holanda
Jap√≥n
Suecia
Suiza
Estados Unidos
TOTAL

Frecuencia
N√∫mero de reactores nucleares
4
22
2
7
1
11
3
1
47
98

Frecuencia relativa
Proporci√≥n
0.041
0.225
0.020
0.071
0.010
0.112
0.031
0.010
0.480
1.000

Cuadro 2.1: Tabla de frecuencias.

Ejemplo.

Tomamos como poblaci√≥n los 98 reactores nucleares m√°s grandes en todo el mundo. Nos

jamos en la variable o dato referente al pa√≠s donde est√°n localizados.
Los datos ser√≠an
B√©lgica, B√©lgica, B√©lgica, B√©lgica, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia,
Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Francia, Finlandia, Finlandia, Alemania, Alemania, Alemania, Alemania,
Alemania, Alemania, Alemania, Holanda, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Jap√≥n, Suecia, Suecia, Suecia,
Suiza, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados
Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados
Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados
Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados
Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados Unidos, Estados
Unidos, Estados Unidos, Estados Unidos.

Las distribuciones de frecuencias y de frecuencias relativas podemos resumirlas en una

cuencias como la que aparece en el Cuadro 2.1.

tabla de fre-

Por su parte, las representaciones mediante diagramas de barras y sectores de estos datos aparecen en la
Figura 2.1 y la Figura 2.2 respectivamente.

2.4. M√©todos gr√°cos para describir datos cuantitativos
Si tenemos una variable cuantitativa discreta y √©sta toma pocos valores, podemos tratarla como si fuera una
variable cualitativa, calcular su distribuci√≥n de frecuencias y dibujar un diagrama de barras.

Ejemplo.

En una empresa con cadena de montaje donde se empaquetan piezas en cajas se realiza

un estudio sobre la calidad de producci√≥n. Los datos siguientes informan sobre el n√∫mero de piezas
defectuosas encontradas en una muestra de cajas examinadas:
000000111111111222222222233333334444444555566666777889
Prof. Dr. Antonio Jos√© S√°ez Castillo

21

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0

10

20

30

40

Reactores nucleares. Pa√≠s de origen

Alemania

B√©lgica

EEUU

Finlandia

Francia

Holanda

Jap√≥n

Suecia

Suiza

Figura 2.1: Diagrama de barras.
Reactores nucleares. Pa√≠s de origen

EEUU

B√©lgica

Alemania

Suiza
Suecia

Jap√≥n

Finlandia

Holanda
Francia

Figura 2.2: Diagrama de sectores.

El diagrama de barras asociado aparecen en la Figura 2.3.
Sin embargo, la mayor√≠a de variables cuantitativas son de tipo continuo, de manera que toman demasiados
valores como para que la representaci√≥n de su distribuci√≥n de frecuencias sea √∫til1 . Por ello el m√©todo gr√°co
m√°s com√∫n y tradicional para datos cuantitativos es el histograma.
El histograma es una variante del diagrama de barras donde se agrupan los valores de la variable en intervalos
para que estos intervalos tengan frecuencias mayores que uno.
Para obtener un histograma de forma manual deben seguirse los siguientes pasos:
1. Calculamos el n√∫mero, N , de intervalos que vamos a utilizar. Se recomienda que sea aproximadamente
igual a la ra√≠z cuadrada del n√∫mero de datos. Sin embargo, los programas estad√≠sticos suelen utilizar
otro m√©todo, llamado

M√©todo de Sturges,

en el que N = dlog2 n + 1e, donde n es el n√∫mero de datos y

[] es la funci√≥n parte entera.
1 Si toma muchos valores, muy probablemente la mayor parte de ellos s√≥lo aparezca una vez, por lo que la distribuci√≥n de
frecuencias ser√° casi siempre constante e igual a 1.

22

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0

2

4

6

8

10

N√∫mero de piezas defectuosas

0

1

2

3

4

5

6

7

8

9

Figura 2.3: Diagrama de barras.
2. Calculamos el rango, R, del histograma, que ser√° ligeramente m√°s amplio que el rango de los datos.
El histograma debe comenzar en un n√∫mero (xm ) ligeramente por debajo del m√≠nimo de los datos y
terminar en un n√∫mero (xM ) ligeramente por encima del m√°ximo. El rango del histograma ser√°, por
tanto, R = xM ‚àí xm .
3. Calculamos la longitud, L, de los intervalos, como el cociente entre el rango del histograma y el n√∫mero
de intervalos, es decir, L =

R
N.

4. Se construyen los N intervalos:

I1 = [xm , xm + L)
I2 = [xm + L, xm + 2L)
I3 = [xm + 2L, xm + 3L)
...
IN = [xm + N √ó L, xM ).
5. Para cada intervalo, contamos el n√∫mero de datos que hay en √©l, es decir, la frecuencia del intervalo.
6. El histograma es un diagrama de barras donde en el eje X se colocan los intervalos y sobre ellos se
construyen barras cuya altura sea la frecuencia o la frecuencia relativa del intervalo. En este caso, las
barras deben dibujarse sin espacio entre ellas. En ocasiones, en vez de tomar la frecuencia relativa como
altura de las barras, se toma dicha frecuencia relativa como √°rea de las barras: en ese caso, se habla de
un histograma en escala de densidad.

Nota. Por cuestiones que detallaremos m√°s adelante es importante destacar que el porcentaje de datos
que cae dentro de un intervalo es proporcional al √°rea de la barra que se construye sobre ese intervalo.
Por ejemplo, si el √°rea de una barra es el 30 % del √°rea total del intervalo, entonces el 30 % de los datos
est√°n en dicho intervalo.
Prof. Dr. Antonio Jos√© S√°ez Castillo

23

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

5
4
1

2

3

Frecuencia

6

7

8

9

Tiempos de procesado

0.00

0.96

1.92

2.88

3.84

4.80

Figura 2.4: Histograma.

Por otra parte, ¬æqu√© pasar√≠a si tomamos un n√∫mero muy grande de datos? El n√∫mero de intervalos
del histograma ser√≠a tambi√©n muy grande, y las barras ser√≠an muy estrechas, de manera que en vez de
parecer un diagrama de barras, parecer√≠a la gr√°ca de una funci√≥n real de variable real. Hablaremos de
esta funci√≥n y del √°rea debajo de ella en breve. Por cierto, ¬æc√≥mo se calcula el √°rea bajo esta funci√≥n?

Ejemplo. Los datos siguientes corresponden al tiempo necesario para procesar 25 trabajos en una CPU.
1.17

1.61

1.16

1.38

3.53

1.23

3.76

1.94

0.96

4.75

0.15

2.41

0.71

0.02

1.59

0.19

0.82

0.47

2.16

2.01

0.92

0.75

2.59

3.07

1.4

Vamos a calcular un histograma para esos datos.
1. Dado que

‚àö

25 = 5, utilizaremos 5 intervalos.

2. El m√≠nimo de los datos es 0.02 y el m√°ximo 4.75, de manera que podemos considerar como rango
del histograma el intervalo [0, 4.8], cuya longitud (rango del histograma) es 4.8.
3. La longitud de los intervalos es, en ese caso,

4.8
5

= 0.96.

4. Construimos los intervalos:

I1 = [0, 0.96)
I2 = [0.96, 1.92)
I3 = [1.92, 2.88)
I4 = [2.88, 3.84)
I5 = [3.84, 4.8)

24

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

5. Calculamos la distribuci√≥n de frecuencia asociada a esos intervalos:
Tiempo de procesado

Frecuencia

[0, 0.96)

8

[0.96, 1.92)

8

[1.92, 2.88)

5

[2.88, 3.84)

3

[3.84, 4.8)

1

6. Finalmente, representamos el diagrama de barras (Figura 2.4).

2.5. M√©todos num√©ricos para describir datos cuantitativos
Es cierto que un diagrama de barras o un histograma nos ayudan a tener una imagen de c√≥mo son los datos,
pero normalmente es necesario complementar esa imagen mediante medidas que, de forma objetiva, describan
las caracter√≠sticas generales del conjunto de datos.
Vamos a ver en este apartado tres tipos de medidas, que b√°sicamente responden a tres preguntas:
est√°n los datos

(medidas de posici√≥n),

forma tienen los datos

c√≥mo de agrupados est√°n los datos

por d√≥nde

(medidas de dispersi√≥n) y

qu√©

(medidas de forma).

2.5.1. Medidas de tendencia central
Las

medidas de tendencia central son medidas de posici√≥n que tratan de establecer un valor que pueda

considerarse

el centro

de los datos en alg√∫n sentido.

2.5.1.1. Media
Sea un conjunto de datos de una variable cuantitativa, x1 , ..., xn . La

Pn
xÃÑ =

i=1

n

xi

media de los datos es

.

Esta medida es la m√°s com√∫n dentro de las de tendencia central y corresponde al

centro de gravedad

de los

datos.
Es inmediato comprobar que si se realiza un cambio de origen y escala sobre los datos, del tipo y = ax + b,
la media sufre el mismo cambio, es decir, yÃÑ = axÃÑ + b.
De igual forma, si tenemos datos de la suma de dos o m√°s variables, la media de la suma es la suma de las
medias de cada variable.
Prof. Dr. Antonio Jos√© S√°ez Castillo

25

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

2.5.1.2. Mediana
Sea un conjunto de datos de una variable cuantitativa, x1 , ..., xn . Ordenemos la muestra de menor a mayor,

x(1) , ..., x(n) .
La

mediana es el valor de la variable que deja el mismo n√∫mero de datos antes y despu√©s que √©l, una vez

ordenados estos.

El c√°lculo de la mediana depender√° de si el n√∫mero de datos, n, es par o impar:
Si n es impar, la mediana es el valor que ocupa la posici√≥n

n+1
2

una vez que los datos han sido ordenados

(en orden creciente o decreciente), porque √©ste es el valor central. Es decir: Me = x( n+1 ) .
2
Si n es par, la mediana es la media aritm√©tica de las dos observaciones centrales. Cuando n es par, los dos
x n +x n
( ) ( +1)
datos que est√°n en el centro de la muestra ocupan las posiciones n2 y n2 +1. Es decir: Me = 2 2 2
.
La mediana corresponde exactamente con la idea de valor central de los datos. De hecho, puede ser un valor
m√°s representativo de √©stos que la media, ya que es m√°s

robusta

que la media. Ve√°mos qu√© signica esto en

un ejemplo.

Ejemplo. Consideremos los datos siguientes:
0012345
Su media es

0+0+1+2+3+4+5
7

= 2.1429, y su mediana 2.

Pero imaginemos que por error o por casualidad obtenemos un nuevo dato enormemente grande en
relaci√≥n al resto de datos, 80. En ese caso, la media ser√≠a

0 + 0 + 1 + 2 + 3 + 4 + 5 + 80
= 11.875
8
y la mediana 2.5. Es decir, un solo dato puede desplazar enormemente la media, hasta convertirla en una
medida poco representativa, pero s√≥lo desplazar√° ligeramente la mediana. Ese es el motivo por el que se
dice que la mediana es una medida

robusta.

2.5.1.3. Moda o intervalo modal
En principio la

moda se dene como el valor m√°s frecuente de los datos. Lo que ocurre es que si √©stos son

datos de una variable continua o discreta con muchos valores, puede que los datos apenas se repitan. En ese
caso, en el que, como vimos en las representaciones gr√°cas, se debe agrupar por intervalos, no debe darse
un valor como moda, sino un

26

intervalo modal, aqu√©l con mayor frecuencia asociada.

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

2.5.2. Cuantiles
Los

cuantiles son medidas de posici√≥n pero no necesariamente ligados al centro

de los datos. La idea a la

que responden es muy sencilla y muy pr√°ctica. Se trata de valorar de forma relativa c√≥mo es un dato respecto
del conjunto global de todos los datos.
Si, por ejemplo, un ni√±o de 4 a√±os pesa 13 kilos, ¬æest√° desnutrido? ¬æest√° sano? La respuesta debe ser que
depende.

¬æD√≥nde vive el ni√±o? Es importante porque, por ejemplo, en Estados Unidos los ni√±os son en general

m√°s grandes que, por ejemplo, en Jap√≥n. Quiz√° m√°s que el peso nos interese saber qu√© posici√≥n relativa tiene
el peso del ni√±o dentro de la poblaci√≥n de la que forma parte. Por ejemplo, si nos dicen que el ni√±o est√° entre
el 1 % de los ni√±os que menos pesan, probablemente tiene un problema de crecimiento.
El

cuantil p (Qp ) de unos datos (0 ‚â§ p ‚â§ 1), ser√≠a un valor de la variable situado de modo que el 100p % de

los valores sean menores o iguales que √©l y el resto (100(1 ‚àí p) %) mayores.

No obstante, en la pr√°ctica vamos a encontrar un problema para encontrar cuantiles, sobre todo con pocos
datos: lo m√°s habitual es que no exista el valor exacto que deje a la izquierda el 100p % de los valores y el
resto a la derecha. Por ese motivo, los programas estad√≠sticos utilizan unas f√≥rmulas de interpolaci√≥n para
obtener el valor del cuantil entre los dos valores de los datos que lo contienen. En nuestro caso, a la hora
de obtener cuantiles, la aplicaci√≥n de esas f√≥rmulas de interpolaci√≥n

a mano

har√≠an muy lentos y pesados

los c√°lculos, por lo que vamos a aplicar un convenio mucho m√°s sencillo: aproximaremos el valor del cuantil
correspondiente de la siguiente forma:
1. Si el 100p % de n, donde n es el n√∫mero de datos, es un entero, k , entonces Qp =

x(k) +x(k+1)
.
2

2. Si el 100p % de n no es un entero, lo redondeamos al entero siguiente, k , y entonces Qp = x(k) .
No olvidemos, sin embargo, que los programas estad√≠sticos van a utilizar las f√≥rmulas de interpolaci√≥n para
calcular el valor de los cuantiles, de manera que no debe extra√±ar si se observan peque√±as diferencias al
comparar nuestros resultados

a mano

con los de estos programas.

Existen diversos nombres para referirse a algunos tipos de cuantiles. Entre ellos:
Los

percentiles

son los cuantiles que dividen la muestra en 100 partes, es decir, son los cuantiles

0.01 (percentil 1), 0.02 (percentil 2), ..., 0.99 (percentil 99). Si notamos por PŒ± al percentil Œ±, con

Œ± = 1, 2, 3, ..., 99, se tiene que PŒ± = QŒ±/100 . En Estad√≠stica Descriptiva es m√°s frecuente hablar de
percentiles que de cuantiles porque se reeren a cantidades entre 0 y 100, en tanto por ciento, que son
m√°s habituales de valorar por todo el mundo.
Los

cuartiles

dividen a la poblaci√≥n en cuatro partes iguales, es decir, corresponden a los cuantiles

0.25, 0.5 (mediana) y 0.75.

Ejemplo. Consideremos de nuevo los datos correspondientes al tiempo de procesado de 25 tareas en una
CPU. Ahora los hemos ordenado de menor a mayor (en 5 las):

Prof. Dr. Antonio Jos√© S√°ez Castillo

27

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0.02

0.75

1.17

1.61

2.59

0.15

0.82

1.23

1.94

3.07

0.19

0.92

1.38

2.01

3.53

0.47

0.96

1.40

2.16

3.76

0.71

1.16

1.59

2.41

4.75

Vamos a calcular distintas medidas de posici√≥n y a comentarlas.
En primer lugar, la media es 1.63. La mediana ocupa el lugar 13 en la muestra ordenada, y su valor es
1.38. Obs√©rvese que la media es algo mayor que la mediana: esto es debido a la presencia de algunos
valores signicativamente m√°s altos que el resto, como pudimos ver en el histograma.
Por su parte, el P25 o cuantil 0.25 ocupa la posici√≥n 7, ya que el 25 % de 25 es 6.25. Por tanto, P25 = 0.82.
De igual forma, P75 = Q0.75 = 2.16, el valor que ocupa la posici√≥n 19. Podemos ver, por tanto, que los
valores m√°s bajos est√°n muy agrupados al principio, y se van dispersando m√°s conforme se hacen m√°s
altos.

2.5.3. Medidas de variaci√≥n o dispersi√≥n
Las

medidas de variaci√≥n o dispersi√≥n est√°n relacionadas con las medidas de tendencia central, ya que

lo que pretenden es cuanticar c√≥mo de concentrados o dispersos est√°n los datos respecto a estas medidas.
Nosotros nos vamos a limitar a dar medidas de dispersi√≥n asociadas a la media.
La idea de estas medidas es valorar en qu√© medida los datos est√°n agrupados en torno a la media. Esta cuesti√≥n
tan simple es uno de los motivos m√°s absurdos de la mala prensa que tiene la Estad√≠stica en la sociedad en
general. La gente no se f√≠a de lo que ellos llaman

la Estad√≠stica

entre otros motivos, porque parece que todo

el mundo cree que una media tiene que ser un valor v√°lido para todos, y eso es materialmente imposible.

Ejemplo. Pensemos en la media del salario de los espa√±oles. En 2005 fue de 18.750 euros al a√±o. Ahora bien,
esa media incluye tanto a las regiones m√°s desarrolladas como a las m√°s desfavorecidas y, evidentemente, la
cifra generar√° mucho malestar en gran parte de la poblaci√≥n (con toda seguridad, m√°s del 50 %), cuyo salario
est√° por debajo.

Ejemplo. Existe una frase muy conocida que dice que  la Estad√≠stica es el arte por el cu√°l si un espa√±ol se
come un pollo y otro no se come ninguno, se ha comido medio pollo cada uno .

Esa frase se usa en muchas

ocasiones para ridiculizar a la Estad√≠stica, cuando en realidad deber√≠a servir para desacreditar a quien la dice,
por su ignorancia.
Hay que decir que la Estad√≠stica no tiene la culpa de que la gente espere de una media m√°s de lo que es capaz
de dar, ni de que muy poca gente conozca medidas de dispersi√≥n asociadas a la media.

2.5.3.1. Varianza muestral
Dados unos datos de una variable cuantitativa, x1 , ..., xn , la

s2n‚àí1

28

Pn
=

varianza muestral2 de esos datos es
2

(xi ‚àí xÃÑ)
.
n‚àí1

i=1

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Nota. Para calcular a mano la varianza resulta m√°s c√≥modo desarrollar un poco su f√≥rmula, como vamos
a ver:

s2n‚àí1

Pn

‚àí xÃÑ)2
=
=
n‚àí1
Pn
x2 ‚àí nxÃÑ2
= i=1 i
.
n‚àí1
i=1 (xi

Pn

i=1

Pn
Pn
x2i ‚àí 2xÃÑ i=1 xi + nxÃÑ2
x2 ‚àí 2xÃÑnxÃÑ + nxÃÑ2
= i=1 i
n‚àí1
n‚àí1

Cuanto mayor sea la varianza de unos datos, m√°s dispersos, heterog√©neos o variables son esos datos. Cuanto
m√°s peque√±a sea una varianza de unos datos, m√°s agrupados u homog√©neos son dichos datos.

Ejemplo. Una muestra aleatoria simple de la altura de 5 personas arroja los siguientes resultados:
1.76

1.72

1.80

1.73

1.79

Calculemos su media y su varianza muestral.
P5
P5
Lo √∫nico que necesitamos es i=1 xi = 8.8 y i=1 x2i = 15.493. A partir de estos datos,

xÃÑ =
y

s2n‚àí1 =

8.8
= 1.76
5

15.493 ‚àí 5 √ó 1.762
= 0.00125
4

En lo que respecta al comportamiento de la varianza muestral frente a cambios de origen y escala, s√≥lo le
afectan los segundos. Es decir, si tenemos que y = ax + b, se verica que s2y;n‚àí1 = a2 s2x;n‚àí1 .
Finalmente, si bien hab√≠amos comentado que en el caso de la media, si tenemos la suma de varias variables,
la media total es la suma de las medias de cada variable, no ocurre as√≠ con la varianza en general.

2.5.3.2. Desviaci√≥n t√≠pica o estandar muestral
El principal problema de la varianza es su unidad de medida. Por c√≥mo se dene si, por ejemplo, la variable
se expresa en kilos, la media tambi√©n se expresa en kilos, pero la varianza se expresa en kilos2 , lo que hace
que sea dif√≠cil valorar si una varianza es muy elevada o muy peque√±a.

Es por ello que se dene la

desviaci√≥n t√≠pica o estandar muestral

de los datos como sn‚àí1 =

q
s2n‚àí1 ,

cuya unidad de medida es la misma que la de la media.

Prof. Dr. Antonio Jos√© S√°ez Castillo

29

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Nota. La Regla Emp√≠rica
Si el histograma asociado a unos datos tiene la forma de una campana o de una joroba, el conjunto de
datos tendr√° las siguientes caracter√≠sticas, lo que en algunos libros se conoce como

Regla Emp√≠rica:

1. Aproximadamente el 68 % de los datos estar√° en el intervalo (xÃÑ ‚àí sn‚àí1 , xÃÑ + sn‚àí1 ) .
2. Aproximadamente el 95 % de los datos estar√° en el intervalo (xÃÑ ‚àí 2sn‚àí1 , xÃÑ + 2sn‚àí1 ) .
3. Casi todos los datos estar√°n en el intervalo (xÃÑ ‚àí 3sn‚àí1 , xÃÑ + 3sn‚àí1 ) .

Figura 2.5: Representaci√≥n gr√°ca de la regla emp√≠rica.

2.5.3.3. Coeciente de variaci√≥n
Como acabamos de decir, debemos proporcionar cada media junto con alguna medida de dispersi√≥n, preferentemente la desviaci√≥n t√≠pica. Una forma de valorar en t√©rminos relativos c√≥mo es de dispersa una variable
es precisamente proporcionar el cociente entre la desviaci√≥n t√≠pica y la media (en valor absoluto), lo que se
conoce como

coeciente de variaci√≥n.

Dado un conjunto de datos de media xÃÑ y desviaci√≥n t√≠pica sn‚àí1 , se dene su coeciente

CV =

de variaci√≥n como

sn‚àí1
.
|xÃÑ|

La principal ventaja del coeciente de variaci√≥n es que no tiene unidades de medida, lo que hace m√°s f√°cil
su interpretaci√≥n.

30

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. Para los datos de tiempo de procesado en una CPU de 25 tareas, la varianza es 1.42, luego su
desviaci√≥n estandar es 1.19, y el coeciente de variaci√≥n

1.19
1.63

= 0.73. Por tanto, la desviaci√≥n est√°ndar es

algo m√°s del 70 % de la media. Esto indica que los datos no est√°n muy concentrados en torno a la media,
probablemente debido a la presencia de los valores altos que hemos comentado antes.

Nota.

El coeciente de variaci√≥n, tal y como est√° denido, s√≥lo tiene sentido para conjuntos de datos

con el mismo signo, es decir, todos positivos o todos negativos. Si hubiera datos de distinto signo, la
media podr√≠a estar pr√≥xima a cero o ser cero, imposibilitando que aparezca en el denominador.

Nota. Suele ser frecuente el error de pensar que el coeciente de variaci√≥n no puede ser mayor que 1, lo
cual es rigurosamente falso. Si lo expresamos en porcentaje, el coeciente de variaci√≥n puede ser superior
al 100 % sin m√°s que la desviaci√≥n t√≠pica sea mayor que la media, cosa bastante frecuente, por cierto.

Nota. A la hora de interpretar el coeciente de variaci√≥n inmediatamente surge la pregunta de ¬æcu√°ndo
podemos decir que es alto y cu√°ndo que es bajo?

Realmente, no existe una respuesta precisa, sino que

depende del contexto de los datos que estemos analizando. Si, por ejemplo, estamos analizando unos datos
que por su naturaleza deben ser muy homog√©neos, un coeciente de variaci√≥n del 10 % ser√≠a enorme, pero
si por el contrario estamos analizando datos que por su naturaleza son muy variables, un coeciente de
variaci√≥n del 10 % ser√≠a muy peque√±o.
Por todo ello, lo recomendable es analizar el coeciente de variaci√≥n entendiendo su signicado num√©rico,
es decir, entendiendo que se reere a la comparaci√≥n de la desviaci√≥n t√≠pica con la media, e interpretando
su valor en relaci√≥n al contexto en el que estemos trabajando.

2.5.4. Medidas de forma. Coeciente de asimetr√≠a
Las

medidas de forma comparan la forma que tiene la representaci√≥n gr√°ca, bien sea el histograma o el

diagrama de barras de la distribuci√≥n, con una situaci√≥n ideal en la que los datos se reparten en igual medida
a la derecha y a la izquierda de la media.
Esa situaci√≥n en la que los datos est√°n repartidos de igual forma a uno y otro lado de la media se conoce
como

simetr√≠a, y se dice en ese caso que la distribuci√≥n de los datos es sim√©trica. En ese caso, adem√°s, su

mediana, su moda y su media coinciden.
Por contra, se dice que una distribuci√≥n es asim√©trica

a la derecha si las frecuencias (absolutas o relativas)

descienden m√°s lentamente por la derecha que por la izquierda. Si las frecuencias descienden m√°s lentamente
por la izquierda que por la derecha diremos que la distribuci√≥n es
Para valorar la simetr√≠a de unos datos se suele utilizar el

asim√©trica a la izquierda.

coeciente de asimetr√≠a de Fisher:

Pn

3
i=1 (xi ‚àíxÃÑ)

As =
Prof. Dr. Antonio Jos√© S√°ez Castillo

n‚àí1

s3n‚àí1

.

31

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Obs√©rvese que para evitar el problema de la unidad y hacer que la medida sea escalar y por lo tanto relativa,
dividimos por el cubo de su desviaci√≥n t√≠pica. De esta forma podemos valorar si unos datos son m√°s o menos
sim√©tricos que otros, aunque no est√©n medidos en la misma unidad de medida. La interpretaci√≥n de este
coeciente de asimetr√≠a es la siguiente:
Tanto mayor sea el coeciente en valor absoluto, m√°s asim√©tricos ser√°n los datos.
El signo del coeciente nos indica el sentido de la asimetr√≠a:
¬à Si es positivo indica que la asimetr√≠a es a la derecha.
¬à Si es negativo, indica que la asimetr√≠a es a la izquierda.

Figura 2.6: Formas t√≠picas de distribuciones de datos.

Ejemplo. Para los datos de tiempo de procesado en una CPU de 25 tareas, el coeciente de asimetr√≠a
de Fisher es 0.91, lo que, como hab√≠amos visto y comentado con anterioridad, pone de maniesto que la
distribuci√≥n es asim√©trica a la derecha, debido a la presencia de tiempos de procesado bastante altos en
relaci√≥n al resto.

2.5.5. Par√°metros muestrales y par√°metros poblacionales
Cuando se trabaja con una muestra de una poblaci√≥n, ya sea √©sta tangible o conceptual, las distintas medidas
de posici√≥n, dispersi√≥n y forma, se denominan

par√°metros muestrales.

Hay que tener en cuenta que

pr√°cticamente siempre se trabaja con muestras, ya que o bien trabajamos con poblaciones conceptuales o
con poblaciones tangibles (nitas, por tanto), pero con much√≠simos elementos.
Frente a estos par√°metros muestrales se encuentran los par√°metros an√°logos referidos a toda la poblaci√≥n.
Estos par√°metros, llamados par√°metros

poblacionales, son, en general, imposibles de conocer3 . Por ejem-

plo, la media poblacional se calcular√≠a igual que la media muestral de unos datos, pero aplicada la f√≥rmula a
todos los elementos de la poblaci√≥n. Como eso es pr√°cticamente imposible de poner en la pr√°ctica, veremos
3 Salvo

32

en el caso de poblaciones nitas con pocos elementos.
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

en cap√≠tulos posteriores que los par√°metros muestrales se utilizan en la pr√°ctica para aproximar o estimar los
par√°metros poblacionales.

2.6. M√©todos para detectar datos cuantitativos at√≠picos o fuera de
rango
Hay ocasiones en que un conjunto de datos contiene una o m√°s observaciones inconsistentes en alg√∫n sentido.
Por ejemplo, en los datos de tiempo de procesado en una CPU de 25 tareas, supongamos que tenemos
una observaci√≥n m√°s, igual a 85, debido a que la CPU se bloque√≥ y hubo que reiniciarla. Este dato, que
probablemente no deseemos incluir, es un ejemplo de caso de dato at√≠pico o valor fuera de rango.
En general, una observaci√≥n que es inusualmente grande o peque√±a en relaci√≥n con los dem√°s valores de un
conjunto de datos se denomina

dato at√≠pico o fuera de rango.

Estos valores son atribuibles, por lo general, a una de las siguientes causas:
1. El valor ha sido introducido en la base de datos incorrectamente.
2. El valor proviene de una poblaci√≥n distinta a la que estamos estudiando.
3. El valor es correcto pero representa un suceso muy poco com√∫n.
A continuaci√≥n vamos a proponer dos maneras de determinar si un dato es un valor fuera de rango.

2.6.1. Mediante la regla emp√≠rica
Este m√©todo es adecuado si el histograma de los datos tiene forma de campana, en cuyo caso podemos aplicar
la regla emp√≠rica para detectar qu√© datos est√°n fuera de los rangos

l√≥gicos

seg√∫n esta regla.

Seg√∫n ella, el 99.5 % de los datos est√°n en el intervalo [xÃÑ ‚àí 3sn‚àí1 , xÃÑ + 3sn‚àí1 ], luego
at√≠picos los

xi

que no pertenezcan al intervalo

se considerar√°n datos

[xÃÑ ‚àí 3sn‚àí1 , xÃÑ + 3sn‚àí1 ] .

2.6.2. Mediante los percentiles
Supongamos que tenemos un conjunto de datos x1 , ..., xn . El procedimiento es el siguiente:
1. Se calculan los cuartiles primero y tercero, es decir, los percentiles 25 y 75, P25 y P75 . Se calcula el
llamado

rango intercuart√≠lico

2. Se consideran

(IR o

RI ),

IR = P75 ‚àí P25 .

datos at√≠picos aquellos inferiores a P25 ‚àí 1.5IR o superiores a P75 + 1.5IR.

Prof. Dr. Antonio Jos√© S√°ez Castillo

33

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Serie 1
Serie 2

Medias
92.01
92.74

Desv. T√≠pica
3.62
3.73

CV
25.40
24.86

Coef. Asimetr√≠a
-1.79
1.71

Cuadro 2.2: Resumen descriptivo de los datos de las placas de silicio

Ejemplo. Vamos a ver si hay alg√∫n dato at√≠pico entre los datos de tiempo de procesado en una CPU de
25 tareas.
Dado que el histograma no ten√≠a forma de campana, el m√©todo de la regla emp√≠rica no es el m√©todo m√°s
adecuado para la detecci√≥n de valores at√≠picos.
Por su parte, P50 = 1.38, P25 = 0.82 y P75 = 2.16. Por tanto, IR = 2.16‚àí0.82 = 1.34, y el intervalo fuera
del c√∫al consideramos valores fuera de rango es [0.82 ‚àí 1.5 √ó 1.34, 2.16 + 1.5 √ó 1.34] = [‚àí1.19, 4.17]. De
esta forma, el valor 4.75 es un valor fuera de rango.
Hay una versi√≥n gr√°ca de este m√©todo para detectar valores at√≠picos mediante los percentiles: se llama

diagrama de caja o diagrama de cajas y bigotes o (en ingl√©s) boxplot. Este diagrama incluye en un
gr√°co:
1. El valor de la mediana (o segundo cuartil, Q2 ): ese es el centro de la caja.

2. El valor de los percentiles 25 y 75, cuartiles primero y tercero respectivamente (Q1 y Q3 ): son los lados
inferior y superior de la caja.
3. El diagrama no representa los l√≠mites P25 ‚àí 1.5 √ó IR y P75 + 1.5 √ó IR. En su lugar, se√±ala los √∫ltimos
puntos no at√≠picos por debajo (Li ) y por encima (Ls ), es decir, se√±ala el √∫ltimo dato por encima de

P25 ‚àí 1.5 √ó IR y el √∫ltimo dato por debajo de P75 + 1.5 √ó IR, y los representa como

bigotes

que salen

de la caja.
4. Normalmente representa con c√≠rculos los datos at√≠picos.

2.7. Sobre el ejemplo de las capas de di√≥xido de silicio
Ya estamos en condiciones de responder en parte a las cuestiones que quedaron latentes en el tema de
introducci√≥n sobre el ejemplo de las placas de silicio.
Vamos a comenzar realizando un resumen descriptivo de los datos, separando por series, proporcionando
media, desviaci√≥n t√≠pica, coeciente de variaci√≥n y coeciente de asimetr√≠a. Todos estos resultados aparecen
en la Tabla 2.2.
En primer lugar, es cierto que, como apunt√°bamos en el tema de introducci√≥n, los valores est√°n en torno a 90
(la media es 92 m√°s o menos). Adem√°s, vemos que s√≠ que hay una variabilidad moderada de los datos, con un
CV en torno al 25 %, lo que indica que, al parecer, las distintas condiciones en que cada medici√≥n se realiz√≥,
afectaron en alguna medida el resultado: todo esto es muy preliminar porque no tenemos la informaci√≥n
completa de en qu√© condiciones se realizaron cada una de las mediciones. Por el contrario, podemos observar
algo muy llamativo. Los datos de la primera serie son claramente asim√©tricos a la izquierda (coeciente de

34

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 2.7: Descripci√≥n de un diagrama de caja. Fuente: http://es.wikipedia.org/wiki/Diagrama_de_caja
asimetria de -1.79), mientras que los de la segunda serie son claramente asim√©tricos a la derecha (coeciente
de asimetr√≠a de 1.71). Dado que no era esperable que surgieran diferencias entre las dos series, debemos
preguntarnos qu√© pas√≥.
Para tratar de analizar m√°s profundamente los datos, vamos a proporcionar tambi√©n los dos diagramas de
caja de ambas series. Aparecen en la Figura 2.8. Con ellas, vamos a resumir ahora las decisiones que los
autores tomaron en vista de los resultados y las conclusiones a las que llegaron.
Obs√©rvese que las diferencias entre las series no afectan sorprendentemente al conjunto de las muestras, sino
s√≥lo a los valores at√≠picos que se ven en ambos diagramas de caja. Eso probar√≠a que, en efecto, no hay ninguna
diferencia sistem√°tica entre las series.
La siguiente tarea es la de inspeccionar los datos at√≠picos. Si miramos con atenci√≥n los datos, vemos que las
8 mediciones m√°s grandes de la segunda serie ocurrieron en la placa 10. Al ver este hecho, los autores del
trabajo inspeccionaron esta placa y descubrieron que se hab√≠a contaminado con un residuo de la pel√≠cula, lo
que ocasion√≥ esas mediciones tan grandes del espesor. De hecho, los ingenieros eliminaron esa placa y toda
la serie entera por razones t√©cnicas. En la primera serie, encontraron tambi√©n que las tres mediciones m√°s
bajas se hab√≠an debido a un calibrador mal congurado, por lo que las eliminaron. No se pudo determinar
causa alguna a la existencia de los dos datos at√≠picos restantes, por lo que permanecieron en el an√°lisis. Por
√∫ltimo, n√≥tese que despu√©s de este proceso de depuraci√≥n de los datos que el an√°lisis mediante Estad√≠stica
Descriptiva ha motivado, la distribuci√≥n de los datos tiene una evidente forma de campana.
Prof. Dr. Antonio Jos√© S√°ez Castillo

35

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 2.8: Diagramas de caja de los datos del espesor de las capas de di√≥xido de silicio

36

Prof. Dr. Antonio Jos√© S√°ez Castillo

Parte II
C√°lculo de Probabilidades

37

Cap√≠tulo 3
Probabilidad

Vemos que la teor√≠a de la probabilidad en el fondo s√≥lo es sentido com√∫n reducido a c√°lculo; nos
hace apreciar con exactitud lo que las mentes razonables toman por un tipo de instinto, incluso
sin ser capaces de darse cuenta[...] Es sorprendente que esta ciencia, que surgi√≥ del an√°lisis de los
juegos de azar, llegara a ser el objeto m√°s importante del conocimiento humano[...] Las principales
cuestiones de la vida son, en gran medida, meros problemas de probabilidad.
Pierre Simon, Marqu√©s de Laplace

Resumen. El cap√≠tulo proporciona un tratamiento de los experimentos cuyos resultados no se pueden predecir
con certeza a trav√©s del concepto de probabilidad. Se analizan las propiedades de la probabilidad y se introduce
tambi√©n el concepto de probabilidad condicionada, que surge cuando un suceso modica la asignaci√≥n de
probabilidades previa.

Palabras clave: experimento aleatorio, experimento determin√≠stico, espacio muestral, suceso, probabilidad,
probabilidad condicionada, independencia de sucesos.

3.1. Introducci√≥n
En nuestra vida cotidiana asociamos usualmente el concepto de

Probabilidad a su calicativo probable,

probables aquellos eventos en los que tenemos un alto grado de creencia en su ocurrencia.
En esta l√≠nea, Probabilidad es un concepto asociado a la medida del azar. Tambi√©n pensamos en el azar

considerando

vinculado, fundamentalmente, con los juegos de azar, pero desde esa √≥ptica tan reducida se nos escapan otros
much√≠simos ejemplos de fen√≥menos de la vida cotidiana o asociados a disciplinas de distintas ciencias donde
el azar juega un papel fundamental. Por citar algunos:
¬æQu√© n√∫mero de unidades de producci√≥n salen cada d√≠a de una cadena de montaje? No existe un n√∫mero
jo que pueda ser conocido a priori, sino un conjunto de posibles valores que podr√≠an darse, cada uno
de ellos con un cierto grado de certeza.
¬æCu√°l es el tama√±o de un paquete de informaci√≥n que se transmite a trav√©s de HTTP? No existe en
realidad un n√∫mero jo, sino que √©ste es desconocido a priori.
39

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

¬æCu√°l es la posici√≥n de un objeto detectado mediante GPS? Dicho sistema obtiene, realmente, una
estimaci√≥n de dicha posici√≥n, pero existen m√°rgenes de error que determinan una regi√≥n del plano
donde el objeto se encuentra con alta probabilidad.
¬æQu√© ruido se adhiere a una se√±al que se env√≠a desde un emisor a un receptor? Dependiendo de las
caracter√≠sticas del canal, dicho ruido ser√° m√°s o menos relevante, pero su presencia no podr√° ser conocida
a priori, y deber√° ser diferenciada de la se√±al primitiva, sin que se conozca √©sta, teniendo en cuenta que
se trata de un ruido

aleatorio.

En todos estos ejemplos el azar es un factor insoslayable para conocer el comportamiento del fen√≥meno en
estudio.

3.2. Experimentos aleatorios y experimentos determin√≠sticos
En general, un experimento del que se conocen todos sus posibles resultados y que, repetido en las mismas
condiciones, no siempre proporciona los mismos resultados se conoce como
En contraposici√≥n, un

experimento aleatorio.

experimento determin√≠stico es aquel donde las mismas condiciones aseguran que

se obtengan los mismos resultados.
Lo que el C√°lculo de Probabilidades busca es encontrar una medida de la incertidumbre o de la certidumbre
que se tiene de todos los posibles resultados, ya que jam√°s (o muy dif√≠cilmente) se podr√° conocer a priori
el resultado de cualquier experimento donde el azar est√© presente: a esta medida de la incertidumbre la
denominaremos

1.

probabilidad

3.3. Denici√≥n de probabilidad
Tenemos, por tanto, que probabilidad es la asignaci√≥n que hacemos del grado de creencia que tenemos sobre
la ocurrencia de algo. Esta asignaci√≥n, sin embargo, debe ser

coherente.

Esta necesidad de que asignemos

probabilidades adecuadamente se va a plasmar en esta secci√≥n en tres reglas, conocidas como

axiomas,

que

debe cumplir cualquier reparto de probabilidades.

3.3.1. √Ålgebra de conjuntos
Si consideramos un experimento aleatorio, podemos caracterizar los posibles resultados de dicho experimento
como conjuntos. Es de inter√©s, por tanto, repasar los conceptos y propiedades b√°sicas del √°lgebra de conjuntos.
En todo este apartado no debemos olvidar que los conjuntos representan en nuestro caso los posibles resultados
de un experimento aleatorio.

conjunto es una colecci√≥n de elementos.
Se dice que B es un subconjunto de A si todos sus elementos lo son tambi√©n de A, y se notar√° B ‚äÇ A.

Un

1 Es mejor que aceptemos desde el principio que la Estad√≠stica no es la ciencia de la adivinaci√≥n: tan s√≥lo se ocupa de
cuanticar c√≥mo de incierto es un evento y, ocasionalmente, de proponer estrategias de predicci√≥n basadas en dicha medida de
la incertidumbre.

40

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Para cada A se verica ‚àÖ ‚äÇ A ‚äÇ A ‚äÇ ‚Ñ¶.
Si C ‚äÇ B y B ‚äÇ A, entonces, C ‚äÇ A. Esto se conoce como propiedad transitiva.
La

uni√≥n

de B y A es un conjunto cuyos elementos son los elementos de A y B , y se nota A ‚à™ B . Esta

operaci√≥n verica la propiedad conmutativa y asociativa.
Si A ‚äÇ B , entonces A ‚à™ B = B.
La

intersecci√≥n

de A y B es el conjunto formado por los elementos comunes de A y B , y se nota AB o

A ‚à© B. Esta operaci√≥n verica la propiedad conmutativa, asociativa y distributiva respecto de la uni√≥n.
Dos conjuntos, A y B , se dicen

mutuamente excluyentes, disjuntos o incompatibles si su intersecci√≥n

es vac√≠a, es decir, A ‚à© B = ‚àÖ.
Si dos conjuntos A y B son disjuntos, su uni√≥n suele notarse A + B .
Los conjuntos A1 , ..., AN se dicen
Una

partici√≥n

mutuamente excluyentes si Ai ‚à© Aj = ‚àÖ para todo i 6= j.

es una colecci√≥n de conjuntos, A1 , ..., AN tal que:

a) A1 ‚à™ ... ‚à™ AN = ‚Ñ¶
b) Ai ‚à© Aj = ‚àÖ para todo i 6= j.
El

conjunto complementario de un conjunto A, AÃÑ √≥ Ac , est√° formado por todos los elementos de ‚Ñ¶ que

no pertenecen a A.
Se sigue por tanto,

A ‚à™ AÃÑ = ‚Ñ¶
A ‚à© AÃÑ = ‚àÖ
c

(Ac ) = A
‚Ñ¶ÃÑ = ‚àÖ
Si B ‚äÇ A ‚Üí AÃÑ ‚äÇ BÃÑ
Si A = B ‚Üí AÃÑ = BÃÑ.
Finalmente, mencionemos las llamadas Leyes de Morgan:

A ‚à™ B = AÃÑ ‚à© BÃÑ

A ‚à© B = AÃÑ ‚à™ BÃÑ.

3.3.2. Espacio muestral
Consideremos un experimento aleatorio.
Prof. Dr. Antonio Jos√© S√°ez Castillo

41

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

El conjunto formado por todos los posibles resultados del experimento aleatorio recibe el nombre de espacio

muestral, y lo notaremos habitualmente como ‚Ñ¶.

Cualquier subconjunto de un espacio muestral recibe el nombre de
Hablaremos de

ensayo o realizaci√≥n

suceso o evento.

de un experimento aleatorio reri√©ndonos a una ejecuci√≥n de dicho

experimento. As√≠, diremos que en un ensayo

ocurre un suceso A

si se observa en dicho ensayo cualquier

resultado incluido en el suceso A.
Una observaci√≥n importante es que el espacio muestral no tiene por qu√© ser √∫nico, sino que depender√° de lo
que deseemos observar del experimento aleatorio. Vamos a poner este hecho de maniesto en los siguientes
ejemplos.

Ejemplo. Si consideramos el lanzamiento de un dado, un espacio muestral ser√≠a ‚Ñ¶={1,2,3,4,5,6}.
Los sucesos m√°s elementales posibles son {1}, {2}, {3}, {4}, {5} y {6}. Otros sucesos no elementales
pueden ser {1,2}, {mayor que 2}, {par}, ...
Sin embargo, supongamos que estamos lanzando un dado porque no tenemos ninguna moneda a mano, y
s√≥lo deseamos ver si el resultado es par o impar. En ese caso, el espacio muestral ser√≠a ‚Ñ¶ = {par, impar}.

Ejemplo. Un experimento habitual en Biolog√≠a consiste en extraer, por ejemplo, peces de un r√≠o, hasta
dar con un pez de una especie que se desea estudiar. El n√∫mero de peces que habr√≠a que extraer hasta
conseguir el ejemplar deseado de la especie en estudio formar√≠a el espacio muestral, ‚Ñ¶ = {1, 2, 3, ...}, si es
que el investigador desea observar exactamente el n√∫mero de peces hasta extraer ese ejemplar deseado.
Obs√©rvese que se trata de un conjunto no acotado, pero numerable.
Como ejemplos de posibles sucesos de inter√©s podr√≠amos poner los eventos {1,2,3,4,5}, {mayor o igual a
5},...
Supongamos ahora que el investigador s√≥lo est√° interesado en comprobar si hacen falta m√°s de 5 extracciones para obtener un ejemplar de la especie en estudio. En ese caso, el espacio muestral ser√≠a

‚Ñ¶ = {> 5, ‚â§ 5}.

Ejemplo.

Si consideramos el experimento aleatorio consistente en elegir un n√∫mero absolutamente al

azar entre 0 y 1, un espacio muestral ser√≠a ‚Ñ¶ = [0, 1]. A diferencia de los anteriores ejemplos, este espacio
muestral no es nito, ni siquiera numerable.
Como ejemplo de sucesos posibles en este espacio muestral podemos destacar, entre otros, {menor que
0.5} , {mayor que 0.25}, {menor que 0.75} ,...
Otro espacio muestral podr√≠a ser observar el valor decimal mayor m√°s cercano. Por ejemplo, si sale 0.25,
me interesa 0.3. En ese caso el espacio muestral ser√≠a ‚Ñ¶ = 0.1, 0.2, ...1. Este espacio muestral servir√≠a,
por ejemplo, para sortear n√∫meros entre 1 y 10, sin m√°s que multiplicar el resultado obtenido por 10.

42

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

En estos √∫ltimos ejemplos podemos ver que hay dos grandes tipos de espacios muestrales seg√∫n el n√∫mero de
sucesos elementales.
Un espacio muestral se dice

discreto si est√° formado por un conjunto nito o innito numerable de sucesos

elementales.
Por el contrario, un espacio muestral se dice

continuo

si est√° formado por un conjunto no numerable de

sucesos elementales.

3.3.3. Funci√≥n de probabilidad
Dado un espacio muestral ‚Ñ¶ correspondiente a un experimento aleatorio, una

funci√≥n de probabilidad

para ese espacio muestral es cualquier funci√≥n que asigne a cada suceso un n√∫mero en el intervalo [0, 1] y que
verique

P [A] ‚â• 0, para cualquier evento A.
P [‚Ñ¶] = 1.
Dada una colecci√≥n de sucesos A1 , A2 , ..., An mutuamente excluyentes, es decir, tales que Ai ‚à© Aj = ‚àÖ para
todo i 6= j,

P [‚à™ni=1 Ai ] =

n
X

P [Ai ] .

i=1

Nota. Hay que notar que se puede dar m√°s de una funci√≥n de probabilidad asociada al mismo espacio
muestral. Por ejemplo, asociado al espacio muestral ‚Ñ¶ = {cara, cruz}, del lanzamiento de una moneda,
pueden darse un n√∫mero innito no numerable de medidas de la probabilidad; concretamente, asociadas
a cada elecci√≥n

P [cara] = p
P [cruz] = 1 ‚àí p,
para cada p ‚àà [0, 1] . Aunque si la moneda no est√° cargada, como sucede habitualmente, se considera el
caso en que p = 12 .

Ejemplo. Volviendo sobre el lanzamiento del dado, si √©ste no est√° cargado, podemos denir la siguiente
funci√≥n de probabilidad:

P [{i}] =

Prof. Dr. Antonio Jos√© S√°ez Castillo

1
, i = 1, 2, ..., 6.
6

43

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 3.1: Circuito

En ese caso, podemos, a su vez, calcular algunas probabilidades. Por ejemplo,

P ({par}) = P [{2, 4, 6}]
= P [{2}] + P [{4}] + P [{6}]
1 1 1
= + + = 0.5.
6 6 6
En este c√°lculo se ha tenido en cuenta la tercera condici√≥n de la denici√≥n axiom√°tica.
Como consecuencia de la denici√≥n se verican, entre otras, las siguientes propiedades, que adem√°s facilitan
bastante los c√°lculos:

P [‚àÖ] = 0.
 
Sea A un suceso cualquiera. Entonces, P AÃÑ = 1 ‚àí P [A] .


Sean A y B dos sucesos cualesquiera. Entonces, P A ‚à© BÃÑ = P [A] ‚àí P [A ‚à© B] .
Sean A y B dos sucesos cualesquiera. Entonces, P [A ‚à™ B] = P [A] + P [B] ‚àí P [A ‚à© B] .

Ejemplo. El circuito que aparece en la Figura 3.1 est√° constituido por dos interruptores (switches ) en
paralelo. La probabilidad de que cualquiera de ellos est√© cerrado es de 12 .
Para que pase corriente a trav√©s del circuito basta con que pase corriente por alguno de los dos interruptores, esto es, que al menos uno de ellos est√© cerrado. Por tanto, si notamos por
corriente a trav√©s del circuito

y

Ei

al suceso

que el interruptor

i

est√© cerrado,

E

al suceso

que pase

entonces,

P [E] = P [E1 ‚à™ E2 ] = P [E1 ] + P [E2 ] ‚àí P [E1 ‚à© E2 ]
1 1
= + ‚àí P [E1 ‚à© E2 ] ‚â§ 1.
2 2
Para conocer esta probabilidad de forma exacta necesitamos saber c√≥mo act√∫an de forma conjunta ambos
circuitos.

44

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

N¬∫ de lanzamientos
N¬∫ de caras
N. de caras
N. de lanzamientos

10
4
0.4

100
46
0.46

250
124
0.496

500
244
0.488

750
379
0.5053

1000
501
0.501

Cuadro 3.1: Aproximaci√≥n frecuentista a la probabilidad de cara en el lanzamiento de una moneda.

3.4. Interpretaci√≥n frecuentista de la probabilidad
La interpretaci√≥n m√°s com√∫n al concepto de probabilidad tiene que ver con los promedios de ocurrencia de
los sucesos del experimento en cuesti√≥n.
Pensemos en el lanzamiento de una moneda: si decimos que la probabilidad de cara es 0.5, entendemos que
si lanzamos la moneda un gran n√∫mero de veces y anotamos el n√∫mero de caras, √©stas ser√°n m√°s o menos la
mitad.
Generalizando este proceso, podr√≠amos decir que la probabilidad de un evento A, P [A] , es

nA
,
n‚Üí‚àû n

P [A] = lƒ±ÃÅm

donde nA es el n√∫mero de ocurrencias de A en n ensayos del experimento.
Esta interpretaci√≥n se conoce como

denici√≥n frecuentista de la probabilidad.

Se trata de una interpretaci√≥n

de car√°cter eminentemente pr√°ctico porque permite una aproximaci√≥n f√≠sica al concepto de probabilidad,
pero se ve limitada por las complicaciones que supone la denici√≥n en t√©rminos de un l√≠mite que, como tal,
s√≥lo se alcanza

en el innito.

Adem√°s, desde un punto de vista realista, ¬æen qu√© ocasiones podremos repetir

el experimento un gran n√∫mero de veces?

Ejemplo. Se han realizado 1000 lanzamientos de una moneda. En el Cuadro 3.1 aparece un resumen de ese
proceso. Puede observarse como cuanto mayor es el n√∫mero de lanzamientos, m√°s se aproxima la frecuencia
relativa al valor 21 , de manera que podr√≠amos pensar que la probabilidad de cara es igual que la probabilidad
de cruz e iguales ambas a

1
2,

aunque esto s√≥lo es una suposici√≥n, o una aproximaci√≥n, ya que para aplicar

estrictamente la denici√≥n frecuentista deber√≠amos continuar hasta el innito, lo que resulta imposible.
Esta interpretaci√≥n frecuentista de la probabilidad permite inferir lo que podemos llamar
radas.

frecuencias espe-

Si un evento A tiene asignada una probabilidad P [A], entonces, si repetimos el experimento aleatorio

n veces,

lo m√°s esperable

es que el n√∫mero de veces que se de el evento A ser√° n √ó P [A] . M√°s adelante

podremos matizar con m√°s rigor a qu√© nos referimos con

lo m√°s esperable.

Ejemplo. Siguiendo con el ejemplo de la moneda, si la lanzamos 348 veces, lo esperable es que salgan
alrededor de 348 √ó 0.5 = 174 caras.

3.5. Interpretaci√≥n subjetiva de la probabilidad
Si nos dicen que la probabilidad de que llueva ma√±ana es del 35 %, ¬æc√≥mo podemos interpretar eso en t√©rminos
frecuentistas? No tiene sentido pensar en que podemos repetir el experimento d√≠a

de ma√±ana

muchas veces y

contar cu√°ntas veces llueve. ¬æPodr√≠amos pensar si hubiera muchos d√≠as como el de ma√±ana, aproximadamente
llover√≠a en el 35 % de ellos ?

Pero eso no tiene sentido porque el d√≠a de ma√±ana es √∫nico.

Prof. Dr. Antonio Jos√© S√°ez Castillo

45

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

La interpretaci√≥n subjetiva de la probabilidad tiene que ver con la vinculaci√≥n de este concepto con el grado
de incertidumbre que tenemos sobre las cosas. Si tenemos un experimento aleatorio, el resultado de dicho
experimento es incierto. La probabilidad de un resultado del experimento es el grado de creencia que yo tengo
en la ocurrencia de dicho resultado. Ese grado de creencia es personal, luego es subjetivo, pero l√≥gicamente,
deber√° estar acorde con la informaci√≥n que tenemos sobre el experimento.

3.6. Espacio muestral con resultados equiprobables. F√≥rmula de Laplace
Otro punto de vista que permite abordar el proceso de asignaci√≥n de probabilidad a sucesos es el siguiente:
continuando con el ejemplo de la moneda, en este experimento son dos los resultados posibles, y no hay razones
para pensar que uno de ellos es

m√°s probable

que otro, as√≠ que tiene sentido considerar que la probabilidad

de cara y la probabilidad de cruz son ambas del 50 %.
En general, si el espacio muestral est√° formado por N resultados posibles y todos ellos tienen la misma
probabilidad (equiprobables), podr√≠amos decir que la probabilidad de un evento A, P [A] , es

P [A] =

NA
,
N

donde NA es el n√∫mero de resultados favorables a la ocurrencia de A.
Esta f√≥rmula, conocida como

f√≥rmula de Laplace

tambi√©n es fundamentalmente pr√°ctica. Por ejemplo, nos

permite deducir que

P [cara] =

1
2

en el lanzamiento de una moneda sin tener que lanzar la moneda un gran n√∫mero de veces.
Sin embargo, la denici√≥n tiene dos grandes inconvenientes: el conjunto de resultados posibles, N , tiene que
ser nito y, adem√°s, todos los resultados posibles deben tener la misma probabilidad (con lo cual, lo denido
queda impl√≠citamente inmerso en la denici√≥n).

3.7. Probabilidad condicionada. Independencia de sucesos
Para introducir de manera intuitiva el concepto de probabilidad condicionada debemos pensar en la probabilidad como medida de la creencia en la ocurrencia de los sucesos.
Pensemos en un experimento aleatorio y en un suceso de dicho experimento, A, en el que, en principio,
tenemos un grado de creencia P [A] ; pero supongamos que conocemos algo del resultado de dicho experimento;
concretamente, sabemos que ha ocurrido un suceso B . Parece l√≥gico pensar que esa informaci√≥n conocida
sobre el resultado del ensayo modicar√° nuestro grado de creencia en A: llamemos a este nuevo grado de
creencia P [A | B],

probabilidad de A conocida B o probabilidad de A condicionada a B .

Ejemplo. Consideremos el suceso A : el d√≠a de hoy va a llover y el suceso B

: el d√≠a de hoy est√° nublado.

Obviamente, la probabilidad P [A] ser√° menor que la probabilidad P [A | B] , ya que el hecho de que est√©
nublado refuerza nuestra creencia en que llueva.

46

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. Consideremos el experimento aleatorio de extraer una carta de una baraja espa√±ola. Sea el suceso
A : obtener una sota, el suceso B1 : obtener una gura y el suceso B2 : obtener una carta de copas.
Las distintas probabilidades, condicionadas o no, bajo la denici√≥n cl√°sica, son las siguientes:

4 sotas
1
=
40 cartas
10
4 sotas
1
P [A | B1 ] =
=
12 f iguras
3
1 sota de copas
1
P [A | B2 ] =
=
.
10 copas
10
P [A] =

Como puede verse, B1 modica la probabilidad a priori, pero no as√≠ B2 . Puede decirse que B2 no ofrece

independientes.
denici√≥n de probabilidad condicionada

informaci√≥n acerca de A, o que A y B2 son
Vamos a dar a continuaci√≥n una

que responde a esta idea de

recalcular la probabilidad en funci√≥n de la informaci√≥n existente.
La

probabilidad condicionada de un suceso A, conocido otro suceso B , denotada por P [A | B], se

dene como el cociente

P [A | B] =

P [A ‚à© B]
,
P [B]

siempre que P [B] 6= 0.
Una funci√≥n de probabilidad condicionada P [¬∑/B ] es una funci√≥n de probabilidad en toda regla: por tanto,
cumple las mismas propiedades que cualquier funci√≥n de probabilidad sin condicionar.
Como hemos comentado, la idea de la probabilidad condicionada es utilizar la informaci√≥n que nos da un
suceso conocido sobre la ocurrencia de otro suceso. Pero, como ya hemos puesto de maniesto en un ejemplo,

no siempre un suceso da informaci√≥n sobre otro. En este caso se dice que ambos sucesos son independientes.
Por tanto:
Dos sucesos A y B se dicen independientes si P [A | B] = P [A] , o equivalentemente si P [B | A] = P [B], o
equivalentemente si P [A ‚à© B] = P [A] √ó P [B] .

Ejemplo. Continuando con el Ejemplo 3.3.3, lo m√°s l√≥gico es pensar que los dos interruptores act√∫an
de forma independiente, en cuyo caso P [E1 ‚à© E2 ] = P [E1 ] P [E2 ] y tenemos que,

1 1
+ ‚àí P [E1 ‚à© E1 ]
2 2
1 1 11
3
= + ‚àí
= .
2 2 22
4

P [E] =

Nota. Es muy importante no confundir la probabilidad condicionada de un suceso a otro con la probabilidad de la intersecci√≥n de ambos sucesos. En la Figura 3.2 puede verse la diferencia entre las probabilidades
condicionadas entre dos sucesos y la probabilidad de su intersecci√≥n. En t√©rminos coloquiales, podemos

Prof. Dr. Antonio Jos√© S√°ez Castillo

47

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

analizar estas probabilidades como el cociente entre
condicionada ese

todo

una parte

y

un todo.

Cuando la probabilidad es

es el suceso que condiciona. Cuando la probabilidad no es condicionada, ese

es todo el espacio muestral. En ambos casos esa

parte

todo

es la intersecci√≥n.

Figura 3.2: Esquema acerca de la denici√≥n de probabilidad condicionada.

Nota. Tambi√©n suele ser bastante com√∫n la confusi√≥n entre sucesos independientes y sucesos incompatibles o mutuamente excluyentes.
En este sentido, recordemos que dos sucesos A y B son incompatibles o mutuamente excluyentes si

A ‚à© B = ‚àÖ, en cuyo caso P [A ‚à© B] = 0.
Por su parte, A y B ser√°n independientes si P [A ‚à© B] = P [A] P [B].
Las diferencias entre ambos conceptos son obvias.

Ejemplo. La probabilidad de que el producto no sea elaborado a tiempo es 0.05. Se solicitan tres pedidos
del producto con la suciente separaci√≥n en el tiempo como para considerarlos eventos independientes.
1. ¬æCu√°l es la probabilidad de que todos los pedidos se env√≠en a tiempo?
En primer lugar, notemos Ei al suceso enviar

a tiempo el pedido i-√©simo.

En ese caso, sabemos que

P [Ei ] = 0.95.
Por su parte, nos piden

P [E1 ‚à© E2 ‚à© E3 ] = P [E1 ] P [E2 ] P [E3 ] = 0.953 ,
debido a que los pedidos son independientes.
2. ¬æCu√°l es la probabilidad de que exactamente un pedido no se env√≠e a tiempo?

48

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

En este caso el suceso que nos piden es m√°s complejo:



P EÃÑ1 ‚à© E2 ‚à© E3 ‚à™ E1 ‚à© EÃÑ2 ‚à© E3 ‚à™ E1 ‚à© E2 ‚à© EÃÑ3







= P EÃÑ1 ‚à© E2 ‚à© E3 + P E1 ‚à© EÃÑ2 ‚à© E3 + P E1 ‚à© E2 ‚à© EÃÑ3

= 0.05 √ó 0.952 + 0.05 √ó 0.952 + 0.05 √ó 0.952 = 0.135,
donde se ha utilizado que los sucesos EÃÑ1 ‚à© E2 ‚à© E3 , E1 ‚à© EÃÑ2 ‚à© E3 y E1 ‚à© E2 ‚à© EÃÑ3 son incompatibles.
3. ¬æCu√°l es la probabilidad de que dos o m√°s pedidos no se env√≠en a tiempo?
Tengamos en cuenta que ya hemos calculado la probabilidad de que todos se env√≠en a tiempo y de
que todos menos uno se env√≠en a tiempo. Entonces,

P [dos o m√°s pedidos no se env√≠en a tiempo]

= 1 ‚àí P [todos se env√≠en a tiempo ‚à™ un pedido no se env√≠e a tiempo]
= 1 ‚àí (0.953 + 0.135).

Ejemplo.

Consideremos un proceso industrial como el que se esquematiza en la Figura 3.3. En dicho

esquema se pone de maniesto que una unidad ser√° producidad con √©xito si pasa en primer lugar un
chequeo previo (A); despu√©s puede ser montada directamente (B), redimensionada (C) y despu√©s montada
(D) o adaptada (E) y despu√©s montada (F); posteriormente debe ser pintada (G) y nalmente embalada
(H). Consideremos que las probabilidades de pasar exitosamente cada subproceso son todas ellas iguales
a 0.95, y que los subprocesos tienen lugar de forma independiente unos de otros. Vamos a calcular en
esas condiciones la probabilidad de que una unidad sea exitosamente producida.
Si nos damos cuenta, A, G y H son ineludibles, mientras que una unidad puede ser producida si pasa
por B, por C y D o por E y F. En notaci√≥n de conjuntos, la unidad ser√° producida si se da

A ‚à© (B ‚à™ C ‚à© D ‚à™ E ‚à© F ) ‚à© G ‚à© H.
Como los procesos son independientes unos de otros, no tenemos problemas con las probabilidades de las
intersecciones, pero tenemos que calcular la probabilidad de una uni√≥n de tres conjuntos, B‚à™C ‚à©D‚à™E‚à©F .
En general,

P [A1 ‚à™ A2 ‚à™ A3 ] = P [(A1 ‚à™ A2 ) ‚à™ A3 ] = P [A1 ‚à™ A2 ] + P [A3 ] ‚àí P [(A1 ‚à™ A2 ) ‚à© A3 ]
= P [A1 ] + P [A2 ] ‚àí P [A1 ‚à© A2 ] + P [A3 ] ‚àí P [A1 ‚à© A3 ‚à™ A2 ‚à© A3 ]

Prof. Dr. Antonio Jos√© S√°ez Castillo

49

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

= P [A1 ] + P [A2 ] ‚àí P [A1 ‚à© A2 ] + P [A3 ]
‚àí (P [A1 ‚à© A3 ] + P [A2 ‚à© A3 ] ‚àí P [A1 ‚à© A2 ‚à© A3 ])

= P [A1 ] + P [A2 ] + P [A3 ]
‚àí P [A1 ‚à© A2 ] ‚àí P [A1 ‚à© A3 ] ‚àí P [A2 ‚à© A3 ]
+ P [A1 ‚à© A2 ‚à© A3 ]
En nuestro caso,

P [B ‚à™ C ‚à© D ‚à™ E ‚à© F ] = P [B] + P [C ‚à© D] + P [E ‚à© F ]
‚àí P [B ‚à© C ‚à© D] ‚àí P [B ‚à© E ‚à© F ] ‚àí P [C ‚à© D ‚à© E ‚à© F ]
+ P [B ‚à© C ‚à© D ‚à© E ‚à© F ]
= 0.95 + 2 √ó 0.952 ‚àí 2√ó0.953 ‚àí 0.954 + 0.955
= 0.9995247
Ya estamos en condiciones de obtener la probabilidad que se nos pide:

P [A ‚à© (B ‚à™ C ‚à© D ‚à™ E ‚à© F ) ‚à© G ‚à© H] = P [A] P [B ‚à™ C ‚à© D ‚à™ E ‚à© F ] P [G] P [H]
= 0.95 √ó (0.9995247) √ó 0.95 √ó 0.95
= 0.8569675.

En estos ejemplos, el c√°lculo de la probabilidad de las intersecciones ha resultado trivial porque los sucesos son
independientes. Son embargo, esto no siempre ocurre. ¬æC√≥mo podemos, en general, obtener la probabilidad
de la intersecci√≥n de dos o m√°s sucesos no necesariamente independientes?
En el caso de s√≥lo dos sucesos, A y B , podemos deducir que

P [A ‚à© B] = P [A|B] √ó P [B]
directamente de la denici√≥n de probabilidad condicionada. A partir de esta f√≥rmula, por inducci√≥n, se puede
obtener la llamada f√≥rmula producto, que se enuncia de la siguiente forma: si A1 , A2 , ..., An son sucesos de
un espacio muestral no necesariamente independientes, se verica

P [A1 ‚à© A2 ‚à© ... ‚à© An ] = P [A1 ]P [A2 |A1 ]...P [An |A1 ‚à© A2 ‚à© ... ‚à© An‚àí1 ]

50

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 3.3: Esquema del proceso industrial del ejemplo

Ejemplo. Un lote de 50 arandelas contiene 30 arandelas cuyo grosor excede las especicaciones de dise√±o.
Suponga que se seleccionan 3 arandelas al azar y sin reemplazo del lote.
1. ¬æCu√°l es la probabilidad de que las tres arandelas seleccionadas sean m√°s gruesas que las especicaciones de dise√±o?
Comenzamos notando los sucesos Ai : la √≠-√©sima arandela extraida es m√°s gruesa que las especicaciones de dise√±o, i = 1, 2, 3.
Entonces, nos piden

P [A1 ‚à© A2 ‚à© A3 ] = P [A1 ] P [A2 /A1 ] P [A3 /A1 ‚à©A2 ]
30 29 28
=
.
50 49 48
2. ¬æCu√°l es la probabilidad de que la tercera arandela seleccionada sea m√°s gruesa que las especicaciones de dise√±o si las dos primeras fueron m√°s delgadas que la especicaci√≥n?


 30
P A3 /AÃÑ1 ‚à©AÃÑ2 =
.
48

3.8. Teorema de la probabilidad total y Teorema de Bayes
Los siguientes dos resultados se conocen como

Teorema de la probabilidad total y Teorema de Bayes

respectivamente, y juegan un importante papel a la hora de calcular probabilidades. Los dos utilizan como
Prof. Dr. Antonio Jos√© S√°ez Castillo

51

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

principal herramienta el concepto de probabilidad condicionada.

Teorema de la Probabilidad Total.

Sea P una funci√≥n de probabilidad en un espacio muestral. Sea

{A1 , ..., AN } ‚äÇ F una partici√≥n del espacio muestral ‚Ñ¶ y sea B un suceso cualquiera. Entonces,
P [B] = P [B | A1 ] P [A1 ] + ... + P [B | AN ] P [AN ] .

Teorema de Bayes. En esas mismas condiciones, si P [B] 6= 0,
P [Ai | B] =

Ejemplo.

P [B | Ai ] P [Ai ]
.
P [B | A1 ] P [A1 ] + ... + P [B | AN ] P [AN ]

Supongamos que tenemos 4 cajas con componentes electr√≥nicas dentro. La caja 1 contiene

2000 componentes, con un 5 % de defectuosas; la caja 2 contiene 500 componentes, con un 40 % de
defectuosas; las cajas 3 y 4 contienen 1000 componentes, con un 10 % de defectuosas.
1. ¬æCu√°l es la probabilidad de escoger al azar una componente defectuosa?
Notemos D : componente defectuosa y Ci : componente de la caja i-√©sima. Entonces, se tiene que

2000
2000 + 500 + 1000 + 1000
500
P [C2 ] =
2000 + 500 + 1000 + 1000
1000
P [C3 ] =
2000 + 500 + 1000 + 1000
1000
P [C4 ] =
2000 + 500 + 1000 + 1000

P [C1 ] =

4
9
1
=
9
2
=
9
2
=
9
=

Adem√°s, P [D | C1 ] = 0.05, P [D | C2 ] = 0.4, P [D | C3 ] = 0.1 y P [D | C4 ] = 0.1.
Utilizando el Teorema de la probabilidad total,

P [D] = P [D | C1 ] P [C1 ] + P [D | C2 ] P [C2 ] + P [D | C3 ] P [C3 ]
+ P [D | C4 ] P [C4 ]
4
1
2
2
= 0.05 + 0.4 + 0.1 + 0.1 = 0. 11111
9
9
9
9
2. Si se escoge una componente al azar y resulta ser defectuosa, ¬æcu√°l es la probabilidad de que
pertenezca a la caja 1?

P [C1 | D] =

52

0.05 49
P [D | C1 ] P [C1 ]
=
= 0.2
P [D]
0.11111

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

¬µF
0.01
0.1
1.0
Total

N√∫mero
1
20
55
70
145

en cada
2
95
35
80
210

caja
3
25
75
145
245

Total
140
165
295
600

Cuadro 3.2: Acumuladores.

Ejemplo. Se disponen tres cajas donde se almacenan acumuladores seg√∫n aparece en el Cuadro 3.2.
Se escoge al azar una caja y de ella, a su vez, un acumulador.
1. ¬æCu√°l es la probabilidad de que se haya seleccionado un acumulador de 0.01¬µF ?
Notemos 0.01¬µF, 0.1¬µF y 1.0¬µF a los sucesos

extraer un acumulador de

respectivamente. De igual forma, notemos c1, c2 y c3 a los sucesos
caja 3,

0.01¬µF , 0.1¬µF y 1.0¬µF

elegir la caja 1, la caja 2 y la

respectivamente. Utilizando el teorema de la probabilidad total,

P [0.01¬µF ] = P [0.01¬µF / c1] P [c1] + P [0.01¬µF / c2] P [c2] + P [0.01¬µF / c3] P [c3]
95 1
25 1
5903
20 1
+
+
=
= 0.23078.
=
145 3 210 3 245 3
25 578
2. Si ha sido seleccionado un acumulador de 1.0¬µF , ¬æcu√°l es la probabilidad de que proceda de la caja
1? Utilizando el teorema de Bayes,

P [c1 / 1.0¬µF ] =

P [1.0¬µF / c1] P [c1]
.
P [1.0¬µF ]

Por su parte,

P [1.0¬µF ] = P [1.0¬µF / c1] P [c1] + P [1.0¬µF / c2] P [c2] + P [1.0¬µF / c3] P [c3]
70 1
80 1 145 1
6205
=
+
+
=
= 0.48518,
145 3 210 3 245 3
12 789
luego

P [c1 / 1.0¬µF ] =

70 1
145 3
6205
12 789

=

2058
= 0.33167.
6205

Ejemplo. Siguiendo con el ejemplo de las arandelas con grosor fuera de las especicaciones de dise√±o,
¬æcu√°l es la probabilidad de que la tercera arandela seleccionada sea m√°s gruesa que las especicaciones
de dise√±o?

P [A3 ] = P [A3 |A1 ‚à©A2 ]P [A1 ‚à© A2 ] + P [A3 |AÃÑ1 ‚à©A2 ]P [AÃÑ1 ‚à© A2 ]

+P [A3 |A1 ‚à©AÃÑ2 ]P [A1 ‚à© AÃÑ2 ] + P [A3 |AÃÑ1 ‚à©AÃÑ2 ]P [AÃÑ1 ‚à© AÃÑ2 ]

Prof. Dr. Antonio Jos√© S√°ez Castillo

53

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

= P [A3 |A1 ‚à©A2 ]P [A1 ]P [A2 |A1 ] + P [A3 |AÃÑ1 ‚à©A2 ]P [AÃÑ1 ]P [A2 |AÃÑ1 ]

+P [A3 |A1 ‚à©AÃÑ2 ]P [A1 ]P [AÃÑ2 |A1 ] + P [A3 |AÃÑ1 ‚à©AÃÑ2 ]P [AÃÑ1 ]P [AÃÑ2 |AÃÑ1 ]

Ejemplo.

=

28 30 29 29 20 30
+
48 50 49 48 50 49

+

29 30 20 30 20 19
+
.
48 50 49 48 50 49

En el canal de comunicaciones ternario que se describe en la Figura 3.4, se ha observado

que el d√≠gito 3 es enviado tres veces m√°s frecuentemente que 1, y 2 dos veces m√°s frecuentemente
que 1. Calculemos la probabilidad de que un d√≠gito cualquiera enviado a trav√©s del canal sea recibido
correctamente.
En primer lugar, si notamos P [X = 1] = p, entonces P [X = 2] = 2p y P [X = 3] = 3p. Por otra parte,
como

1 = P [X = 1] + P [X = 2] + P [X = 3] = 6p,
se tiene que

P [X = 1] =

1
1
1
, P [X = 2] = y P [X = 3] = .
6
3
2

Ahora, utilizando el teorema de la probabilidad total,

P [dƒ±ÃÅgito OK] = P [dƒ±ÃÅgito OK / X = 1] P [X = 1]
+ P [dƒ±ÃÅgito OK / X = 2] P [X = 2]
+ P [dƒ±ÃÅgito OK / X = 3] P [X = 3]
= P [Y = 1 / X = 1] P [X = 1]
+ P [Y = 2 / X = 2] P [X = 2]
+ P [Y = 3 / X = 3] P [X = 3]
1
1
1
= (1 ‚àí Œ±) + (1 ‚àí Œ≤) + (1 ‚àí Œ≥) = P.
6
3
2

Ejemplo.

Continuando con el anterior, si se recibe un 1, ¬æcu√°l es la probabilidad de que se hubiera

enviado un 1?
Utilizando el teorema de Bayes,

P [X = 1 / Y = 1] =

54

P [Y = 1 / X = 1] P [X = 1]
.
P [Y = 1]

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 3.4: Canal ternario de comunicaciones con probabilidad de cruce

Por su parte,

P [Y = 1] = P [Y = 1 / X = 1] P [X = 1]
+ P [Y = 1 / X = 2] P [X = 2]
+ P [Y = 1 / X = 3] P [X = 3]
=
luego

P [X = 1 / Y = 1] =

Œ≥
1‚àíŒ± Œ≤
+ + ,
6
6
4
1‚àíŒ±
6
Œ≤
1‚àíŒ±
+
6
6

Œ≥
4

+

=2

‚àí1 + Œ±
.
‚àí2 + 2Œ± ‚àí 2Œ≤ ‚àí 3Œ≥

3.9. M√°s sobre el Teorema de Bayes
La importancia del Teorema de Bayes en Estad√≠stica va mucho m√°s all√° de su aplicaci√≥n como f√≥rmula
que facilita probabilidades condicionadas. La losof√≠a que subyace en √©l ha dado lugar a toda una forma de
entender la Estad√≠stica, llamada por ello

Estad√≠stica Bayesiana.

Vamos a tratar de explicar los fundamentos

de esta manera de entender el teorema.
Supongamos que hay un suceso A sobre el que tenemos un serio desconocimiento acerca de si se da o no se
da. Tanto es as√≠ que tenemos que determinar la probabilidad de dicho suceso, P [A]. Es importante entender
que nosotros somos conscientes de que A ha ocurrido o no ha ocurrido: el problema es precisamente que
no sabemos qu√© ha pasado. Decimos que es importante porque P [A] no representa la
ocurra,

probabilidad de que A

sino nuestro grado de creencia en que ha ocurrido.

Es posible que no tengamos, en principio, datos para conocer de forma exacta cu√°l es la probabilidad de A.
A√∫n as√≠, podr√≠amos atrevernos, como

expertos en el tema,

A esta probabilidad inicial que damos la vamos a llamar

a dar una estimaci√≥n de dicha probabilidad, P [A].

probabilidad a priori.

Ahora bien, hemos dado una probabilidad a priori P [A] sin ninguna informaci√≥n sobre A. Supongamos ahora
Prof. Dr. Antonio Jos√© S√°ez Castillo

55

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

que tenemos nueva informaci√≥n que nos dar√° pistas acerca de si A ha ocurrido o no, y que dicha informaci√≥n
est√° recogida en un suceso que llamaremos B1 . En ese caso, podr√≠amos y deber√≠amos actualizar la probabilidad
de A bas√°ndonos en esta nueva informaci√≥n, proporcionando una nueva probabilidad de A que tenga en cuenta

B1 , es decir, P [A |B1 ], que llamaremos

probabilidad a posteriori.

En esa

es donde entra el Teorema de Bayes, ya que nos dice que

actualizaci√≥n de la probabilidad

P [A |B1 ] =

P [B1 |A ] P [A]
 .
P [B1 |A ] P [A] + P [B1 |AÃÑ ] P AÃÑ

Obs√©rvese que la probabilidad a posteriori es proporcional a la probabilidad a priori.
Finalmente, es muy importante ver que podemos extender esta forma de trabajar aplicando el teorema de
una forma recursiva. Despu√©s de conocer B1 , nuestra nueva probabilidad para A es P [A |B1 ]. Abusando de
la notaci√≥n, podemos decir que esa es nuestra nueva probabilidad a priori y si, por ejemplo, tenemos m√°s
informaci√≥n sobre A, dada por otro suceso B2 ,

informaci√≥n independiente de B1 , la nueva probabilidad

a posteriori ser√≠a

P [A |B1 ‚à©B2 ] =
=

P [B2 |A‚à©B1 ] P [A |B1 ]

 

P [B2 |A‚à©B1 ] P [A |B1 ] + P B2 |AÃÑ‚à©B1 P AÃÑ |B1
P [B2 |A ] P [A |B1 ]

.
P [B2 |A ] P [A |B1 ] + P [B2 |AÃÑ ] P AÃÑ |B1

Es muy importante observar que en este cociente P [A |B1 ] ocupa el lugar que antes ocupaba la probabilidad
a priori. Adem√°s, esta segunda probabilidad a posteriori podr√≠a considerarse como la nueva probabilidad a
priori para una nueva aplicaci√≥n del teorema basada en el conocimiento de nueva informaci√≥n dada por un
suceso B3 . Este proceso de actualizaci√≥n de las probabilidades a priori basada en la informaci√≥n disponible
puede realizarse cuantas veces sea necesario.
Vamos a ilustrar esto en un par de ejemplos.

3.9.1. Ejemplo del juez
Supongamos que un juez debe decidir si un sospechoso es inocente o culpable. √âl sabe que debe ser cuidadoso
y garantista con los derechos del acusado, pero tambi√©n por su experiencia parte de una creencia en que
el sospechoso puede ser culpable que, en cualquier caso, estima por debajo de lo que realmente cree para,
insisto, ser garantista con los derechos del acusado. Pongamos que estima esta probabilidad en un 10 %.
Ahora empieza a examinar las pruebas. La primera de ellas es una prueba de ADN en la que el acusado dio
positivo: encontraron material gen√©tico en el arma del crimen que, seg√∫n la prueba, es suyo. Esa prueba de
ADN da positivo en el 99.5 % de las veces en que se comparan dos ADN's id√©nticos, pero tambi√©n da positivo
(err√≥neamente) en el 0.005 % de las veces en que se aplica a dos ADN's distintos. Teniendo en cuenta esta
informaci√≥n, el juez aplica por primera vez el teorema de Bayes con los siguientes datos:

P [culpable] = 0.1, que es la probabilidad a priori que el juez considera.
La probabilidad de que la prueba de ADN de positivo si el acusado es culpable es

P [ADN + |culpable ] = 0.995.

56

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

La probabilidad de que la prueba de ADN de positivo si el acusado es inocente es

P [ADN + |inocente ] = 0.00005.
Ahora ya puede actualizar su grado de creencia en la culpabilidad del sospechoso:

P [ADN + |culpable ] √ó P [culpable]
P [ADN + |culpable ] √ó P [culpable] + P [ADN + |inocente ] √ó P [inocente]
0.995 √ó 0.1
=
= 0.999548
0.995 √ó 0.1 + 0.00005 √ó 0.9

P [culpable |ADN + ] =

Es decir, ahora piensa que el sospechoso es culpable con un 99.9548 % de certeza. Fij√©monos en que nuestra
probabilidad a priori aparece en los t√©rminos 0.1 en el numerador y 0.1 y 0.9 en el denominador. Esa, 0.1,
era la probabilidad que ten√≠amos

antes de la prueba de que fuera culpable (y 0.9 de que fuera inocente);

despu√©s de la prueba esa probabilidad es 0.999548 de que sea culpable (y 0.000452 de que sea inocente).

Sin embargo, el sospechoso insiste en su inocencia, y propone someterse a una prueba de un detector de
mentiras. Los expertos saben que un culpable es capaz de enga√±ar a esta m√°quina en el 10 % de las veces, y
que la m√°quina dir√° el 1 % de las veces que un inocente miente. Nuestro sospechoso se somete a la m√°quina y
√©sta dice que es inocente. ¬æCu√°l ser√° ahora la probabilidad que el juez asigna a la culpabilidad del sospechoso?
Teniendo en cuenta que:

P [maquina‚àí |culpable ] = 0.1,
P [maquina+ |inocente ] = 0.01,
debe aplicar de nuevo el Teorema de Bayes, considerando ahora que la probabilidad a priori de que sea
culpable es 99.9548 %:

P [maquina‚àí |culpable ] √ó P [culpable]
P [maquina‚àí |culpable ] √ó P [culpable] + P [maquina‚àí |inocente ] √ó P [inocente]
0.1 √ó 0.999548
= 0.9955431.
=
0.1 √ó 0.999548 + (1 ‚àí 0.01) √ó (1 ‚àí 0.999548)

P [culpable |maquina‚àí ] =

Es decir, a√∫n con esa prueba negativa, el juez a√∫n tiene un 99.55431 % de certidumbre de que el sospechoso
es culpable. De nuevo, podemos resumir este paso diciendo que

antes de la segunda prueba

nuestra

probabilidad de que fuera culpable era de 0.999548 (que aparece en la f√≥rmula ocupando la posici√≥n de la
probabilidad a priori), mientras que

despu√©s de la segunda prueba esa probabilidad es 0.9955431.

El proceso puede verse resumido en el Cuadro 3.3.

3.9.2. Ejemplo de la m√°quina de detecci√≥n de fallos
En un proceso industrial de producci√≥n en serie de cap√≥s de coche, existe una m√°quina encargada de detectar
desperfectos que desechen una pieza de cap√≥. Esa m√°quina est√° calibrada para detectar una pieza defectuosa
con un 90 % de acierto, pero tambi√©n detecta como defectuosas el 5 % de las piezas no defectuosas. El
encargado de calidad estima, por estudios previos, que el porcentaje general de piezas defectuosas es del 5 %.
Este encargado, consciente de que la m√°quina puede dar por buenas piezas que son defectuosas, decide actuar
de la siguiente forma: una pieza que sea detectada como no defectuosa pasar√° otras dos veces por la misma
m√°quina detectora y s√≥lo ser√° declarada no defectuosa cuando en ninguna de esas tres pruebas, de defectuosa.
Prof. Dr. Antonio Jos√© S√°ez Castillo

57

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

1¬™ prueba: ADN +
2¬™ prueba: maquina‚àí

Antes de
la prueba
0.1
0.999548

P [Culpable]

Despu√©s de
la prueba

P [ADN +|culpable ]√ó0.1
P [ADN +|culpable ]√ó0.1+P [ADN +|inocente ]√ó(1‚àí0.1) = 0.999548
P [maquina‚àí|culpable ]√ó0.999548
P [maquina‚àí|culpable ]√ó0.999548+P [maquina‚àí|inocente ]√ó(1‚àí0.999548) = 0.9955431

Cuadro 3.3: Esquema del proceso iterativo del teorema de Bayes en el ejemplo del juez. La probabilidad a
(antes de cada prueba) es la que se utiliza en la f√≥rmula para obtener la probabilidad a posteriori
(desp√∫√©s de cada prueba). La probabilidad a posteriori (despu√©s) de una prueba es la probabilidad a priori
(antes) de la siguiente prueba.
priori

Supongamos que una pieza pasa las tres veces y da no defectuosa: ¬æcu√°l es la probabilidad de que realmente
sea no defectuosa?
Vamos a empezar notando adecuadamente los sucesos. Notaremos D al suceso ser defectuosa y por + a dar
positivo como defectuosa en la prueba de la m√°quina. Sabemos que:

P [D] = 0.05, que es la probabilidad a priori;
P [+ |D ] = 0.9 y
P [+ |DÃÑ ] = 0.05.
La probabilidad a priori de que una pieza sea no defectuosa es de 0.95, pero si es detectada como defectuosa
una primera vez, dicha probabilidad pasa a ser

 
P [+ÃÑ |DÃÑ ] P DÃÑ
 
P [+ÃÑ |DÃÑ ] P DÃÑ + P [+ÃÑ |D ] P [D]
0.95 √ó 0.95
=
= 0.9944904.
0.95 √ó 0.95 + 0.1 √ó 0.05



P DÃÑ |+ÃÑ =

Esa probabilidad pasa a ser la probabilidad a priori para la segunda vez que da no defectuosa. Por tanto, la
probabilidad de que sea no defectuosa si da negativo por segunda vez es

P [+ÃÑ |DÃÑ ] 0.9944904
P [+ÃÑ |DÃÑ ] 0.9944904 + P [+ÃÑ |D ] (1 ‚àí 0.9944904)
0.95 √ó 0.9944904
=
= 0.9994172.
0.95 √ó 0.9944904 + 0.1 √ó (1 ‚àí 0.9944904)



P DÃÑ |+ÃÑ+ÃÑ =

Finalmente, la probabilidad de que sea no defectuosa si da negativo por tercera vez es

P [+ÃÑ |DÃÑ ] 0.9994172
P [+ÃÑ |DÃÑ ] 0.9994172 + P [+ÃÑ |D ] (1 ‚àí 0.9994172)
0.95 √ó 0.9994172
=
= 0.9999386.
0.95 √ó 0.9994172 + 0.1 √ó (1 ‚àí 0.9994172)



P DÃÑ |+ÃÑ+ÃÑ+ÃÑ =

Como podemos ver, si una pieza da no defectuosa tres veces, la probabilidad de que sea realmente no
defectuosa es alt√≠sima, del orden del 99.99 %, as√≠ que el m√©todo ideado por el responsable de calidad parece
consistente.

58

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

1¬™ prueba: +ÃÑ
2¬™ prueba: +ÃÑ
3¬™ prueba: +ÃÑ

Antes de
la prueba
0.95
0.9944904
0.9994172

 
P DÃÑ

Despu√©s de
la prueba

P [+ÃÑ|DÃÑ ]0.95
P [+ÃÑ|DÃÑ ]0.95+P [+ÃÑ|D ](1‚àí0.95) = 0.9944904
P [+ÃÑ|DÃÑ ]0.9944904
P [+ÃÑ|DÃÑ ]0.9944904+P [+ÃÑ|D ](1‚àí0.9944904) = 0.9994172
P [+ÃÑ|DÃÑ ]0.9994172
P [+ÃÑ|DÃÑ ]0.9994172+P [+ÃÑ|D ](1‚àí0.9994172) = 0.9999386

Cuadro 3.4: Esquema del proceso iterativo del teorema de Bayes en el ejemplo de la m√°quina de detecci√≥n
de fallos. La probabilidad a priori (antes de cada prueba) es la que se utiliza en la f√≥rmula para obtener la
probabilidad a posteriori (desp√∫√©s de cada prueba). La probabilidad a posteriori (despu√©s) de una prueba es
la probabilidad a priori (antes) de la siguiente prueba.

Prof. Dr. Antonio Jos√© S√°ez Castillo

59

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

60

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 4
Variable aleatoria. Modelos de
distribuciones de probabilidad

Mas a pesar de todo eso, aunque la mala suerte exista, muy pocos reporteros veteranos creen de
verdad en ella. En la guerra, las cosas suelen discurrir m√°s bien seg√∫n la ley de las probabilidades:
tanto va el c√°ntaro a la fuente que al nal hace bang.
Arturo P√©rez Reverte, en

Territorio Comanche

Resumen. En este cap√≠tulo continuamos con el estudio de la probabilidad, utilizando el concepto de variable
aleatoria para referirnos a experimentos donde el resultado queda caracterizado por un valor num√©rico. Se
presentan algunos de los modelos m√°s habituales de asignaci√≥n de probabilidades y sus propiedades m√°s
relevantes.

Palabras clave: variable aleatoria, variable discreta, funci√≥n masa de probabilidad, variable continua, funci√≥n
de densidad de probabilidad, funci√≥n de distribuci√≥n, media, varianza, distribuci√≥n binomial, distribuci√≥n
de Poisson, distribuci√≥n geom√©trica, distribuci√≥n uniforme, distribuci√≥n exponencial, distribuci√≥n Gamma,
distribuci√≥n normal.

4.1. Introducci√≥n
En el tema anterior hemos visto que la Estad√≠stica se ocupa de experimentos aleatorios. En general, en Ciencia
y Tecnolog√≠a se suele analizar cualquier experimento mediante una o varias medidas del mismo. Por ejemplo,
se analiza un objeto seg√∫n su peso, su volumen, su densidad, su contenido de agua...; o se analiza el tr√°co
de Internet seg√∫n el n√∫mero de conexiones a un servidor, el volumen total de tr√°co generado, la velocidad...
En estos sencillos ejemplos observamos que se ha descrito un fen√≥meno f√≠sico, como puede ser un objeto o
el estado de una red de comunicaciones en un momento dado, mediante uno o varios n√∫meros o variables.
Cuando ese fen√≥meno es de tipo aleatorio, vamos a llamar a esa asignaci√≥n

variable aleatoria .

Consideremos un experimento probabil√≠stico con un espacio muestral ‚Ñ¶ en el que se ha denido una funci√≥n
de probabilidad P [¬∑] .
61

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Una

variable aleatoria (a partir de ahora v.a.) es un n√∫mero real asociado al resultado de un experimento

aleatorio. Se trata, por tanto, de una funci√≥n real con dominio en el espacio muestral, X : ‚Ñ¶ ‚Üí R.
Podemos pensar en una v.a. como en una variable asociada a una poblaci√≥n conceptual, ya que s√≥lo podr√°
observarse cuando se tomen muestras suyas.
En la notaci√≥n que vamos a utilizar representaremos las variables aleatorias como funciones siempre en
may√∫sculas, y a sus valores concretos siempre en min√∫scula. Es decir, si queremos referirnos a una v.a. antes
de observar su valor, podemos notarla como X, por ejemplo; pero una vez que se observa el valor de dicha
variable (ya no es, por tanto, algo aleatorio), debemos notar a ese valor en min√∫scula, por ejemplo, como x.
Por ejemplo, podemos decir que la variable aleatoria X que corresponde a la puntuaci√≥n obtenida al lanzar el
dado puede tomar los valores x = 1, 2, 3, 4, 5, 6. Podremos preguntarnos por la probabilidad de que X tome
el valor x = 4 o de que X ‚â§ 6. Si lanzamos el dado y observamos que ha salido un 6, diremos que x = 6.
No olvidemos que el objeto de la Estad√≠stica con respecto a la observaci√≥n de fen√≥menos aleatorios es medir
la certidumbre o la incertidumbre asociada a sus posibles resultados. Al describir estos resultados mediante
variables aleatorias, lo que tenemos son resultados num√©ricos sujetos a incertidumbre. El objetivo ahora es
cuanticar la probabilidad de esos resultados num√©ricos de alguna forma.

4.2. Variable aleatoria discreta
4.2.1. Denici√≥n
Se dice que una v.a. es

discreta

si el conjunto de todos los valores que puede tomar es un conjunto, a lo

sumo, numerable (discreto).

Ejemplo. Son variables discretas:
El n√∫mero de accidentes laborales en una empresa al a√±o.
El n√∫mero de errores en un mensaje transmitido.
El n√∫mero de piezas defectuosas producidas a lo largo de un d√≠a en una cadena de producci√≥n.
El n√∫mero de d√≠as de baja de un trabajador al mes.

4.2.2. Funci√≥n masa de probabilidad
Dada una v.a. discreta, X , se dene su

funci√≥n masa de probabilidad como
f (x) = P [X = x] ,

para cada x ‚àà R.

62

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Nota. Obs√©rvese que una funci√≥n masa de una v.a. discreta est√° denida en todos los puntos de la recta
real, pero s√≥lo valdr√° distinto de cero en un conjunto, a lo sumo, numerable, que corresponde con los
√∫nicos valores que pueden darse de la variable.
Sea X una v.a. discreta y f (x) su funci√≥n masa. Entonces:
1. f (x) ‚â• 0 para todo x ‚àà R.
P
2.
x‚ààR f (x) = 1.
3. En general, para cualquier conjunto B,

P [X ‚àà B] =

X

f (xi ) ,

xi ‚ààB

donde xi son valores posibles de X.

4.2.3. Funci√≥n masa de probabilidad emp√≠rica
En la pr√°ctica nadie conoce la aut√©ntica funci√≥n masa de una variable discreta, pero podemos aproximarla
mediante la

funci√≥n masa de probabilidad emp√≠rica

asociada a una muestra de resultados.

Si tenemos una colecci√≥n de posibles resultados de la variable X , x1 , ..., xN , esta funci√≥n asigna al valor x la
frecuencia con la que dicho valor se da en la muestra, es decir,

femp (x) =

nuÃÅmero de valores xi iguales a x
.
N

Si el tama√±o, N , de la muestra es grande, esta funci√≥n tiende a la aut√©ntica, es decir, para cada x ‚àà R.

lƒ±ÃÅm femp (x) = f (x) .

N ‚Üí‚àû

Ejemplo. En la Figura 4.1 aparece la funci√≥n masa emp√≠rica correspondiente al lanzamiento de un dado
600 veces. Esta funci√≥n emp√≠rica aparece representada en barras verticales, mientras que la funci√≥n masa
te√≥rica,

f (x) = 16 , para x = 1, 2, 3, 4, 5, 6 aparece representada como una l√≠nea horizontal. Puede apreciar-

se c√≥mo proporcionan probabilidades te√≥ricas y emp√≠ricas bastante parecidas. No obstante, ¬ædeber√≠amos
concluir a la luz de estos 600 datos que el dado no est√° cargado?

4.2.4. Media y varianza de una variable aleatoria discreta
Dada una v.a. discreta, X , con funci√≥n masa de probabilidad f (x), se dene su media o esperanza matem√°tica
como

EX =

X

x √ó f (x).

x

Prof. Dr. Antonio Jos√© S√°ez Castillo

63

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 4.1: Funci√≥n masa emp√≠rica de una muestra de 600 lanzamientos de un dado.
Como en el caso de la media muestral de unos datos, la media de una v.a. se interpreta como el centro de
gravedad de los valores que puede tomar la variable, con la diferencia que en una media muestral, el
cada valor lo da la frecuencia de dicho valor en los datos y aqu√≠ el

peso

peso

de

lo determina la probabilidad, dada

por la funci√≥n masa.
Dada una v.a. discreta, X , con funci√≥n masa de probabilidad f (x), se dene su varianza como

V arX =

X

(x ‚àí EX)2 √ó f (x).

x

La forma m√°s c√≥moda de calcular en la pr√°ctica la varianza es desarrollando previamente el cuadrado que
aparece en su denici√≥n, ya que

V arX =

X

=

X

(x ‚àí EX)2 √ó f (x) =

x

x

X

(x2 ‚àí 2xEX + EX 2 ) √ó f (x)

x

x2 √ó f (x) ‚àí 2EX √ó

X

x √ó f (x) + EX 2 √ó

x

X

f (x)

x

=E[X 2 ] ‚àí 2EX 2 + EX 2 = E[X 2 ] ‚àí EX 2 .
Al igual que ocurre con la varianza muestral es conveniente denir la desviaci√≥n t√≠pica de una v.a., como
‚àö
œÉ = V arX , que tiene las mismas unidades que la media y que se puede interpretar como una media del
grado de variaci√≥n del conjunto de valores que puede tomar la v.a. respecto del valor de la media.

4.3. Modelos de distribuciones de probabilidad para variables discretas
Seg√∫n lo que hemos visto hasta ahora, la forma en que se asigna probabilidad a los resultados de una
variable aleatoria discreta viene dada por la funci√≥n masa de probabilidad. A esta manera de determinar la

64

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

probabilidad asociada a los resultados de la variable la vamos a llamar a partir de ahora distribuci√≥n de
probabilidad de una v.a. D√©monos cuenta que, como acabamos de comentar, para determinar la distribuci√≥n
de probabilidad de una v.a. s√≥lo tenemos que dar su funci√≥n funci√≥n masa de probabilidad.
Sin embargo, debemos tener en cuenta que en la vida real nadie conoce cu√°l es la aut√©ntica distribuci√≥n de
probabilidad de una v.a., porque nadie sabe a priori cu√°l es la funci√≥n masa de dicha variable. Todo lo m√°s,
podemos calcular la funci√≥n masa emp√≠rica a partir de los datos de una muestra. A√∫n as√≠, llegar√° el momento
de

pasar al l√≠mite,

es decir, de inducir una f√≥rmula te√≥rica que corresponda a la distribuci√≥n de probabilidad

que proponemos y que se parezca a la distribuci√≥n emp√≠rica de los datos de la muestra.
Para ayudar a ese

paso al l√≠mite,

en Estad√≠stica se estudian

modelos te√≥ricos de distribuciones de pro-

babilidad. Se trata de f√≥rmulas te√≥ricas de funciones masa que pueden resultar adecuadas para determinadas
variables aleatorias.
Hay una met√°fora que puede ayudar a entender c√≥mo se asigna una distribuci√≥n de probabilidad y sobre la que
abundaremos en lo sucesivo: ¬æqu√© ocurre cuando queremos comprar unos pantalones? En general acudimos
a una tienda de moda y:
1. De entre una serie de modelos, elegimos el modelo que creemos que mejor nos va.
2. Buscamos la talla que hace que mejor se ajuste a nosotros, seg√∫n nuestras caracter√≠sticas.
Pues bien, en el caso de las v.a.
nuestras caracter√≠sticas

son las posibles observaciones que tenemos sobre la v.a. que, por ejemplo,

pueden determinar una distribuci√≥n emp√≠rica asociada a una muestra;
los modelos

de la tienda, entre los que elegimos el que m√°s nos gusta, son los modelos te√≥ricos que

vamos a empezar a estudiar a continuaci√≥n;
y

la talla

que hace que los pantalones se ajusten a nosotros adecuadamente son los par√°metros de los

modelos te√≥ricos.
En lo que resta de este cap√≠tulo vamos a describir algunos de los modelos te√≥ricos de probabilidad m√°s
habituales en el √°mbito de las Ingenier√≠as, comenzando por el caso de v.a. discretas.

4.3.1. Distribuci√≥n binomial
Sea X una v.a. discreta que toma los valores x = 0, 1, ..., n, donde n es un n√∫mero natural conocido. Se dice

que X sigue una distribuci√≥n

binomial de par√°metros n y p (y se nota X ‚Üí B (n, p)) si su funci√≥n masa

es

f (x) =
=

n

!

x

n‚àíx

px (1 ‚àí p)

n!
n‚àíx
px (1 ‚àí p)
, x = 0, 1, 2, ..., n.
x! (n ‚àí x)!

Prof. Dr. Antonio Jos√© S√°ez Castillo

65

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0.4
B(10,0.25)
0.3
0.2
0.1
0

0

1

2

3

4

5

6

7

8

9

10

0.4
B(10,0.5)
0.3
0.2
0.1
0

0

1

2

3

4

5

6

7

8

9

10

0.4
B(10,0.75)
0.3
0.2
0.1
0

0

1

2

3

4

5

6

7

8

9

10

Figura 4.2: Funciones masa de distribuciones binomiales.
Sea X ‚Üí B (n, p). Entonces

EX = np
V arX = np (1 ‚àí p) .

Caracterizaci√≥n de la distribuci√≥n binomial.

Supongamos que un determinado experimento aleatorio

se repite n veces de forma independiente y que en ese experimento hay un suceso que denominamos

√©xito,

que ocurre con probabilidad constante p. En ese caso, la variable aleatoria X que mide el n√∫mero de √©xitos
sigue una B (n, p).
En esta caracterizaci√≥n es importante observar que las dos hip√≥tesis fundamentales de esta distribuci√≥n son:
los experimentos se repiten de forma
la probabilidad de √©xito es

independiente y

constante.

En la medida en que estas dos hip√≥tesis no sean v√°lidas, la distribuci√≥n binomial no ser√° adecuada para la
variable que cuenta el n√∫mero de √©xitos.
Un ejemplo particular de distribuci√≥n binomial lo constituye la denominada

distribuci√≥n de Bernouilli.

Se trata de una distribuci√≥n B (1, p), con funci√≥n masa

(
f (x) =

66

1 ‚àí p si x = 0
p si x = 1

.

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0

x
P [X = x]

4
0



1
0

0.2 0.8
= 0.41

4
1

4



2
1

0.2 0.8
= 0.41

3

4
2



3
2

0.2 0.8
= 0.15

2

4
3



4
3

1

0.2 0.8
= 0.03

4
4



0.24 0.80
= 0.00

Cuadro 4.1: Funci√≥n masa de una B (4, 0.2)

Ejemplo.

Consideremos como v.a. el n√∫mero de d√≠as a la semana que un joven de hoy consu-

me alcohol. ¬æPodr√≠amos pensar que se trata de una v.a. con distribuci√≥n B (7, p), donde p =
nuÃÅmero medio de dƒ±ÃÅas de consumo
?
7

Probablemente no, porque

1. Puede darse el efecto resaca, es decir, si se consume mucho un d√≠a, huir del alcohol al d√≠a siguiente; o
el efecto inverso un clavo quita otro clavo ; o ...; en denitiva, circunstancias que rompan la hip√≥tesis
de independencia en el consumo en d√≠as distintos.
2. Est√° claro que la probabilidad de consumir un martes no es, en general, la misma que un s√°bado.
Tampoco todos los j√≥venes tienen la misma probabilidad de consumir alcohol un d√≠a cualquiera.

Ejemplo.

Un ingeniero se ve obligado a transmitir d√≠gitos binarios a trav√©s de un sistema de comu-

nicaciones bastante imperfecto. Por estudios previos, estima que la probabilidad de que un d√≠gito se
transmita incorrectamente es del 20 %. El ingeniero env√≠a un mensaje de 4 d√≠gitos y se pregunta cu√°ntos
se recibir√°n incorrectamente.
Desde el punto de vista estad√≠stico nosotros no podemos responder a esa pregunta. En realidad, nadie
puede responder a esa pregunta con certeza, porque existe incertidumbre latente en ella: el azar determinar√° cu√°ntos d√≠gitos se cruzan. Lo que s√≠ podemos hacer es facilitarle el grado de certeza, es decir, la
probabilidad, de cada uno de los posibles resultados.
Concretamente, si analizamos la variable X :

n√∫mero de d√≠gitos que se reciben incorrectamente,

teniendo

en cuenta que el ensayo de cada env√≠o de cada d√≠gito se har√° de forma independiente y que nos ha dicho
que la probabilidad de que un d√≠gito se reciba incorrectamente es 0.2, podemos armar que un modelo de
probabilidad adecuado para dicha variable es una distribuci√≥n B(4, 0.2). Esta distribuci√≥n nos permite
calcular la probabilidad de que se crucen 0, 1, 2, 3 o 4 de los d√≠gitos. Lo esquematizamos en la tabla
adjunta. Vistos los resultados, debemos decirle al ingeniero que es hartamente improbable que le fallen
los 4 d√≠gitos, pero que tiene una probabilidad (ver Cuadro 4.1) de

0.41 + 0.15 + 0.03 + 0.00 = 0.59
de que le falle el env√≠o de al menos uno de ellos.

Prof. Dr. Antonio Jos√© S√°ez Castillo

67

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

4.3.2. Distribuci√≥n de Poisson
Sea X una v.a. discreta, que puede tomar los valores x = 0, 1, 2, ... Se dice que X sigue una

de Poisson de par√°metro Œª (y se nota X ‚Üí P (Œª)) si su funci√≥n masa es
f (x) = e‚àíŒª

distribuci√≥n

Œªx
, x = 0, 1, 2, ...
x!

Sea X ‚Üí P (Œª). Entonces

EX = Œª
V arX = Œª.

Caracterizaci√≥n de la distribuci√≥n de Poisson.

Consideremos el n√∫mero de √©xitos en un periodo de

tiempo donde los √©xitos acontecen a raz√≥n de Œª veces por unidad de tiempo (en promedio) y de forma
independiente. En ese caso

X : nuÃÅmero de ocurrencias del suceso por unidad de tiempo
es una variable de

Poisson de par√°metro Œª, y se nota X ‚Üí P (Œª) .

En esta caracterizaci√≥n, las hip√≥tesis fundamentales ahora son:
la

independencia de las realizaciones y

el promedio

constante de ocurrencias por unidad de tiempo.

Ejemplo. La distribuci√≥n de Poisson suele utilizarse como modelo para el n√∫mero de accidentes ocurridos
en los individuos de una poblaci√≥n a lo largo de un periodo de tiempo. Lo que mucha gente no termina
de asumir es que hacer esa suposici√≥n equivale a decir que todos esos individuos tienen el mismo riesgo
de tener un accidente y que el hecho de que un individuo tenga un accidente no modica para nada la
probabilidad de sufrir un nuevo accidente. Es evidente que en muchas situaciones de la vida real eso no
es cierto, as√≠ que el modelo no ser√° adecuado en ellas.

Ejemplo. Otra aplicaci√≥n muy com√∫n de la distribuci√≥n de Poisson es al n√∫mero de part√≠culas por unidad
de volumen en un uido cuando una disoluci√≥n est√° realmente bien disuelta. En caso de que los datos
indiquen que la distribuci√≥n de Poisson no es adecuada, podr√≠amos de hecho inferir que la disoluci√≥n no
est√° bien disuelta.

Ejemplo.

En el contexto de las redes de telecomunicaciones, el uso m√°s com√∫n de la distribuci√≥n de

Poisson es en el √°mbito del n√∫mero de solicitudes de servicio a un servidor. Por ejemplo, se suele considerar
que el n¬∫ de llamadas a una centralita o el n¬∫ de conexiones a un servidor sigue una distribuci√≥n de Poisson.

68

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Sin embargo, hay que decir que aunque este uso de la distribuci√≥n de Poisson es muy com√∫n, es evidente
que la hip√≥tesis de que el promedio Œª debe ser constante, no se da en estas aplicaciones, ya que uno de
los fen√≥menos m√°s conocidos en telecomunicaciones es el de la

hora cargada :

no es el mismo promedio de

llamadas el que se produce a las 12 del mediod√≠a que a las 3 de la ma√±ana. Lo que se suele hacer es aplicar
uno de los principios m√°s importantes aunque menos escritos de la ingenier√≠a, la ley de Murphy (si
puede ir mal, prep√°rate para ello, porque en algun momento ir√° mal ):

algo

as√≠, las redes de telecomunicaciones

suelen dimensionarse para ser capaces de funcionar en el peor de los escenarios posibles, es decir, cuando
el promedio de solicitudes es el que se da en la hora cargada.

Aproximaci√≥n de la binomial. Ley de eventos raros.

Supongamos que, como en la caracterizaci√≥n

de la distribuci√≥n binomial, un determinado experimento aleatorio se repite n veces de forma independiente
y que en ese experimento hay un suceso que denominamos

√©xito,

que ocurre con probabilidad constante p.

Adicionalmente, supongamos que el experimento se repite un gran n√∫mero de veces, es decir, n es grande y
que el √©xito es un suceso raro, es decir, p es peque√±o, siendo el promedio de ocurrencias, ¬µ = np. En ese caso,
la variable aleatoria X que mide el n√∫mero de √©xitos sigue (aproximadamente) una P (¬µ).
En esta segunda caracterizaci√≥n se suele considerar aceptable la aproximaci√≥n si n > 20 y p < 0.05. Si

n > 100, la aproximaci√≥n es generalmente excelente siempre y cuando np < 10. Hay que tener en cuenta que
para esos valores de los par√°metros, la distribuci√≥n binomial tendr√≠a bastantes problemas para ser computada,
ya que se exigir√≠a, entre otros c√°lculos, el c√°lculo de n! para un valor de n alto, por lo que la aproximaci√≥n
es muy √∫til.

Ejemplo.

Supongamos que un fabricante de maquinaria pesada tiene instalados en el campo 3840

generadores de gran tama√±o. Si la probabilidad de que cualquiera de ellos falle durante el a√±o en curso
es de

1
1200 ,

determinemos la probabilidad de que

a.

4 generadores fallen durante el a√±o en curso,

b.

M√°s 1 de un generador falle durante el a√±o en curso.

El promedio de motores que fallan en el a√±o es Œª = np = (3840)(1/1200) = 3.2.
Sea X la variable que dene el n√∫mero de motores que pueden fallar en el a√±o, con valores x =

0, 1, 2, 3, ...., 3840.
En principio, X ‚Üí B (3840, 1/1200) , pero dado que n es muy grande y p muy peque√±o, podemos
considerar que X ‚Üí P (3.2). Por tanto,

P [X = 4] =

e‚àí3.2 3.24
= 0.178 09
4!

Por su parte,

P [X > 1] = 1 ‚àí P [X = 0, 1] = 1 ‚àí

Prof. Dr. Antonio Jos√© S√°ez Castillo

e‚àí3.2 3.20
e‚àí3.2 3.21
‚àí
= 0.828 80
0!
1!

69

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0.4
P(1)
0.3
0.2
0.1
0
‚àí5

0

5

10

15

20

25

0.2
P(5)
0.15
0.1
0.05
0
‚àí5

0

5

10

15

20

25

0.2
P(10)
0.15
0.1
0.05
0
‚àí5

0

5

10

15

20

25

Figura 4.3: Funciones masa de distribuciones de Poisson.

4.3.3. Distribuci√≥n geom√©trica
Sea X una v.a. discreta que puede tomar los valores x = 0, 1, 2, ... Se dice que sigue una

geom√©trica de par√°metro p (y se nota X ‚Üí Geo (p)), con 0 < p < 1, si su funci√≥n masa es

distribuci√≥n

x

f (x) = p (1 ‚àí p) , para x = 0, 1, 2, ...
Sea X ‚Üí Geo (p). Entonces,

1‚àíp
p
1‚àíp
V arX =
.
p2
EX =

Caracterizaci√≥n de la distribuci√≥n geom√©trica. Supongamos que un determinado experimento aleatorio
se repite sucesivamente de forma independiente y que en ese experimento hay un suceso que denominamos
√©xito,

que ocurre con probabilidad constante p. En ese caso, la variable aleatoria X que cuenta el n√∫mero de

fracasos hasta que ocurre el primer √©xito sigue una Geo (p).

70

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0.4
Geo(0.25)
0.3
0.2
0.1
0
‚àí5

0

5

10

15

20

25

0.8
Geo(0.5)
0.6
0.4
0.2
0
‚àí5

0

5

10

15

20

25

0.8
Geo(0.75)
0.6
0.4
0.2
0
‚àí5

0

5

10

15

20

25

Figura 4.4: Funciones masa de distribuciones geom√©tricas.

Ejemplo. Siguiendo con un ejemplo anterior, sobre el ingeniero que env√≠a d√≠gitos a trav√©s de un canal
imperfecto, ahora se plantea cu√°ntos d√≠gitos se recibir√°n correctamente hasta que uno se cruce, sabiendo
que la probabilidad de que uno cualquiera lo haga es de 0.2.
La variable de inter√©s ahora es Y : n¬∫

de d√≠gitos que se reciben bien hasta el primero que se cruza.

Esta

variable tiene como modelo de probabilidad una distribuci√≥n Geo(0.2). Gracias a este modelo, podemos
decirle, por ejemplo, que la probabilidad de que env√≠e bien dos y que falle el tercero es de

P [Y = 2] = 0.2 √ó 0.82 = 0.128.

4.3.4. Distribuci√≥n binomial negativa
Sea una v.a. discreta que puede tomar los valores x = 0, 1, 2, ... Se dice que X sigue una

binomial negativa de par√°metros a y p (y se nota X
masa es

f (x) =
donde Œì (x) =

¬¥‚àû
0

distribuci√≥n

‚Üí BN (a, p)), con a > 0 y 0 < p < 1, si su funci√≥n

Œì (a + x)
x
pa (1 ‚àí p) para x = 0, 1, 2, ...
Œì (a) Œì (x + 1)

sx‚àí1 e‚àís ds es la funci√≥n gamma.

Obs√©rvese que la distribuci√≥n geom√©trica es un caso particular de la binomial negativa, cuando a = 1.
Prof. Dr. Antonio Jos√© S√°ez Castillo

71

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Sea X ‚Üí BN (a, p). Entonces

1‚àíp
p
1‚àíp
V arX = a 2
p
EX = a

Caracterizaci√≥n de la distribuci√≥n binomial negativa. Sea un determinado experimento aleatorio que
se repite sucesivamente de forma independiente y donde hay un suceso que denominamos

√©xito,

que ocurre

con probabilidad constante p. En ese caso, la variable aleatoria X que cuenta el n√∫mero de fracasos hasta
que ocurre el

k-√©simo

√©xito sigue una BN (k, p). En este caso, adem√°s, y dado que Œì (r) = (r ‚àí 1)! si r es un

entero,

(k + x ‚àí 1)! k
x
p (1 ‚àí p) para x = 0, 1, 2, ...
(k ‚àí 1)!x!
!
k+x‚àí1
x
=
pk (1 ‚àí p) para x = 0, 1, 2, ...
k‚àí1

f (x) =

Caracterizaci√≥n de la distribuci√≥n binomial negativa. Sean X1 , ..., Xn v.a. independientesa con distribuci√≥n Geo (p). En ese caso, X =

Pn

i=1

Xi sigue una BN (n, p). De nuevo obs√©rvese que el primer par√°metro

es un entero.
a Podemos quedarnos por ahora con la idea de que v.a. independientes son aquellas tales que el resultado de cualquiera de
ellas no afecta al resto.

Ejemplo.

Continuando con el ejemplo de la transmisi√≥n de d√≠gitos a trav√©s de un sistema imperfec-

to, ¬æcu√°ntos d√≠gitos se transmitir√°n correctamente hasta que dos lo hagan incorrectamente? De nuevo
tenemos que asumir que no hay una respuesta para esto, pero s√≠ podemos considerar un modelo de
probabilidad para ello que nos ayude a tomar decisiones.
Sea Z :

n¬∫ de d√≠gitos que se reciben bien hasta que dos se cruzan.

Esta v.a. sigue una distribuci√≥n

BN (2, 0.2). Gracias a este modelo, podemos decirle al ingeniero, por ejemplo, que la probabilidad de
que se le crucen 2 d√≠gitos con 10 o menos env√≠os es

P [Z ‚â§ 8] =

8
X
z=0

72

P [Z = z] =

8
X
(2 + z ‚àí 1)!
z=0

(2 ‚àí 1)!z!

0.22 0.8z = 0.62

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0.1

0.06
BN(2.5,0.25)

BN(5,0.25)
0.04

0.05
0.02
0
‚àí10

0

10

20

30

0
‚àí10

40

0.4

0

10

20

BN(5,0.5)

0.3

0.15

0.2

0.1

0.1

0.05
0

10

20

30

40

0.8

0
‚àí10

0

10

20

30

40

0.4
BN(2.5,0.75)

BN(5,0.75)

0.6

0.3

0.4

0.2

0.2

0.1

0
‚àí10

40

0.2
BN(2.5,0.5)

0
‚àí10

30

0

10

20

30

40

0
‚àí10

0

10

20

30

40

Figura 4.5: Funciones masa de distribuciones binomiales negativas.

4.4. Variable aleatoria continua
4.4.1. Denici√≥n
Una variable aleatoria es

continua

si el conjunto de valores que puede tomar s√≥lo puede encerrarse en

intervalos, formando, por tanto, un conjunto con un n√∫mero innito no numerable de elementos.

Ejemplo. Son variables aleatorias continuas:
La tensi√≥n de fractura de una muestra de asfalto.
El grosor de una l√°mina de aluminio.
El pH de una muestra de lluvia.
La duraci√≥n de una llamada telef√≥nica.

4.4.2. Histograma
Hay una diferencia fundamental entre las variables discretas y las continuas: en las discretas podemos, al
menos, numerar los posibles valores y contar el n√∫mero de veces que sale cada valor posible en una muestra.
Sin embargo, por el car√°cter que tienen los intervalos de n√∫meros reales, por muy grande que fuera la muestra
Prof. Dr. Antonio Jos√© S√°ez Castillo

73

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Histograma con N=1000 datos

0.2

0.4

Densidad

0.4

0.0

0.0

0.2

Densidad

0.6

0.6

0.8

0.8

Histograma con N=100 datos

0

1

2

3

4

5

6

0

2

4

6

8

Figura 4.6: Histogramas.
que tom√°ramos de una variable continua, jam√°s tendr√≠amos m√°s de un valor de algunos puntos que puede
tomar la variable1 .
Por esa raz√≥n, en una variable continua no podemos denir una funci√≥n masa emp√≠rica, precisamente porque
los valores de una variable continua no tienen masa de probabilidad.
Sin embargo, como sabemos, existe una representaci√≥n an√°loga a la funci√≥n masa emp√≠rica que permite
aproximar las probabilidades de los valores de una variable continua: el histograma.
Vamos a considerar un sencillo ejemplo para ilustrar esta cuesti√≥n: mediante R simulamos dos muestras de
una variable, una con N = 100 valores y otra con N = 1000. Histogramas asociados a estas muestras, con
10 y 31 intervalos, respectivamente, aparecen en la Figura 4.6. Teniendo en cuenta que el √°rea de las barras
representa la frecuencia relativa con que se dan los valores de los sucesivos intervalos en la muestra, en estos
histogramas podemos ver que la variable toma mayoritariamente valores cercanos a cero; tanto m√°s lejano al
cero es un valor, menos probable parece ser. Este descenso de la probabilidad es adem√°s, muy acusado, casi
exponencial.
Por otra parte, obs√©rvese que al pasar de 100 datos en la muestra a 1000 datos, el histograma esboza la forma
de una funci√≥n real de variable real. En general, cuanto mayor es N m√°s se aproximan los histogramas a la
forma de una funci√≥n continua. Vamos a ir viendo cu√°l es la utilidad de esa funci√≥n desde el punto de vista
del C√°lculo de Probabilidades.
Si en el histograma de la izquierda de la Figura 4.6 quisi√©ramos calcular la probabilidad en la muestra de
alguno de los intervalos que denen el gr√°co, la respuesta ser√≠a el √°rea de la barra sobre dicho intervalo. Si
quisi√©ramos la probabilidad en la muestra de varios intervalos, sumar√≠amos las √°reas de las barras.
El problema es que para que las probabilidades en la muestra se parezcan a las verdaderas probabilidades
es necesario que el tama√±o de la muestra sea grande, cuanto mayor, mejor. En ese caso, tendr√≠amos un
1 Esto

74

suceder√≠a siempre que tomemos un n√∫mero suciente de decimales en cada valor.
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

histograma m√°s parecido al de la derecha de la Figura 4.6. En √©l, de nuevo, si queremos, por ejemplo, calcular

P [a < X < b] ,
deber√≠amos sumar las √°reas de las barras que forman el intervalo (a, b), si es que hay intervalos que forman,
exactamente, el intervalo (a, b) .
Pero si el tama√±o de la muestra es lo sucientemente amplio para poder

pasar al l√≠mite

y encontrar una

funci√≥n real de variable real f (x) que represente la l√≠nea que dene el histograma, calcular una probabilidad
del tipo P [a < X < b] sumando las √°reas de las barras de los intervalos innitesimales que forman el intervalo

(a, b) equivale a integrar dicha funci√≥n en el intervalo (a, b), es decir,
ÀÜ

b

P [a < X < b] =

f (x) dx.
a

4.4.3. Funci√≥n de densidad
Dada una v.a. continua, X , la

funci√≥n de densidad de probabilidad

de X es aquella funci√≥n f (x) tal

que para cualesquiera a, b ‚àà R o a, b = ¬±‚àû,

ÀÜ

b

P [a < X < b] =

f (x) dx
a

Nota.

Dado que a efectos del c√°lculo de integrales un punto no afecta al resultado de la integral, si

a, b ‚àà R, podemos decir que
ÀÜ

b

P [a < X < b] =

f (x) ,
a

ÀÜ

b

P [a ‚â§ X < b] =

f (x) ,
a

ÀÜ

b

P [a < X ‚â§ b] =

f (x) ,
a

ÀÜ
P [a ‚â§ X ‚â§ b] =

b

f (x) .
a

Este hecho pone de maniesto que los valores concretos de una variable aleatoria continua no tienen
masa de probabilidad, ya que

ÀÜ

x0

P [X = x0 ] =

f (x) dx = 0,
x0

pero s√≠ tienen densidad de probabilidad, f (x0 ). Esta densidad de probabilidad representa la probabilidad
de los intervalos innitesimales de valores alrededor de x0 . As√≠, aunque P [X = x0 ] = 0, si f (x0 ) toma
un valor alto, querr√° decir que los valores alrededor de x0 son muy probables.

Dada una v.a. continua, X con funci√≥n de densidad f (x):
Prof. Dr. Antonio Jos√© S√°ez Castillo

75

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

1. f (x) ‚â• 0 para todo x ‚àà R.
2.

¬¥‚àû
‚àí‚àû

f (x) = 1.

3. En general, para cualquier conjunto de n√∫meros reales, B ,

ÀÜ
P [X ‚àà B] =

f (x) dx.
B

4.4.4. Funci√≥n de distribuci√≥n
Se dene la

funci√≥n de distribuci√≥n de probabilidad de una v.a. continua X
ÀÜ

como

x

F (x) = P [X ‚â§ x] =

f (t) dt.
‚àí‚àû

Si X es una v.a. continua con funci√≥n de densidad f (x) y funci√≥n de distribuci√≥n F (x), entonces
1. lƒ±ÃÅmx‚Üí‚àí‚àû F (x) = 0.
2. lƒ±ÃÅmx‚Üí‚àû F (x) = 1.
3. F es creciente.
4. F es continua.
5. f (x) = F 0 (x) .

Ejemplo.

Consid√©rese una variable aleatoria continua, X, con funci√≥n de densidad f (x) = ce‚àía|x| .

Vamos a calcular la constante c, la funci√≥n de distribuci√≥n y P [X ‚â• 0].
En primer lugar,

ÀÜ

ÀÜ

‚àû

1=

ÀÜ

0

f (x) dx =
‚àí‚àû
ÀÜ 0

=

f (x) dx +
‚àí‚àû

ÀÜ

c exp (ax) dx +
‚àí‚àû

‚àû

f (x) dx
0

‚àû

c exp (‚àíax) dx =
0

2c
,
a

luego es necesario que c = a2 .
Por otra parte,

ÀÜ

(

x

F (x) =

f (t) dt =
‚àí‚àû

Por √∫ltimo, P [X ‚â• 0] =

¬¥‚àû
0

1 ax
si x < 0
2e
1
1‚àíe‚àíax
si x ‚â•
2 +
2

0

f (x) dx = 12 .

La funci√≥n de densidad y la de distribuci√≥n, para a = 1, aparecen en la Figura 4.7.

76

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 4.7: Funci√≥n de densidad (izquierda) y de distribuci√≥n (derecha).

Ejemplo. Consideremos una v.a. continua con funci√≥n de distribuci√≥n dada por
F (x) =

Ô£±
Ô£¥
Ô£≤

0 si x < 0
x si 0 ‚â§ x < 1 .

Ô£¥
Ô£≥

1 si x ‚â• 1

En ese caso, la funci√≥n de densidad es

(
0

f (x) = F (x) =

1 si 0 ‚â§ x ‚â§ 1
0 en otro caso

Gr√°camente, ambas funciones aparecen en la Figura 4.8. En esta variable, todos los puntos tienen la
misma densidad de probabilidad, indicando que todos los intervalos de la misma longitud, dentro de

[0, 1] , tienen la misma probabilidad.

4.4.5. Funci√≥n de distribuci√≥n emp√≠rica
Al igual que ocurre con la funci√≥n masa emp√≠rica con respecto a la funci√≥n masa y al histograma con respecto
a la funci√≥n de densidad, la funci√≥n de distribuci√≥n, indistintamente de que se trate de una variable discreta
o continua, tambi√©n tiene una

versi√≥n muestral.

Concretamente, si tenemos una variable aleatoria X y una muestra suya de tama√±o N, (x1 , ..., xN ) , la funci√≥n

de distribuci√≥n emp√≠rica se dene como

SN (x) =

nuÃÅmero de valores ‚â§ x
.
N

Esta funci√≥n se utiliza para aproximarse a la funci√≥n de distribuci√≥n, ya que para un gran n√∫mero de valores,
Prof. Dr. Antonio Jos√© S√°ez Castillo

77

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 4.8: Funci√≥n de densidad (izquierda) y de distribuci√≥n (derecha).
la curva emp√≠rica se parecer√° bastante a la funci√≥n de distribuci√≥n. Dicho de otra forma,

lƒ±ÃÅm SN (x) = F (x) ,

N ‚Üí‚àû

para cada x.

Ejemplo. En el ejemplo anterior se hablaba de una variable aleatoria continua cuya funci√≥n de distribuci√≥n es

F (x) =

Ô£±
Ô£¥
Ô£≤

0 si x < 0
x si x ‚àà [0, 1] .
1 si x > 1

Ô£¥
Ô£≥

En la Figura 4.9 hemos representado dos funciones de distribuci√≥n emp√≠ricas asociadas a sendas muestras
de tama√±o N = 10 (izquierda) y N = 100 (derecha).
Obs√©rvese que cuando aumenta el tama√±o de la muestra (N ), la funci√≥n de distribuci√≥n emp√≠rica se
parece cada vez m√°s a la funci√≥n de distribuci√≥n.

4.4.6. Media y varianza de una v.a. continua
Sea X una v.a. continua con funci√≥n de densidad f (x). Se dene su media o esperanza matem√°tica como

ÀÜ

‚àû

x √ó f (x)dx.

EX =
‚àí‚àû

La interpretaci√≥n de la media de una v.a. continua es, de nuevo, la de un valor central alrededor del que se
dan el conjunto de realizaciones de la v.a. Otra interpretaci√≥n es la de

78

valor esperado, en el sentido de que

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 4.9: Funciones de distribuci√≥n emp√≠ricas.

es el valor de la variable aleatoria en el que a priori se tienen m√°s esperanzas.

Ejemplo. Sea una v.a. continua con funci√≥n de densidad
(
fX (x) =

1
x2 ‚àíx1

si x1 ‚â§ x ‚â§ x2

0 en otro caso

.

Calculemos su media:

ÀÜ

x2

1
¬∑ dx
x
‚àí
x1
2
x1
 2 x2
x
1 x2 ‚àí x21
1
¬∑
= ¬∑ 2
=
x2 ‚àí x1
2 x1
2 x2 ‚àí x1

EX =

=

x¬∑

1 (x2 ‚àí x1 ) ¬∑ (x2 + x1 )
1
¬∑
= (x1 + x2 ) ,
2
x2 ‚àí x1
2

es decir, el punto medio del intervalo [x1 , x2 ].

Ejemplo. Sea una v.a. continua con funci√≥n de densidad
(
fX (x) =

Prof. Dr. Antonio Jos√© S√°ez Castillo

Œªe‚àíŒªx si x ‚â• 0
0 en otro caso

.

79

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Calculemos su media:

ÀÜ

‚àû

x ¬∑ Œª ¬∑ e‚àíŒªx ¬∑ dx

EX =
0

u=x

ÀÜ ‚àû
dv = Œª ¬∑ e‚àíŒªx ¬∑ dx 
‚àû
e‚àíŒªx ¬∑ dx
=
‚àíx ¬∑ e‚àíŒªx 0 +
0

‚àû
1 ‚àíŒªx
1
=0+ ‚àí e
= .
Œª
Œª
0

Vamos a introducir ahora el concepto de varianza de una v.a. continua, que de nuevo se interpreta como una
medida de la concentraci√≥n de los valores de la v.a. en torno a su media.

Sea una v.a. X . Se dene su

varianza como V ar [X] = E

h
i
2
(X ‚àí EX) .

Es decir, es la media de las desviaciones al cuadrado de los valores de la variable respecto de su media.

La ra√≠z cuadrada de la varianza, œÉ =

p

V ar [X] se conoce como

desviaci√≥n t√≠pica.

Como en el caso de las v.a. discretas, existe un m√©todo m√°s c√≥modo para el c√°lculo de cualquier varianza.
En concreto,

h
i
h
i
2
2
V ar [X] = E (X ‚àí EX) = E X 2 ‚àí 2X ¬∑ EX + (EX)
 
 
2
2
= E X 2 ‚àí 2 ¬∑ EX ¬∑ EX + (EX) = E X 2 ‚àí (EX) .
Como se comentaba anteriormente, la interpretaci√≥n de la varianza es la de un promedio que mide la distancia
de los valores de la variable a la media de √©sta. Si la varianza es peque√±a, indica una alta concentraci√≥n de
los valores de la variable en torno a la media; y viceversa, si la varianza es grande, indica alta dispersi√≥n de
los valores de la variable respecto de la media.

Ejemplo. Calculemos la varianza de una v.a. continua con funci√≥n de densidad
(
fX (x) =

ÀÜ

1
x2 ‚àíx1

0 en otro caso

x2

 
E X2 =

x2 ¬∑
x1

si x1 ‚â§ x ‚â§ x2

.

1
1 x32 ‚àí x31
¬∑ dx =
x2 ‚àí x1
3 x2 ‚àí x1

x2 + x1 x2 + x21
= 2
.
3

80

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Anteriormente hab√≠amos demostrado que

EX =

x1 + x2
,
2

por tanto,

 
V ar [X] = E X 2 ‚àí EX 2
2

=

2

(x1 + x2 )
(x2 ‚àí x1 )
x22 + x1 x2 + x21
‚àí
=
.
3
4
12

Nota. Estimaciones muestrales de media y varianza de una v.a.
Probablemente las mentes m√°s despiertas ya se hayan planteado qu√© relaci√≥n hay entre la media y la
varianza de una v.a. (discreta o continua) y la media y la varianza de unos datos, denidas en el cap√≠tulo
de Estad√≠stica Descriptiva.
La respuesta la veremos m√°s adelante, pero podemos ir avanzando que la relaci√≥n es parecida a la que se
da entre los diagramas de barras y las funciones masa o entre los histogramas y las funciones de densidad.
Es decir, si tenemos unos datos de una variable, en otras palabras, una muestra de una variable, la media
y la varianza de la muestra ser√°n aproximaciones de la media y la varianza de la variable aleatoria,
aproximaciones que deben ser tanto mejores cuanto mayor sea el tama√±o de la muestra.

Nota. Comportamiento de la media y la varianza frente a cambios de origen y escala.
Un cambio de origen de una variable consiste en sumar o restar una determinada cantidad a los valores
de la variable, mientras que un cambio de escala supone multiplicar por un factor dichos valroes. En
general, si X es una variable cualquiera, un cambio de origen y escala supone considerar aX + b.
Ya comentamos en el cap√≠tulo de Estad√≠stica Descriptiva el comportamiento de la media y la varianza
muestral frente a estos cambios de origen y escala. Ahora nos referimos aqu√≠ al comportamiento de
sus hom√≥logos poblacionales. Este resultado es muy √∫til en la pr√°ctica y es v√°lido tanto para variables
continuas como para discretas. Concretamente, si X es una v.a. y a, b ‚àà R, entonces

E [aX + b] = aE [X] + b
V ar [aX + b] = a2 V arX

Nota. Si tenemos una colecci√≥n de variables aleatorias independientes, es decir, que son observadas sin
que ninguna de ellas pueda inuir sobre las otras, es muy √∫til plantearse en ocasiones por la media y la
varianza de la suma de todas ellas.
Vamos a considerar las variables X1 , ..., Xn , que pueden ser discretas o continuas. Pues bien, se tiene que
la media de la suma es la suma de las medias y que la varianza de la suma es la suma de las varianzas;

Prof. Dr. Antonio Jos√© S√°ez Castillo

81

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

es decir,

E [X1 + ... + Xn ] = EX1 + ... + EXn
V ar [X1 + ... + Xn ] = V arX1 + ... + V arXn

4.5. Modelos de distribuciones de probabilidad para variables continuas
Como en el caso de las variables discretas, vamos a describir a continuaci√≥n los modelos de distribuciones de
probabilidad m√°s usuales para variables continuas.
De nuevo tenemos que insistir que la utilidad de estos modelos radica en que van a facilitarnos la manera en
que se reparte la probabilidad de los valores de la variable.

4.5.1. Distribuci√≥n uniforme (continua)
Se dice que una v.a. continua X que s√≥lo puede tomar valores en el intervalo (x1 , x2 ) sigue una distribuci√≥n

uniforme entre x1 y x2

(y se nota X ‚Üí U (x1 , x2 )) si su funci√≥n de densidad es

(
f (x) =

1
x2 ‚àíx1

si x1 < x < x2

0 en otro caso

.

Sea X ‚Üí U (x1 , x2 ). Entonces

x1 + x2
2
2
(x2 ‚àí x1 )
V arX =
.
12
EX =

Caracterizaci√≥n de la distribuci√≥n uniforme. Si X

es una v.a. tal que dos intervalos cualesquiera entre

x1 y x2 de la misma longitud, tienen la misma probabilidad, entonces X ‚Üí U (x1 , x2 ) .
El ejemplo m√°s habitual de esta variable es la variable uniforme en el intervalo (0, 1) ; valores simulados de
esta variable son los que se calculan con la orden

RND de cualquier calculadora.

4.5.2. Distribuci√≥n exponencial
Esta distribuci√≥n suele ser modelo de aquellos fen√≥menos aleatorios que miden el tiempo que transcurre entre
que ocurren dos sucesos. Por ejemplo, entre la puesta en marcha de una cierta componente y su fallo o el
tiempo que transcurre entre dos llamadas consecutivas a una centralita.

82

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Sea X una v.a. continua que puede tomar valores x ‚â• 0. Se dice que X sigue una distribuci√≥n

de par√°metro Œª (y se nota X ‚Üí exp (¬µ)) si su funci√≥n de densidad

f (x) =

exponencial

Œªe‚àíŒªx si x ‚â• 0
.
0 en otro caso

Obs√©rvese que su funci√≥n de distribuci√≥n es


F (x) = P [X ‚â§ x] =

1 ‚àí e‚àíŒªx si x ‚â• 0
.
0 en otro caso

Sea X ‚Üí exp (Œª). Entonces,

1
Œª
1
V arX = 2 .
Œª
EX =

Caracterizaci√≥n de la distribuci√≥n exponencial. Sea X ‚Üí P (Œª) una v.a. discreta que cuenta el n√∫mero
de √©xitos en un determinado periodo de tiempo. En ese caso, el tiempo que pasa entre dos √©xitos consecutivos,

T , es una v.a. que sigue una exp (Œª).

Ejemplo. Un elemento radiactivo emite part√≠culas seg√∫n una variable de Poisson con un promedio de
15 part√≠culas por minuto. En ese caso, el tiempo, T , que transcurre entre la emisi√≥n de una part√≠cula y
la siguiente sigue una distribuci√≥n exponencial de par√°metro Œª = 15 part√≠culas por minuto. Este modelo
nos permite, por ejemplo, calcular la probabilidad de que entre part√≠cula y part√≠cula pasen m√°s de 10
segundos, dado por

ÀÜ

‚àû

15e‚àí15t dt = e‚àí15/6 .

P [T > 10/60] =
1/6

Ejemplo.

Recordemos que hab√≠amos comentado que la distribuci√≥n de Poisson se sol√≠a utilizar en el

contexto de las redes de comunicaciones como modelo para el n√∫mero de solicitudes a un servidor por
unidad de tiempo. Seg√∫n esta caracterizaci√≥n que acabamos de ver, eso equivale a decir que el tiempo
que pasa entre dos solicitudes a un servidor sigue una distribuci√≥n exponencial.
Por ejemplo, supongamos que el n√∫mero de conexiones a un servidor FTP sigue una distribuci√≥n de
Poisson de media 2.5 conexiones a la hora. En ese caso, podr√≠amos preguntarnos cu√°l es la probabilidad
de que pasen m√°s de dos horas sin que se produzca ninguna conexi√≥n. Teniendo en cuenta que el tiempo
entre conexiones seguir√≠a una distribuci√≥n exponencial de par√°metro 2.5, esa probabilidad ser√≠a

ÀÜ

‚àû

2.5e‚àí2.5x dx = e‚àí5

P [T > 2] =
2

Prof. Dr. Antonio Jos√© S√°ez Castillo

83

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

o bien


P [T > 2] = 1 ‚àí P [T ‚â§ 2] = 1 ‚àí FT (2) = 1 ‚àí 1 ‚àí e‚àí2.5√ó2 = e‚àí5 .

Hay una interesante y curiosa propiedad de la distribuci√≥n exponencial, conocida como
memoria.

propiedad de no

Si X es una v.a. con distribuci√≥n exp(Œª) y t y s son dos n√∫meros positivos. Entonces:

P [X > t + s|X > s] = P [X > t]
La forma de demostrarlo es muy sencilla:

P [X > t + s|X > s] =
=

P [X > t + s ‚à© X > s]
P [X > t + s]
=
P [X > s]
P [X > s]
e‚àíŒª(s+t)
= e‚àíŒªt = P [X > t]
e‚àíŒªs

Vamos a tratar de entender la trascendencia de esta propiedad en el siguiente ejemplo.

Ejemplo. El tiempo de vida, T , de un circuito, sigue una distribuci√≥n exponencial de media dos a√±os.
Calculemos la probabilidad de que un circuito dure m√°s de tres a√±os:
1

P [T > 3] = e‚àí 2 3
Supongamos que un circuito lleva 5 a√±os funcionando, y que nos planteamos la probabilidad de que a√∫n
funcione 3 a√±os m√°s. Seg√∫n la propiedad de no memoria, esa probabilidad es la misma que si el circuito
acabara de comenzar a funcionar, es decir,
1

P [T > 3 + 5|T > 5] = P [T > 3] = e‚àí 2 3
Desde un punto de vista pr√°ctico, parece poco creible, porque entendemos que los 5 a√±os previos de
funcionamiento deben haber afectado a la abilidad del circuito, pero si creemos que la distribuci√≥n del
tiempo de vida de √©ste es exponencial, tenemos que asumir esta propiedad.

4.5.3. Distribuci√≥n Gamma
Sea X una v.a. continua que puede tomar valores x ‚â• 0. Se dice que X sigue una

distribuci√≥n Gamma de

par√°metros a y Œª (y se nota X ‚Üí Gamma (a, Œª)) si su funci√≥n de densidad es
a‚àí1

f (x) =
donde Œì (x) =

¬¥‚àû
0

Œª (Œªx)
e‚àíŒªx
u (x) ,
Œì (a)

sx‚àí1 e‚àís ds es la funci√≥n gamma.

Obs√©rvese que en el caso en que a = 1 se tiene la distribuci√≥n exponencial.

84

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

1
exp(1)

0.5

0

0

2

4

6

8

10

12

14

16

18

20

0.2
exp(5)
0.15
0.1
0.05
0

0

2

4

6

8

10

12

14

16

18

20

0.1
exp(10)

0.05

0

0

2

4

6

8

10

12

14

16

18

20

Figura 4.10: Funciones de densidad de distribuciones exponenciales.

En el contexto de las telecomunicaciones, hay un caso especialmente interesante. Si a = n, n√∫mero natural, la
distribuci√≥n se denomina

Erlang. Lo que la hace interesante es que esta distribuci√≥n se utiliza como modelo

del tiempo que pasa entre n llamadas telef√≥nicas, por ejemplo.
Otro caso particular lo constituye la distribuci√≥n œá2 con r grados de libertad, que no es m√°s que una

Gamma 2r , 12 . Esta distribuci√≥n se utiliza, por ejemplo, para evaluar la bondad del ajuste de una distribuci√≥n
te√≥rica a unos datos, como veremos m√°s adelante.
Sea X ‚Üí Gamma (a, Œª). Entonces

a
Œª
a
V arX = 2 .
Œª
EX =

Caracterizaci√≥n de la distribuci√≥n Gamma. Sea X ‚Üí P (Œª) una v.a. discreta que cuenta el n√∫mero de
√©xitos en un determinado periodo de tiempo. En ese caso, el tiempo que pasa entre el k‚àí√©simo √©xito y el

k + r, T , es una v.a. que sigue una Gamma (r, Œª). Dado que r es un entero, en realidad es una Erlang (r, Œª).

Caracterizaci√≥n de la distribuci√≥n Gamma. Sean X1 , ..., Xn v.a. independientes con distribuci√≥n exp (Œª).
En ese caso, X =

Pn

i=1

Xi sigue una Gamma (n, Œª). De nuevo obs√©rvese que el primer par√°metro es un entero,

luego se trata de una Erlang.
Prof. Dr. Antonio Jos√© S√°ez Castillo

85

0.00 0.05 0.10 0.15 0.20

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0.00

0.10

0.20

Gamma(2.5,1)

10

15

20

25

30

0.00

0.02

0.04

Gamma(2.5,0.2)

5

10

15

20

25

5

10

15

20

10

15

0

5

10

15

0.000 0.005 0.010 0.015

0.020
0.010
0.000

Gamma(2.5,0.1)

0

5

25

20

25

30

Gamma(5,0.2)

30

0.030

0

0

0.00 0.01 0.02 0.03 0.04

5

0.06

0

Gamma(5,1)

20

25

30

Gamma(5,0.1)

30

0

5

10

15

20

25

30

Figura 4.11: Funciones de densidad de distribuciones Gamma

4.5.4. Distribuci√≥n normal
Sea X una v.a. continua que puede tomar cualquier valor real. Se dice que X sigue una distribuci√≥n normal

o gaussiana, de par√°metros ¬µ y œÉ (y se nota X ‚Üí N (¬µ, œÉ)), si su funci√≥n de densidad es
"

2

(x ‚àí ¬µ)
f (x) = ‚àö
exp ‚àí
2
2œÉ 2
2œÄœÉ
1

#
para todo x ‚àà R.

Obs√©rvese que es la √∫nica distribuci√≥n que hemos visto hasta ahora que toma todos los valores entre ‚àí‚àû y

+‚àû.
Sea X ‚Üí N (¬µ, œÉ). Entonces

EX = ¬µ
V arX = œÉ 2 .
El propio nombre de la distribuci√≥n normal indica su frecuente uso en cualquier √°mbito cient√≠co y tecnol√≥gico.
Este uso tan extendido se justica por la frecuencia o normalidad con la que ciertos fen√≥menos tienden a
parecerse en su comportamiento a esta distribuci√≥n, ya que muchas variables aleatorias continuas presentan
una funci√≥n de densidad cuya gr√°ca tiene forma de campana. Esto, a su vez, es debido a que hay muchas
variables asociadas a fen√≥menos naturales cuyas caracter√≠sticas son compatibles con el modelo aleatorio que
supone el modelo de la normal:
Caracteres morfol√≥gicos de individuos (personas, animales, plantas, ...) de una especie (tallas, pesos,
envergaduras, di√°metros, per√≠metros, ...).

86

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0.4

0.1
N(0,4)

N(0,1)
0.3
0.2

0.05

0.1
0
‚àí10

‚àí5

0

5

10

0.4

0
‚àí10

‚àí5

0

5

10

0.1
N(1,1)

N(1,4)

0.3
0.2

0.05

0.1
0
‚àí10

‚àí5

0

5

10

0.4

0
‚àí10

‚àí5

0

5

10

0.1
N(‚àí1,1)

N(‚àí1,4)

0.3
0.2

0.05

0.1
0
‚àí10

‚àí5

0

5

10

0
‚àí10

‚àí5

0

5

10

Figura 4.12: Funciones de densidad de la distribuci√≥n normal
Caracteres siol√≥gicos (efecto de una misma dosis de un f√°rmaco, o de una misma cantidad de abono).
Caracteres sociol√≥gicos (consumo de cierto producto por un mismo grupo de individuos, puntuaciones
de examen...).
Caracteres psicol√≥gicos (cociente intelectual, grado de adaptaci√≥n a un medio, ...).
Errores cometidos al medir ciertas magnitudes.
Valores estad√≠sticos muestrales, como por ejemplo la media.
Otras distribuciones como la binomial o la de Poisson son aproximadas por la normal, ...
En general, como veremos enseguida, cualquier caracter√≠stica que se obtenga como suma de muchos factores
independientes encuentra en la distribuci√≥n normal un modelo adecuado.
Existe otra raz√≥n m√°s pragm√°tica para el uso tan extendido de la distribuci√≥n normal: sus propiedades
matem√°ticas son, como iremos viendo, casi inmejorables. Eso conduce a que casi siempre se trate de forzar al
modelo normal como modelo para cualquier variable aleatoria, lo cual, en ocasiones puede conducir a errores
importantes en las aplicaciones pr√°cticas. Lo cierto es que tambi√©n son frecuentes las aplicaciones en las que
los datos no siguen una distribuci√≥n normal. En ese caso puede ser relevante estudiar qu√© factores son los
que provocan la p√©rdida de la normalidad y, en cualquier caso, pueden aplicarse t√©cnicas estad√≠sticas que no
requieran de esa hip√≥tesis.
Prof. Dr. Antonio Jos√© S√°ez Castillo

87

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipicaci√≥n de la distribuci√≥n normal.

Sea X ‚Üí N (¬µ, œÉ). Entonces,

Z=
propiedad que suele conocerse como

X ‚àí¬µ
‚Üí N (0, 1) ,
œÉ

tipicaci√≥n de la normal.

Esta conocida propiedad tiene una aplicaci√≥n pr√°ctica muy usual. Dadas las caracter√≠sticas de la densidad
gaussiana, no es posible calcular probabilidades asociadas a la normal de forma exacta, ya que las integrales
del tipo

ÀÜ
a

b

"

2

(x ‚àí ¬µ)
‚àö
exp ‚àí
2
2œÉ 2
2œÄœÉ
1

#
dx

no pueden ser expresadas en t√©rminos de las funciones usuales, y s√≥lo pueden calcularse por m√©todos num√©ricos. No obstante, existen tablas donde aparecen multitud de valores de la funci√≥n de distribuci√≥n de la
distribuci√≥n N (0, 1) y a partir de ellos se pueden calcular otras tantas probabilidades, utilizando la propiedad
de tipicaci√≥n. Por ejemplo, si queremos calcular la probabilidad de que una variable X ‚Üí N (¬µ, œÉ) est√© en
el intervalo [a, b], tenemos


P [a ‚â§ X ‚â§ b] = P






a‚àí¬µ
X ‚àí¬µ
b‚àí¬µ
b‚àí¬µ
a‚àí¬µ
‚â§
‚â§
= FZ
‚àí FZ
,
œÉ
œÉ
œÉ
œÉ
œÉ

donde FZ (¬∑) es la funci√≥n de distribuci√≥n de una variable Z ‚Üí N (0, 1), que puede evaluarse mediante el uso
de tablas. Vamos a verlo en un ejemplo.

Ejemplo. En el art√≠culo √çndices de relaci√≥n peso-talla como indicadores de masa muscular en el adulto
del sexo masculino de la revista Revista Cubana Aliment. Nutr. (1998;12(2):91-5) aparece un
colectivo de varones con un peso cuya media y desviaci√≥n est√°ndar son, respectivamente, 65.6 y 11.7.
1. ¬æC√≥mo podemos, mediante las tablas de la N (0, 1), calcular, por ejemplo, la probabilidad de que
uno de esos varones pese m√°s de 76.25 kilos?


P [X > 76.25] = P

76.25 ‚àí 65.6
X ‚àí 65.6
>
11.7
11.7



= P [Z > 0.91] = 1 ‚àí P [Z < 0.91] = 1 ‚àí 0.819
2. ¬æY la probabilidad de que pese menos de 60 kilos?



X ‚àí 65.6
60 ‚àí 65.6
P [X < 60] = P
<
11.7
11.7



= P [Z < ‚àí0.48] = P [Z > 0.48]
= 1 ‚àí P [Z < 0.48] = 1 ‚àí 0.684
3. ¬æY la probabilidad de que pese entre 60 y 76.25 kilos?

P [60 < X < 76.25] = P [X < 76.25] ‚àí P [X < 60] = 0.819 ‚àí (1 ‚àí 0.684)

88

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 4.13: B√∫squeda de probabilidades en la tabla de la N (0, 1). Valor de la probabilidad a la izquierda de
0.91

4. ¬æCu√°nto pesar√° aquel var√≥n tal que un 5 % de varones de ese colectivo pesan m√°s que √©l? Es decir,
¬æcu√°l ser√° el valor de x tal que P [X > x] = 0.05 o, equivalentemente, P [X < x] = 0.95. Dado que


P [X < x] = P




x ‚àí 65.6
x ‚àí 65.6
X ‚àí 65.6
<
=P Z<
11.7
11.7
11.7

tan s√≥lo tenemos que buscar el valor z =

x‚àí65.6
11.7

tal que P [Z < z] = 0.95, 1.645 (aproximadamente),

en cuyo caso, x = 65.6 + 11.7 √ó 1.645.

Prof. Dr. Antonio Jos√© S√°ez Castillo

89

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 4.14: B√∫squeda de valores z en la tabla de la N (0, 1). Valor de Z que deja a la derecha una probabilidad
de 0.95

Teorema Central del L√≠mite. Sean X1 , ..., XN

v.a. independientes, todas ellas con la misma distribuci√≥n

de probabilidad, distribuci√≥n de media ¬µX y desviaci√≥n t√≠pica œÉX . En ese caso, la suma de estas variables
sigue aproximadamente una distribuci√≥n normal cuando N es elevado, es decir,
N
X



‚àö
Xi ‚âà N N ¬µX , N œÉX .

i=1

Tipicando, podemos reenunciar el Teorema Central del L√≠mite diciendo que

PN

Xi ‚àí N ¬µX
‚àö
‚âà N (0, 1) .
N œÉX

i=1

Este teorema es el que proporciona una justicaci√≥n matem√°tica del porqu√© la distribuci√≥n gaussiana es un
modelo adecuado para un gran n√∫mero de fen√≥menos reales en donde la v.a. observada en un momento dado
es el resultado de sumar un gran n√∫mero de sucesos aleatorios elementales.

Ejemplo. Consideremos X1 , ..., XN

central del l√≠mite,

PN

variables
q independientes con distribuci√≥n U [0, 1]. Seg√∫n el teorema
N
12 . Para poner este resultado de maniesto se ha realizado

i=1 Xi ‚âà N 0.5N,

el siguiente experimento:
Para N = 1, 2, 5 y 10, se ha simulado una muestra de 10000 datos de

90

PN

i=1

Xi , dibujando su histograma

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

140

250
N=1

N=2

120

200

100
80

150

60

100

40
50

20
0

0

0.2

0.4

0.6

0.8

1

300

0

0

0.5

1

1.5

2

350
N=5

N=10
300

250

250

200

200
150
150
100

100

50
0

50
0

1

2

3

4

5

0

0

2

4

6

8

10

Figura 4.15: Ilustraci√≥n del Teorema Central del L√≠mite.

en cada caso. Estos histogramas aparecen en la Figura 4.15. En ella se pone de maniesto como seg√∫n

N crece, el histograma se va pareciendo cada vez m√°s a una densidad gaussiana.

Ejemplo. Supongamos que estamos realizando un examen de 150 preguntas, cada una de ellas con una
puntuaci√≥n de 1 punto y que en funci√≥n de c√≥mo hemos estudiado, consideramos que la probabilidad
de contestar acertadamente una pregunta cualquiera es de 0.7. D√©monos cuenta que el resultado de una
pregunta cualquiera sigue una distribuci√≥n B (1, 0.7), cuya media es 1 √ó 0.7 = 0.7 y cuya varianza es

1 √ó 0.7 √ó (1 ‚àí 0.7) = 0.21.
Por su parte, el resultado nal de la prueba ser√° la suma de las 150 puntuaciones. Podr√≠amos ver este
resultado seg√∫n una B (150, 0.7), pero los c√°lculos ser√≠an muy tediosos debido a los factoriales de la funci√≥n
masa de la distribuci√≥n binomial. En este caso, merece la pena que utilicemos el Teorema Central del
L√≠mite, seg√∫n el cu√°l el resultado nal, X , seguir√≠a aproximadamente una distribuci√≥n

‚àö

N 150 √ó 0.7, 150 √ó 0.21 ,
es decir, X ‚Üí N (105, 5.612) . As√≠, si por ejemplo, nos planteamos cu√°l es la probabilidad de aprobar,
√©sta ser√°

P [X > 75] = P [Z > ‚àí0.952] = 0.830.
Esta aplicaci√≥n se conoce, en general, como

aproximaci√≥n normal de la binomial.

Enunciando el Teorema Central del L√≠mite en t√©rminos de la media, XÃÑ , de las variables X1 , ..., XN , podemos
decir que si N es grande,

Prof. Dr. Antonio Jos√© S√°ez Castillo

‚àö
XÃÑ ‚âà N (¬µ, œÉ/ N )

91

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Ejemplo. Un ingeniero dise√±a un aparato de medida que realiza una aproximaci√≥n m√°s imprecisa que
el aparato tradicional pero mucho m√°s barata. Para reducir el margen de error de la medida realizada,
el ingeniero propondr√° que se realicen un n√∫mero determinado de medidas sobre el mismo objeto y que
se considere la media de estas medidas como valor nal de la medida del objeto.
Inicialmente, el ingeniero hace una valoraci√≥n que le lleva a concluir que el aparato est√° bien calibrado,
es decir, que la media de la medida del aparato coincide con la medida real, y que la desviaci√≥n t√≠pica
de las medidas del aparato es igual a 0.75.
¬æCu√°ntas medidas debe proponer el ingeniero para que el error de medida sea inferior a 0.1 con un 95 %
de probabilidad?
Empecemos considerando que cada medida, Xi , tiene como media el verdadero
valor de la medida del
P
objeto, x0 , y desviaci√≥n t√≠pica 0.75. Por su parte, la medida nal ser√° XÃÑ =

n
i=1

Xi

n

, donde realmente nos

interesa conocer el valor de n. Para ello, tengamos en cuenta que se nos pide que




P XÃÑ ‚àí x0  < 0.1 ‚â• 0.95.


‚àö
. Por su parte,
y que, considerando el Teorema Central del L√≠mite, XÃÑ ‚Üí N x0 , 0.75
n

‚àö
‚àö 





0.1 n
0.1 n
P XÃÑ ‚àí x0  < 0.1 = P x0 ‚àí 0.1 < XÃÑ < x0 + 0.1 = P ‚àí
<Z<
0.75
0.75


‚àö 
0.1 n
.
=1‚àí2√ó 1‚àíP Z <
0.75
h



Si queremos que P XÃÑ ‚àí x0  < 0.1 ‚â• 0.95, entonces P Z <

‚àö i
0.1 n
0.75

‚â• 0.975, de donde

‚àö
0.1 n
0.75

‚â• 1.96 y

entonces, n ‚â• 216.09.
Como conclusi√≥n, m√°s le vale al ingeniero disminuir la desviaci√≥n t√≠pica del aparato de medida.

4.6. Cuantiles de una distribuci√≥n. Aplicaciones
Para acabar el tema vamos a ver una de las aplicaciones m√°s sencillas pero a la vez m√°s √∫tiles de los modelos
de probabilidad. Debo decir que son numerosas las ocasiones que desde distintos ambientes cient√≠cos y de la
Ingenier√≠a he asesorado a profesionales con respecto a cuestiones que tienen que ver con lo que esta secci√≥n
analiza. Los ejemplos que vamos a considerar son,
Concretamente, vamos a comenzar deniendo el

grosso modo,

cuantil

s√≠ntesis de ellas.

p (p ‚àà [0, 1]) de una distribuci√≥n de probabilidad

de una v.a. X . Sea √©sta discreta o continua, denominemos f (x) a su funci√≥n masa o de densidad.
Se dene el cuantil p, Qp de su distribuci√≥n como el primer valor, x, de la variable tal que P [X ‚â§ x] ‚â• p:
Si la variable es discreta, Qp ser√°, por tanto, el primer valor tal que

X

f (x) ‚â• p.

xi ‚â§x

92

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

N√≥tese que, al ser la variable discreta, puede que no logremos obtener una igualdad del tipo

P

xi ‚â§x

f (x) =

p.
Si la variable es continua, Qp s√≠ puede obtenerse como el valor x tal que

ÀÜ

x

f (t) dt = p,
‚àí‚àû

o lo que es lo mismo, como el valor x tal que F (x) = p, siendo F la funci√≥n de distribuci√≥n de la
variable.
Es muy frecuente que la probabilidad p a la que se asocia un cuantil se exprese en porcentaje. En ese caso,
los cuantiles tambi√©n se pueden llamar percentiles. Por ejemplo, el cuantil 0.5 es el percentil 50, la mediana.

Desde luego, lo m√°s importante es que interpretemos qu√© signica el cuantil p de una v.a. Como en Estad√≠stica
Descriptiva, se reere al valor de la variable que deja por debajo de s√≠ una proporci√≥n p de valores de la variable.
Entonces, si un valor concreto corresponde con un cuantil

alto,

podemos decir que realmente es un valor

alto

dentro de la distribuci√≥n de probabilidad de la variable, y viceversa. Vamos a tratar de aclararlo con algunos
ejemplos.

4.6.1. La bombilla de bajo consumo marca ANTE
En el cap√≠tulo de introducci√≥n coment√°bamos las especicaciones t√©cnicas que aparec√≠an en el envoltorio
de una bombilla de 14W de la marca ANTE, entre las que se dec√≠a que ten√≠a una duraci√≥n de 8 a√±os.
Eso contradice nuestra sensaci√≥n de que este tipo de l√°mparas duran mucho menos y, en cualquier caso, es
una simplicaci√≥n inadmisible, porque es evidente que la duraci√≥n de la bombilla es una variable sujeta a
incertidumbre, es decir, una variable aleatoria.
Vamos a hacer un par de asunciones. En primer lugar, es probable que lo que quisieran decir en el envoltorio
es que la

duraci√≥n media

es de 8 a√±os (lo cu√°l, por cierto, tambi√©n podr√≠a ser objeto de controversia).

En segundo lugar, dado que tenemos que proponer un modelo de distribuci√≥n de probabilidad para la duraci√≥n de la l√°mpara, vamos a considerar el m√°s sencillo que suele emplearse en este tipo de aplicaciones:
la distribuci√≥n exponencial. Esta hip√≥tesis tambi√©n podr√≠a ser discutida, pero otros modelos m√°s complejos,
como la distribuci√≥n Weibull, complicar√≠an bastante nuestros c√°lculos que, por otra parte, tienen s√≥lo nes
ilustrativos.
Por tanto, vamos a suponer que la duraci√≥n de la bombilla es una variable aleatoria, D, con distribuci√≥n
exponencial de media 8 a√±os y, por tanto, con par√°metro Œª = 1/8. Ahora que ya tenemos un modelo
probabil√≠stico podemos plantearnos muchas cosas:
¬æEs muy probable que la l√°mpara alcance su vida media?

ÀÜ
P [D > 8] =
8

‚àû

1 ‚àíx
e 8 dx = e‚àí8/8 = 0.3678794.
8

Obs√©rvese que eso es algo que ocurrir√° con cualquier exponencial: la probabilidad de que se supere la
media es s√≥lo del 36.79 %. Dicho de otra forma, la media es el percentil 63 aproximadamente, lo que
implica que s√≥lo el 37 % aproximadamente de las l√°mparas superan su vida media... ¬æsorprendente?
Prof. Dr. Antonio Jos√© S√°ez Castillo

93

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

¬æY cu√°l es el valor que superan el 50 % de las l√°mparas? Se trata de la mediana, M e = F ‚àí1 (0.5) ,
donde F () es la funci√≥n de distribuci√≥n. Por tanto, la mediana es la soluci√≥n de la ecuaci√≥n

1 ‚àí e‚àíŒªM e = 0.5,
que resulta ser M e =

log0.5
‚àíŒª

= 8 √ó log2 = 5.545177. Luego, visto de otra forma, el 50 % de las l√°mparas

se rompen antes de 5.545 a√±os.
Para terminar, animo a los lectores interesados a que busquen informaci√≥n sobre el c√≥mputo de la vida
media de este tipo de l√°mparas, basado en la realizaci√≥n de pruebas aceleradas sobre una muestra (bastante
reducida, por cierto) de l√°mparas.

4.6.2. Las visitas al pediatra de los padres preocupados
Los que tenemos hijos peque√±os observamos con cierta ansiedad la evoluci√≥n de su peso y su altura. Cuando
vamos al pediatra, √©ste pesa y mide al beb√© y, obviamente, te dice

c√≥mo est√°.

Pero el problema es que no

basta con que me diga cu√°nto pesa y mide mi hijo o mi hija, sino que me diga cu√°nto pesa y cu√°nto mide en
relaci√≥n con los ni√±os o ni√±as de su misma edad. En esa cuesti√≥n es d√≥nde entran los percentiles.
En este caso jugamos con la ventaja de que se han hecho multitud de estudios previos que determinan que
tanto el peso como la altura son variables que siguen una distribuci√≥n normal. M√°s a√∫n, se han determinado
las medias y las desviaciones t√≠picas de ni√±os y ni√±as desde los 0 meses hasta la edad adulta.
Vamos a ponernos en una situaci√≥n concreta, centr√°ndonos en el peso. Tengo un hijo de tres meses que pesa
5.6 kilos. La pregunta es
sabe por estudios

¬æest√° gordo? ¬æes bajito?

previos2

En cualquier caso,

c√≥mo de gordo o de bajito.

El pediatra

que el peso de ni√±os de tres meses es una N (6, 1.2). Lo que se plantea es en qu√©

posici√≥n se sit√∫a el peso de mi hijo, 5.6 kilos, dentro de esa distribuci√≥n. Si X es el peso, dado que

P [X ‚â§ 5.6] = 0.369,
el pediatra me dir√° que mi hijo est√° en el percentil 37, lo que quiere decir que es un pel√≠n bajo de peso, pero
dentro de niveles razonables.

2 Fuente:

94

http://www.familia.cl/salud/curvas_de_crecimiento/curvas_de_crecimiento.htm
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 4.16: Curvas de crecimiento de 0 a 24 meses.

Prof. Dr. Antonio Jos√© S√°ez Castillo

95

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

96

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 5
Variables aleatorias con distribuci√≥n
conjunta

El matrimonio es la principal causa de divorcio.
Groucho Marx

Resumen. En el estudio de las variables aleatorias hemos pasado por alto el hecho de que un conjunto de
dos o m√°s variables puede verse afectado por una serie de relaciones entre ellas. El an√°lisis desde el punto
de vista estad√≠stico de estas relaciones es el objetivo de este cap√≠tulo. Como caso especial, describiremos de
forma detallada el modelo que para estas relaciones proporciona la distribuci√≥n normal multivariante

Palabras clave: distribuci√≥n conjunta, distribuci√≥n marginal, distribuci√≥n condicionada, covarianza, coeciente de correlaci√≥n, normal multivariante.

5.1. Introducci√≥n
El mundo real est√° repleto de relaciones a todos los niveles. Nosotros, por razones obvias, estaremos interesados principalmente en las relaciones que afectan a variables que describen fen√≥menos propios del ambiente
cient√≠co-tecnol√≥gico. Estas relaciones pueden tener muy diversas tipologias. Por ejemplo, podr√≠amos pensar
en relaciones causa-efecto, como la que, por ejemplo, explicar√≠a que una p√°gina Web tenga un tama√±o considerable

debido

a que lleva incrustado varios archivos de v√≠deo y audio, o la que se establece entre la edad

en a√±os de un vestigio y su contenido en carbono 141 . Pero no s√≥lo tendremos relaciones causa-efecto: por
ejemplo, sabemos que el peso y la estatura de un ser humano son variables muy relacionadas, hasta el punto
que no podemos decir que una persona este obesa s√≥lo con saber su peso, sino que debemos valorarlo
relaci√≥n a

en

su estatura.

Por otra parte, cuando un fen√≥meno es determin√≠stico y est√° bien estudiado, las relaciones entre variables
son leyes m√°s o menos sencillas, pero, en cualquier caso, son inmutables. Por ejemplo,

densidad =
1 Relaci√≥n

masa
.
vol.

que, por cierto, sabemos que permite la dataci√≥n del vestigio.

97

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Pero, ¬æqu√© ocurre cuando el fen√≥meno es aleatorio? Las variables en ese caso son aleatorias y las relaciones que
se puedan dar entre ellas no siempre tienen por qu√© obedecer a una ley objetiva e inamovible. Por ejemplo,
todos somos conscientes de que, como dec√≠amos, existe una relaci√≥n entre el peso y la altura de una persona,
pero no existe una

raz√≥n de conversi√≥n

capaz de calcular el peso exacto de alguien a partir de su altura. Es

evidente que el tiempo de descarga de una p√°gina web estar√° relacionado con el tama√±o de los archivos que
la conguran, pero ¬æc√≥mo de

evidente ?

y ¬æde qu√© forma es esa relaci√≥n? Ambas preguntas tratar√°n de ser

contestadas a lo largo de este cap√≠tulo.
Sean X1 , ..., XN variables aleatorias. El vector ordenado

Ô£´

Ô£∂
X1
Ô£¨ . Ô£∑
Ô£¨ . Ô£∑
Ô£≠ . Ô£∏
XN
es un

vector aleatorio de dimensi√≥n N .

Hablaremos de

vectores aleatorios continuos o vectores aleatorios discretos cuando cada una de sus

variables sean continuas o discretas, respectivamente. Podr√≠an darse

vectores mixtos,

pero su tratamiento

estad√≠stico no nos interesa por ahora.

Ejemplo. Consideremos el valor de una se√±al anal√≥gica que depende del tiempo, x (t). En esta notaci√≥n,
entendemos que el valor de la se√±al podr√≠a ser distinto en cada instante de tiempo t. Es muy frecuente
que la se√±al se observe realmente contaminada por un ruido aleatorio que tambi√©n depender√° del tiempo,

N (t). En ese caso, si observamos la se√±al en los instantes t1 , ..., tN , el vector
Ô£´

Ô£∂
x (t1 ) + N (t1 )
Ô£¨
Ô£∑
..
Ô£¨
Ô£∑
.
Ô£≠
Ô£∏
x (tn ) + N (tn )
es un vector aleatorio.

Ejemplo. Se estudia el tiempo que un usuario de Internet dedica a ver una p√°gina WEB (T ) en relaci√≥n
con variables como la cantidad de texto que contiene (T x), el n√∫mero de im√°genes (I) y animaciones
Flash (F ) de la p√°gina. Entonces, el vector

Ô£´

T

Ô£¨
Ô£¨ Tx
Ô£¨
Ô£¨ I
Ô£≠
F

Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏

es un vector aleatorio.

98

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. Se contabiliza la duraci√≥n de las llamadas telef√≥nicas a una centralita. Para cada conjunto de
n-usuarios

de la centralita, cada uno de ellos ocupa un tiempo Ti en su llamada. En ese caso, el vector

Ô£´

Ô£∂
T1
Ô£¨ . Ô£∑
Ô£¨ . Ô£∑
Ô£≠ . Ô£∏
Tn
es un vector aleatorio.

5.2. Distribuciones conjunta, marginal y condicionada
El principal objetivo a abordar en el tema es c√≥mo medir la incertidumbre asociada a los sucesos que describe
un vector aleatorio. Ya vimos que en el caso de una variable aleatoria se trataba de hacerlo a partir de la
funci√≥n masa o la funci√≥n de densidad. Ahora, como vamos a ver, es algo m√°s complejo.

5.2.1. Distribuci√≥n conjunta
La

distribuci√≥n conjunta de probabilidad

de un vector aleatorio es, esencialmente, la manera en que

se reparte la probabilidad entre todos los posibles resultados del vector. Para describirla vamos a denir los
conceptos de funci√≥n de densidad o funci√≥n masa an√°logos a los asociados a una variable aleatoria.
Sea (X1 , ..., XN ) un vector aleatorio discreto. Entonces, se dene su

funci√≥n masa conjunta como

fX1 ,...,XN (x1 , ..., xN ) = P [X = x1 , ..., XN = xN ] .

Por su parte, si (X1 , ..., XN ) es un vector aleatorio continuo, entonces, su

funci√≥n de densidad conjunta

es una funci√≥n tal que

ÀÜ


P (X1 , ..., XN ) ‚àà A ‚äÇ RN =

ÀÜ
...

A‚äÇRN

fX1 ,...,XN (x1 , ..., xN ) dx1 ...dxN

Ejemplo. Consideremos un vector aleatorio bidimensional,(X, Y )0 , que tiene densidad conjunta
(
fX,Y (x, y) =

ce‚àíx‚àíy si 0 < y < x
0 en otro caso

.

En primer lugar, podemos calcular la constante c teniendo en cuenta que

ÀÜ
fX,Y (x, y) dxdy = 1.
R2

Prof. Dr. Antonio Jos√© S√°ez Castillo

99

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Por ello,

ÀÜ

‚àû

ÀÜ

x
‚àíx ‚àíy

ce

1=
0

e

ÀÜ



‚àû

dy dx =

0

0


c
ce‚àíx 1 ‚àí e‚àíx dx = ,
2

de donde c = 2.
En segundo lugar, por ejemplo, calculemos

ÀÜ

ÀÜ

1

1‚àíy

2e‚àíx e‚àíy dxdy

P [X + Y ‚â§ 1] =
y

0

ÀÜ

1

h
i
2e‚àíy e‚àíy ‚àí e‚àí(1‚àíy) dy

=
0

‚àí1 ‚àí 2e + e2
=
.
e2
(ver Figura 5.1)

Figura 5.1: Regi√≥n del plano donde se calcula la probabilidad.

Ejemplo. Consideremos dos variables, X

e Y , que tienen densidad conjunta

(
fX,Y (x, y) =

1
15

si 0 ‚â§ x ‚â§ 3, 0 ‚â§ y ‚â§ 5

0 en otro caso

.

Esta densidad constante en el rect√°ngulo denido indica que la distribuci√≥n de probabilidad es uniforme
en dicho rect√°ngulo. Vamos a calcular la probabilidad de que Y sea mayor que X (ver Figura 5.2)

ÀÜ

3

ÀÜ

5

P [Y > X] =
0

ÀÜ

x


1
dy dx
15

3

5‚àíx
=
dx
15
0
x x2 3 7
= ‚àí
| =
.
3
30 0 10

100

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 5.2: Regi√≥n del plano donde se calcula la probabilidad.

5.2.2. Distribuciones marginales
Una vez que somos capaces de describir la distribuci√≥n de probabilidad de un vector aleatorio mediante su
funci√≥n masa o su funci√≥n de densidad conjunta, surge un nuevo problema: qu√© ocurre si deseamos conocer la
distribuci√≥n de probabilidad de una o m√°s variables del vector, no del vector en su conjunto. Esa distribuci√≥n
de una o m√°s variables de un vector se conoce como

distribuci√≥n marginal.

0

Sea (X1 , ..., XN ) un vector aleatorio y (Xi1 , ..., Xik ) un subvector de variables suyo. En ese caso:
Si el vector es continuo,

ÀÜ
fXi1 ,...,Xik (xi1 , ..., xik ) =

ÀÜ
...

Y

fX1 ,...XN (x1 , ..., xn )

xj ‚àà
/ (xi1 ,...,xik )

dxj .

xj ‚àà
/ (xi1 ,...,xik )

Si el vector es discreto,

fXi1 ,...,Xik (xi1 , ..., xik ) =

X

fX1 ,...XN (x1 , ..., xn ) .

xj ‚àà
/ (xi1 ,...,xik )

Ejemplo. Sea el vector bidimensional (X, Y ) con funci√≥n de densidad conjunta fX,Y

(x, y) = x ¬∑ e‚àíx(y+1)

para x, y > 0.
La funci√≥n de densidad marginal de X ,

ÀÜ

ÀÜ

‚àû

fX (x) =
‚àí‚àû

‚àû

xe‚àíx(y+1) dy = e‚àíx

fX,Y (x, y) dy =
0

para x > 0.
An√°logamente, la funci√≥n de densidad marginal de Y ,

ÀÜ

ÀÜ

‚àû

‚àí‚àû

‚àû

xe‚àíx(y+1) dx =

fX,Y (x, y) ¬∑ dx =

fY (y) =

0

1
(1 + y)

2

para y > 0.

Prof. Dr. Antonio Jos√© S√°ez Castillo

101

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Ejemplo. Consideremos dos variables discretas, Q y G, cuya funci√≥n masa, fQ,G (q, g) , viene dada por
fQ,G (q, g)

g=0

g=1

g=2

g=3

q=0

0.06

0.18

0.24

0.12 .

q=1

0.04

0.12

0.16

0.08

Sus marginales respectivas son:

fQ (q) =

X

fQ,G (q, g)

g

(
=

0.04 + 0.12 + 0.16 + 0.08 si q = 1
(

=
y

0.06 + 0.18 + 0.24 + 0.12 si q = 0
0.6 si q = 0
0.4 si q = 1

Ô£±
Ô£¥
Ô£¥
0.06 + 0.04
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤0.18 + 0.12
fG (g) =
Ô£¥
Ô£¥
0.24 + 0.16
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥0.12 + 0.08

si g = 0
si g = 1
si g = 2
si g = 3

Ejemplo. En un ejemplo anterior consider√°bamos dos variables X
(
fX,Y (x, y) =

1
15

e Y que tienen densidad conjunta

si 0 ‚â§ x ‚â§ 3, 0 ‚â§ y ‚â§ 5

0 en otro caso

.

Vamos a calcular sus densidades marginales:

ÀÜ
fX (x) =
=

fX,Y (x, y) dy
‚àí‚àû
( ¬¥5
1
dy si 0 ‚â§ x
0 15

102

‚â§3

0 en otro caso
(

=

‚àû

1
3

si 0 ‚â§ x ‚â§ 3

0 en otro caso

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

ÀÜ
fY (y) =
=

‚àû

fX,Y (x, y) dx
‚àí‚àû
( ¬¥3
1
dx si 0 ‚â§ y
0 15

‚â§5

0 en otro caso
(

=

1
5

si 0 ‚â§ y ‚â§ 5

0 en otro caso

.

Por tanto, ambas marginales corresponden a sendas densidades uniformes.

Ejemplo. La densidad conjunta de X

e Y es

(
fX,Y (x, y) =

2x si 0 ‚â§ x ‚â§ 1, |y| < x2
0 en otro caso

.

Calculemos ambas marginales:

ÀÜ

‚àû

fX (x) =

fX,Y (x, y) dy
‚àí‚àû

( ¬¥ 2
x
‚àíx2

=

2xdy si 0 ‚â§ x ‚â§ 1

0 en otro caso
(

=

ÀÜ
fY (y) =

4x3 si 0 ‚â§ x ‚â§ 1
0 en otro caso

‚àû

fX,Y (x, y) dx
‚àí‚àû
( ¬¥1
‚àö

=

|y|

2xdx si ‚àí 1 ‚â§ y ‚â§ 1
0 en otro caso

(
=

1 ‚àí |y| si ‚àí 1 ‚â§ y ‚â§ 1
0 en otro caso

.

5.2.3. Distribuciones condicionadas
0

Si tenemos un vector X = (X1 , ..., XN ) , podemos considerar la distribuci√≥n de probabilidad de un vector
0

formado por un subconjunto de variables de X , (Xi1 , ..., Xik ) , condicionada al hecho de que se han dado
determinados valores en otro subconjunto de variables de X, Xj1 = xj1 , ..., Xjl = xjl .
Prof. Dr. Antonio Jos√© S√°ez Castillo

103

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Esta distribuci√≥n vendr√° caracterizada por su funci√≥n masa o su funci√≥n de densidad

condicionadas, seg√∫n

sea el vector discreto o continuo, y tendr√° la expresi√≥n

fXi1 ,...,Xik |Xj1 =xj1 ,...,Xjl =xjl (xi1 , ..., xik ) =

fXi1 ,...,Xik ,Xj1 ,...,Xjl (xi1 , ..., xik , xj1 , ..., xjl )
fXj1 ,...,Xjl (xj1 , ..., xjl )

,

donde fXi1 ,...,Xik ,Xj1 ,...,Xjl (xi1 , ..., xik , xj1 , ..., xjl ) es la funci√≥n masa o la funci√≥n de densidad conjunta de
las variables Xi1 , ..., Xik , Xj1 , ..., Xjl y fXj1 ,...,Xjl (xj1 , ..., xjl ) es la funci√≥n masa o la funci√≥n de densidad
conjunta de las variables Xj1 , ..., Xjl .
En el caso m√°s habitual en el que el vector tenga dimensi√≥n dos, tenemos la densidad o la funci√≥n masa de

X condicionada a Y = y,
fX|Y =y (x) =

fX,Y (x, y)
fY (y)

o la densidad o la funci√≥n masa de Y condicionada a X = x,

fY |X=x (y) =

Ejemplo. Sean las variables X

fX,Y (x, y)
.
fX (x)

e Y con la funci√≥n masa conjunta siguiente:
y\x

0

1

2

0

3/28

9/28

3/28

1

3/14

3/14

0

2

1/28

0

0

Las marginales son

fX (x) =

Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

3
3
1
28 + 14 + 28 si x = 0
9
3
28 + 14 + 0 si x = 1
3
28 + 0 + 0 si x = 2

y

fY (y) =

Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

3
9
3
28 + 28 + 28 si y = 0
3
3
14 + 14 + 0 si y = 1
1
28 + 0 + 0 si y = 2

Como ejemplos de las condicionadas (hay 6 en total) calculemos la funci√≥n masa de X condicionada a

Y = 1 y la de Y condicionada a X = 1.

fX|Y =1 (x) =

Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥

fY |X=1 (y) =

Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥

104

3
14
6
14
3
14
6
14

si x = 0

0

si x = 2

6
14

9
28
15
28
3
14
15
28

0
15
28

si x = 1 .

si y = 0
si x = 1 .
si x = 2
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Como es evidente, una vez que tenemos caracterizada la distribuci√≥n condicionada de una variable aleatoria
al valor de otra, cualquier caracter√≠stica de dicha distribuci√≥n, como la media o la varianza, puede calcularse
a partir de su funci√≥n masa o su funci√≥n de densidad.

Ejemplo. Tal y como plante√°bamos al comienzo del cap√≠tulo, supongamos que la posici√≥n (X, Y ) de un
tel√©fono m√≥vil que recibe cobertura de una antena de telefon√≠a se encuentra dentro de un c√≠rculo de radio

r alrededor de esa antena, que supondremos sin p√©rdida de generalidad que se encuentra en el origen
del plano. Vamos a suponer que esa posici√≥n es

completamente al azar

dentro del c√≠rculo. Eso equivale a

considerar que la densidad conjunta debe ser constante en el c√≠rculo; para que su integral sea la unidad,
es evidente que

fX,Y (x, y) =

1
œÄr2

si x2 + y 2 ‚â§ r2 y cero en cualquier punto fuera del c√≠rculo. Vamos a ver qu√© podemos averiguar sobre las
coordenadas X e Y por separado (marginales) y sobre c√≥mo afectan la una a la otra (condicionadas).
En primer lugar,

ÀÜ
fX (x) =

‚àö

r 2 ‚àíx2

‚àö
‚àí r 2 ‚àíx2

‚àö
2 r 2 ‚àí x2
1
dy
=
œÄr2
œÄr2

si ‚àír < x < r. La marginal de Y es an√°loga,

p
r2 ‚àí y2
fY (y) =
œÄr2
2

si ‚àír < y < r. Est√° claro que para cada coordenada por separado, los puntos m√°s

densos, m√°s probables,

son los cercanos al origen, que es donde se da el m√°ximo de ambas funciones.
Ahora supongamos que conocemos una de las coordenadas y veamos qu√© podemos decir sobre la otra:

fX|Y =y0 (x) =

fX,Y (x, y0 )
1
= p
2
fY (y0 )
2 r ‚àí y02

p
p
si ‚àí r2 ‚àí y02 < x < r2 ‚àí y02 . An√°logamente,
fY |X=x0 (y) =

fX,Y (x0 , y)
1
= p
fX (x0 )
2 r2 ‚àí x20

p
p
si ‚àí r2 ‚àí x20 < y < r2 ‚àí x20 . Si nos damos cuenta, ambas son distribuciones uniformes, lo que equivale
a decir que saber una coordenada no me da ninguna informaci√≥n sobre la otra coordenada.

Ejemplo. A las 12 de la noche de un d√≠a de la semana comienzan a ser registrados las nuevas llamadas
a un switch de telefon√≠a. Sea X el instante de llegada de la primera llamada, medida en segundos
transcurridos tras la medianoche. Sea Y el instante de llegada de la segunda llamada. En el modelo m√°s

Prof. Dr. Antonio Jos√© S√°ez Castillo

105

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

habitual utilizado en telefon√≠a, X e Y son variables aleatorias continuas con densidad conjunta dada por

(
fX,Y (x, y) =

Œª2 e‚àíŒªy si 0 ‚â§ x < y
0 en otro caso

,

donde Œª es una constante positiva. Vamos a calcular las distribuciones marginales y condicionadas que
pueden darse:
Marginal de X :

ÀÜ

‚àû

Œª2 e‚àíŒªy dy = Œªe‚àíŒªx si 0 ‚â§ x,

fX (x) =
x

luego se trata de una distribuci√≥n exponencial de par√°metro Œª.
Marginal de Y :

ÀÜ

y

Œª2 e‚àíŒªy dx = Œª2 ye‚àíŒªy si y ‚â• 0.

fY (y) =
0

Si nos jamos, esta densidad es una Gamma (2, Œª), es decir una Erlang de par√°metros 2 y Œª.
Condicionada de Y a los valores de X :

fY /X=x (y) =

fX,Y (x, y)
= Œªe‚àíŒª(y‚àíx) si y > x.
fX (x)

En esta expresi√≥n no debe olvidarse que x es un valor jo, dado.
Condicionada de X a los valores de Y :

fX/Y =y (x) =

1
fX,Y (x, y)
= si 0 ‚â§ x < y.
fY (y)
y

Es decir, conocido el instante en que lleg√≥ la segunda llamada (y), no se sabe nada de cu√°ndo lleg√≥
la primera llamada, ya que la distribuci√≥n de X condicionada a Y = y es uniforme en (0, y).

Ejemplo. Consideremos que la variable X representa el input de un canal de comunicaci√≥n, con posibles
valores +1 y ‚àí1 equiprobables, y sea Y el d√≠gito que llega al destino, con valores tambi√©n +1 y ‚àí1. El
canal es un canal binario sim√©trico con probabilidad de cruce del 5 %.
Con los datos expuestos podemos caracterizar mediante sus funciones masa las distribuciones marginales
de X e Y , la distribuci√≥n conjunta de ambos y las dos distribuciones condicionadas posibles de cada
variable respecto de la otra.
La distribuci√≥n marginal de X viene dada por

(
fX (x) =

106

1
2 si x = 1
1
2 si x = ‚àí1

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

La distribuci√≥n marginal de Y viene dada por

P [Y = +1] = P [Y = +1 | X = +1] P [X = +1] + P [Y = +1 | X = ‚àí1] P [X = ‚àí1]
= 0.95 √ó 0.5 + 0.05 √ó 0.5 = 0.5
P [Y = ‚àí1] = 0.5,
es decir

(
fY (y) =

1
2 si y = 1
1
2 si y = ‚àí1

La distribuci√≥n de Y condicionada al suceso X = +1 viene dada por:

(
fY |X=+1 (y) =

0.95 si y = 1
0.05 si y = ‚àí1

La distribuci√≥n de Y condicionada al suceso X = ‚àí1 viene dada por:

(
fY |X=‚àí1 (y) =

0.95 si y = ‚àí1
0.05 si y = 1

La distribuci√≥n conjunta de X e Y viene dada por

fX,Y (x, y) = P [Y = y | X = x] P [X = x]
Ô£±
Ô£¥
0.95 √ó 0.5 si x = +1, y = +1
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤ 0.05 √ó 0.5 si x = +1, y = ‚àí1
=
0.05 √ó 0.5 si x = ‚àí1, y = +1
Ô£¥
Ô£¥
Ô£¥ 0.95 √ó 0.5 si x = ‚àí1, y = ‚àí1
Ô£¥
Ô£¥
Ô£¥
Ô£≥
0
en otro caso
La distribuci√≥n de X condicionada al suceso Y = +1 viene dada por

fX,Y (x, +1)
fX|Y =+1 (x) =
=
fY (+1)

(

0.95 si x = 1
.
0.05 si x = ‚àí1

La distribuci√≥n de X condicionada al suceso Y = ‚àí1 viene dada por

fX,Y (x, ‚àí1)
fX|Y =‚àí1 (x) =
=
fY (‚àí1)

(

0.05 si x = 1
0.95 si x = ‚àí1

.

5.3. Independencia estad√≠stica
En el cap√≠tulo referente a probabilidad hablamos de independencia de sucesos. Dec√≠amos entonces que dos
sucesos A y B eran independientes si y s√≥lo si P [A ‚à© B] = P [A] ¬∑ P [B] .
Prof. Dr. Antonio Jos√© S√°ez Castillo

107

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Esta denici√≥n puede extenderse al caso en que tengamos dos variables aleatorias X e Y .

Concretamente, diremos que X

e Y son estad√≠sticamente independientes si y s√≥lo si
fX,Y (x, y) = fX (x) ¬∑ fY (y) ,

donde fX,Y (¬∑), fX (¬∑) y fY (¬∑) son funci√≥n de densidad o funci√≥n masa, dependiendo de si las variables son
discretas o continuas.
La interpretaci√≥n del hecho de que dos variables aleatorias sean estad√≠sticamente independientes es que el
comportamiento de una no tiene ning√∫n efecto sobre la otra y viceversa. Cabe preguntarse en ese caso, qu√©
sentido tiene una distribuci√≥n condicionada de una variable a otra que no guarda ninguna relaci√≥n con ella.
Vamos a comprobarlo calculando las distribuciones condicionadas de variables aleatorias estad√≠sticamente
independientes:

fX|Y =y (x) =

fX (x) ¬∑ fY (y)
fX,Y (x, y)
=
= fX (x) ;
fY (y)
fY (y)

es decir, el comportamiento aleatorio de una variable aleatoria condicionada al valor de otra que es estad√≠sticamente independiente de ella (descrito mediante la funci√≥n fX|Y =y (x)) es completamente igual que si no
se condiciona a dicho valor (descrito por la funci√≥n fX (x)).

Ejemplo. Sea el vector (X, Y ) con funci√≥n de densidad conjunta
(
fX,Y (x, y) =

24xy si x, y ‚â• 0 y x + y ‚â§ 1
0 en otro caso

.

La funci√≥n de densidad marginal de X :

ÀÜ

1‚àíx

2

24xy ¬∑ dy = 12x (1 ‚àí x) si 0 ‚â§ x ‚â§ 1

fX (x) =
0

La funci√≥n de densidad marginal de Y :

ÀÜ

1‚àíy

2

24xy ¬∑ dx = 12y (1 ‚àí y) si 0 ‚â§ y ‚â§ 1.

fY (y) =
0

Como

fX,Y (x, y) 6= fX (x) ¬∑ fY (y) ,
las variables X e Y no son independientes.

Ejemplo. Sea ahora el vector (X, Y ) con funci√≥n de densidad conjunta
(
fX,Y (x, y) =

108

4xy si 0 ‚â§ x, y y x, y ‚â§ 1
0 en otro caso

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

La funci√≥n de densidad marginal de X :

ÀÜ

1

4xy ¬∑ dy = 2x si 0 ‚â§ x ‚â§ 1

fX (x) =
0

La funci√≥n de densidad marginal de Y :

ÀÜ

1

4xy ¬∑ dx = 2y si 0 ‚â§ y ‚â§ 1.

fY (y) =
0

Como

fX,Y (x, y) = fX (x) ¬∑ fY (y) ,
las variables aleatorias X e Y son independientes.

Ejemplo. Supongamos que dos componentes electr√≥nicas tienen una duraci√≥n cuya distribuci√≥n de probabilidad puede considerarse exponencial de par√°metro Œª = 2 horas‚àí1 . Las componentes funcionan en
paralelo, por lo que podemos considerar que son independientes. Por lo tanto, su funci√≥n de densidad
conjunta ser√°

fX,Y (x, y) = 2e‚àí2x 2e‚àí2y = 4e‚àí2(x+y)
si x, y > 0.
¬æCu√°l ser√° la probabilidad de que alguna de las componentes dure m√°s de dos horas? Podemos plantearlo
como

P [X > 2 ‚à™ Y > 2] = P [X > 2] + P [Y > 2] ‚àí P [X > 2 ‚à© Y > 2]
= P [X > 2] + P [Y > 2] ‚àí P [X > 2] P [Y > 2] ,
donde se ha utilizado en la probabilidad de la intersecci√≥n el hecho de que las variables son independientes.
Ahora s√≥lo bastar√≠a recordar que P [X > 2] = e‚àí2√ó2 y P [Y > 2] = e‚àí2√ó2 .
¬æCu√°l ser√≠a la probabilidad de que la duraci√≥n total de ambas componentes sea inferior a dos horas? La
duraci√≥n total vendr√≠a dada por X + Y , luego se nos pregunta por

ÀÜ

2

ÀÜ

2‚àíx

4e‚àí2(x+y) dydx

P [X + Y < 2] =
0

ÀÜ

0
2

=

h

i
2e‚àí2x 1 ‚àí e‚àí2(2‚àíx) dx

0

ÀÜ

2


2e‚àí2x ‚àí 2e‚àí4 dx

=
0


= 1 ‚àí e‚àí4 ‚àí 2e‚àí4 √ó 2
= 1 ‚àí 5e‚àí4

De la interpretaci√≥n que hemos dado de variables independientes se sigue de manera inmediata que si dos
variables aleatorias son independientes, esto es, no mantienen ninguna relaci√≥n, tampoco lo har√°n funciones
Prof. Dr. Antonio Jos√© S√°ez Castillo

109

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

suyas. Este hecho se recoge en el siguiente resultado. Lo podemos enunciar m√°s formalmente diciendo que si

X e Y son variables aleatorias independientes y V = g (X) y W = h (Y ) son funciones suyas, entonces, V y
W tambi√©n son independientes.
En el √°mbito de las Telecomunicaciones se dan numerosas situaciones donde aparece una variable aleatoria

W , suma de otras dos variables aleatorias (generalmente continuas) estad√≠sticamente independientes, X
e Y, es decir, W = X + Y. Por ejemplo, se da cuando a una se√±al X se le adhiere un ruido que le es
completamente ajeno (independiente), Y . En ese caso, la suma representa la se√±al resultante y querremos
conocer su comportamiento aleatorio a partir del de X e Y . Esto se conoce como teorema

de convoluci√≥n.

Concretamente, sean X e Y dos variables aleatorias independientes y sea W = X + Y . Entonces:
Si X e Y son continuas,

ÀÜ

‚àû

fY (y) ¬∑ fX (w ‚àí y) ¬∑ dy

fW (w) =
‚àí‚àû

= fX ‚àó fY (w)
donde fX y fY son las funciones de densidad de X e Y , respectivamente.
Si X e Y son discretas,

fW (w) =

X

fY (y) ¬∑ fX (w ‚àí y)

y

= fX ‚àó fY (w)
donde fX y fY son las funciones masa de X e Y , respectivamente.

Ejemplo.

Un sistema opera con una componente clave cuya duraci√≥n, T1 , sigue una distribuci√≥n ex-

ponencial de par√°metro Œª. Si esta componente falla, inmediatamente se pone en funcionamiento una
componente exactamente igual que hasta entonces ha funcionado en standby, cuya duraci√≥n notamos por

T2 , variable aleatoria independiente de T1 .
Si pretendemos conocer la distribuci√≥n de probabilidad de la duraci√≥n total del sistema, que vendr√° dada
por la variable aleatoria T = T1 + T2 , podemos poner en pr√°ctica el teorema de convoluci√≥n. Para ello,
tengamos en cuenta que

fTi (x) = Œªe‚àíŒªx , i = 1, 2,
para x > 0. Por tanto,

ÀÜ

z

Œªe‚àíŒªx Œªe‚àíŒª(z‚àíx) dx = Œª2 ze‚àíŒªz

fT (z) =
0

para z > 0. Como vemos, se trata de una distribuci√≥n Erlang de par√°metros 2 y Œª. Si recordamos, esta
era una de las caracterizaciones de la distribuci√≥n Erlang, suma de exponenciales independientes.

0

En el caso de que en vez de dos variables aleatorias se tenga un vector X = (X1 , ..., XN ) , la manera natural
de extender el concepto de independencia es inmediata.

110

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Se dice que el vector est√° formado por

componentes independientes si

fX1 ,...,XN (x1 , ..., xN ) = fX1 (x1 ) ¬∑ ... ¬∑ fXN (xN ) .

Finalmente, si se tienen dos vectores aleatorios XN √ó1 e YM √ó1 , se dice que son

independientes si

fX,Y (x1 , ..., xN , y1 , ..., yM ) = fX (x1 , ..., xN ) fY (y1 , ..., yM ) .

5.4. Medias, varianzas y covarianzas asociadas a un vector aleatorio
Si tenemos un vector aleatorio formado por las variables aleatorias X1 , ..., XN y g (¬∑) es una funci√≥n de estas
variables, entonces, la

media o esperanza matem√°tica de esta funci√≥n es
ÀÜ

ÀÜ

‚àû

E [g (X1 , ..., XN )] =

‚àû

g (x1 , ..., xN ) ¬∑ fX1 ,...,XN (x1 , ..., xN ) ¬∑ dxN ¬∑ ... ¬∑ dx1

...
‚àí‚àû

‚àí‚àû

donde fX1 ,...,XN (x1 , ..., xN ) es la funci√≥n de densidad o la funci√≥n masa del vector aleatorio (entendiendo en
este √∫ltimo caso la integral como una suma).
Como consecuencia inmediata de esta denici√≥n, tenemos una primera e importante propiedad: este operador
esperanza multivariante tambi√©n es lineal, en el sentido que se recoge en el siguiente resultado.
0

Concretamente, podemos formalizarlo diciendo que si tenemos un vector aleatorio (X1 , ..., XN ) y Œ±1 , ..., Œ±N
escalares cualesquiera, entonces

E [Œ±1 X1 + ... + Œ±N XN ] = Œ±1 E [X1 ] + ... + Œ±N E [XN ] ,
es decir, la media de la suma ponderada es la suma ponderada de las medias. Podemos tratar de recordar
este resultado si pensamos que es exactamente la misma propiedad que tiene el operador integral, que

parte

las sumas y saca fuera los escalares.

5.4.1. Covarianza y coeciente de correlaci√≥n lineal
Anteriormente hemos comentado que estudiar vectores aleatorios desde una perspectiva estad√≠stica tiene
sentido, sobre todo, porque permite analizar las relaciones que se dan entre las variables del vector. Por
ejemplo, vimos c√≥mo los valores de una variable pueden afectar en mayor o menor medida a la distribuci√≥n
de probabilidad de las otras variables.
Sin embargo, ser√≠a muy interesante disponer de una medida num√©rica sencilla de calcular y de interpretar
para cuanticar al menos en parte cu√°l es el grado de relaci√≥n existente entre dos variables de un vector
aleatorio.
Prof. Dr. Antonio Jos√© S√°ez Castillo

111

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

En este sentido, dado el vector aleatorio (X, Y ), se dene la

correlaci√≥n entre X

e Y como

RXY = m11 = E [XY ] ,
a partir de la cual se puede calcular la

covarianza entre

X e Y como

Cov (X, Y ) = E [(X ‚àí EX) ¬∑ (Y ‚àí EY )] = E [XY ] ‚àí EX ¬∑ EY = RXY ‚àí EX ¬∑ EY.
La covarianza entre dos variables2 es una medida de la asociaci√≥n lineal existente entre ellas. Ser√° positiva si
la relaci√≥n entre ambas es directa (si crece una crece la otra) y negativa si es inversa (si crece una decrece la
otra); adem√°s, ser√° tanto mayor en valor absoluto cuanto m√°s fuerte sea la relaci√≥n lineal existente.
Para poder valorar esta relaci√≥n lineal en t√©rminos relativos se estandariza la covarianza, dando lugar a lo
que se conoce como

coeciente de correlaci√≥n lineal:

Cov [X, Y ]
œÅ= p
.
V ar [X] ¬∑ V ar [Y ]
Vamos a detallar claramente los posibles valores de œÅ y su interpretaci√≥n:
Este coeciente es siempre un n√∫mero real entre -1 y 1.
Si es cero, indica una ausencia total de relaci√≥n lineal entre las variables.
Si es uno o menos uno indica una relaci√≥n lineal total entre las variables, directa o inversa seg√∫n lo
indique el signo (esto lo veremos enseguida).
En la medida en que est√© m√°s lejos del cero indica una relaci√≥n lineal m√°s intensa entre las variables.

Si dos variables aleatorias tienen covarianza cero o equivalentemente, si RXY = EX ¬∑ EY, se dicen que son

incorreladas. Por su parte, si dos variables aleatorias son tales que RXY

= 0, se dice que son

ortogonales.

Dos variables aleatorias son incorreladas si carecen de cualquier tipo de relaci√≥n lineal. Por otra parte, denimos anteriormente el concepto de independencia entre variable aleatoria, que implicaba la ausencia de relaci√≥n
entre ellas. Tenemos, as√≠, dos conceptos, independencia e incorrelaci√≥n, que est√°n bastante relacionados.
En concreto, dos variable aleatoria independientes, X e Y , son siempre incorreladas, es decir, œÅX,Y = 0. La
raz√≥n es que, por ser independientes,

fX,Y (x, y) = fX (x) ¬∑ fY (y) ,
2 Si

se considera la covarianza de una variable aleatoria consigo misma,
h
i
Cov (X, X) = E [(X ‚àí EX) (X ‚àí EX)] = E (X ‚àí EX)2 = V arX,

esta cantidad coincide con su varianza.

112

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

luego

ÀÜ

‚àû

RXY =
‚àí‚àû
ÀÜ ‚àû

=

ÀÜ

‚àû

xy ¬∑ fX (x) ¬∑ fY (y) ¬∑ dy ¬∑ dx
ÀÜ ‚àû
yfY (y) dy = EX ¬∑ EY,
xfX (x) dx ¬∑
‚àí‚àû

‚àí‚àû

‚àí‚àû

en cuyo caso Cov [X, Y ] = 0.
La pregunta obvia que surge a la luz de este resultado es: ¬æ y al contrario? ¬æDos variable aleatoria incorreladas
ser√°n independientes? O equivalentemente, ¬æsi dos variable aleatoria no tienen ninguna relaci√≥n de tipo lineal
(incorreladas), ocurrir√° que tampoco tienen ninguna relaci√≥n de ning√∫n tipo (independientes)? La respuesta
es que no en general.

Ejemplo. Sea Œ± una variable aleatoria con distribuci√≥n uniforme en (0, 2œÄ). Sean
X = cos Œ±
Y = sin Œ±.
Se tiene que

ÀÜ

2œÄ

EX =

cos Œ±

1
dŒ± = 0
2œÄ

sin Œ±

1
dŒ± = 0
2œÄ

0

ÀÜ

2œÄ

EY =
0

ÀÜ

2œÄ

E [XY ] =

sin Œ± cos Œ±
0

1
=
2œÄ

ÀÜ

1
dŒ±
2œÄ

2œÄ

sin 2Œ±dŒ± = 0,
0

por lo que X e Y son variables incorreladas. Sin embargo, puede demostrarse f√°cilmente que no son
independientes.

Nota.

La relaci√≥n m√°s fuerte de tipo lineal que puede darse corresponde al caso en que una variable

aleatoria Y es exactamente una combinaci√≥n lineal de otra, X , es decir, Y = aX + b. En ese caso,

œÅXY = 1 ¬∑ signo (a) .
La demostraci√≥n es muy sencilla. Tengamos en cuenta que

 
E [XY ] = E [X (aX + b)] = aE X 2 + bE [X] ,

Prof. Dr. Antonio Jos√© S√°ez Castillo

113

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

luego

Cov (X, Y ) = E [XY ] ‚àí EX ¬∑ EY
 
= aE X 2 + bE [X] ‚àí EX (aEX + b)

 
= a E X 2 ‚àí EX 2 = aV arX
h
i
2
V arY = E ((aX + b) ‚àí (aEX + b))
h
i
h
i
2
2
= E (aX ‚àí aEX) = E a2 (X ‚àí EX)
h
i
2
= a2 E (X ‚àí EX) = a2 V arX,
y

œÅXY = ‚àö

aV arX
Cov (X, Y )
=‚àö
= 1 ¬∑ signo (a) .
V arX ¬∑ V arY
V arXa2 V arX

Nota. Es importante insistir en que la covarianza y su versi√≥n estandarizada, el coeciente de correlaci√≥n
lineal, proporcionan una medida de la relaci√≥n lineal, no de otro tipo. Por ejemplo, supongamos que la
Figura 5.3 representa los valores conjuntos de dos variables X e Y . Est√° claro que ambas guardan una
clar√≠sima relaci√≥n dada por una par√°bola: de hecho, Y = X 2 . Sin embargo, el coeciente de correlaci√≥n
lineal entre ambas ser√° muy bajo, ya que en realidad, la relaci√≥n que las une no es lineal en absoluto,
sino parab√≥lica. En este caso, lo recomendable ser√≠a, a la vista del gr√°co, decir que s√≠ existe una fuerte
‚àö
relaci√≥n lineal entre X e ¬± Y .

Figura 5.3: Muestra conjunta de valores de dos variables aleatorias.
Cuando se tienen muestras de pares de variables aleatorias, podemos calcular la versi√≥n muestral del coeciente de correlaci√≥n lineal. Esa versi√≥n muestral dar√° una estimaci√≥n del verdadero valor del coeciente de
correlaci√≥n (poblacional). Esta cuesti√≥n se aborda con m√°s detalle en el cap√≠tulo de regresi√≥n. Aqu√≠ tan s√≥lo
queremos plasmar con ejemplos c√≥mo se traduce el hecho de que dos variables tengan un mayor o menor
coeciente de correlaci√≥n. En la Figura 5.4 observamos representaciones conjuntas de muestras de pares de
variables en unos ejes cartesianos (nubes de puntos). Cada punto de cada eje cartesiano representa un valor

114

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

dado de la muestra del par (X, Y ). Aparecen 4 guras, correspondientes a 4 simulaciones de pares de variables

(X, Y ) con distintos coecientes de correlaci√≥n.

ro=1

ro=‚àí1

8

6

6

5
4

4

3
2
2
0

1

‚àí2
‚àí4
‚àí4

0
‚àí2

0

2

4

‚àí1
‚àí4

‚àí2

ro=0

0

2

4

2

4

ro=0.7075

4

6

3

4

2
1

2

0

0

‚àí1
‚àí2

‚àí2
‚àí3
‚àí4

‚àí2

0

2

4

‚àí4
‚àí4

‚àí2

0

Figura 5.4: Nubes de puntos correspondientes a distintos posibles coecientes de correlaci√≥n lineal.

Ejemplo. Sean X

e Y las variable aleatoria que miden el tiempo que transcurre hasta la primera y la

segunda llamada, respectivamente, a una centralita telef√≥nica. La densidad conjunta de estas variables
es fX,Y (x, y) = e‚àíy para 0 < x < y . En un ejemplo anterior ya vimos que, l√≥gicamente, el tiempo hasta
la segunda llamada depende del tiempo hasta la primera llamada, pero ¬æen qu√© grado? Vamos a abordar
este problema calculando el coeciente de correlaci√≥n lineal entre ambas variables.

Prof. Dr. Antonio Jos√© S√°ez Castillo

115

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Como œÅX,Y =

Cov(X,Y )
‚àö
V arXV arY

, tenemos que calcular Cov (X, Y ), V arX y V arY.
ÀÜ ÀÜ
E [XY ] =

xyfX,Y (x, y) dxdy
ÀÜ
ÀÜ ‚àûÀÜ y
xye‚àíy dxdy =
=
0
3

0

ÀÜ

‚àû

ye‚àíy

0



x2
2

y
dy
0

y ‚àíy
e dy = 3.
2

=
0

ÀÜ
fX (x) =

‚àû

ÀÜ

‚àû

e‚àíy dy = e‚àíx , para x > 0,

fX,Y (x, y) dy =
x

luego

ÀÜ

ÀÜ

EX =

‚àû

xe‚àíx dx = 1.

xfX (x) dx =
0

ÀÜ
fY (y) =

ÀÜ

y

e‚àíy dx = ye‚àíy , para y > 0,

fX,Y (x, y) dx =
0

luego

ÀÜ

ÀÜ

EY =

‚àû

y 2 e‚àíy dy = 2.

yfY (y) dy =
0

Por tanto,

Cov (X, Y ) = 3 ‚àí 1 √ó 2 = 1.
Por su parte,

ÀÜ
 
E X2 =

ÀÜ

‚àû

x2 e‚àíx dx = 2

2

x fX (x) dx =
0

V arX = 2 ‚àí 12 = 1
y

ÀÜ
 
E Y2 =

ÀÜ

‚àû

y 3 e‚àíy dy = 6

y 2 fY (y) dy =
0

V arY = 6 ‚àí 22 = 2,
as√≠ que, nalmente,

œÅX,Y = ‚àö

1
= 0.707.
1√ó2

El resultado indica que, en efecto, el grado de relaci√≥n lineal es alto y directo.

Las propiedades del operador esperanza son muy √∫tiles en la pr√°ctica, por ejemplo, cuando se trata de conocer
la varianza de combinaciones lineales de varias variables. Veamos alg√∫n ejemplo al respecto y despu√©s un
resultado general que los englobe todos.

116

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. Calculemos la varianza de X1 + X2 :
h
i


 
 
2
E (X1 + X2 ) = E X12 + X22 + 2X1 X2 = E X12 + E X22 + 2E [X1 X2 ]

h
i
2
2
V ar (X1 + X2 ) = E (X1 + X2 ) ‚àí E [X1 + X2 ]
 
 
2
= E X12 + E X22 + 2E [X1 X2 ] ‚àí (EX1 + EX2 )
 2
 2
= E X1 + E X2 + 2E [X1 X2 ] ‚àí EX12 ‚àí EX22 ‚àí 2EX1 EX2
 
 
= E X12 ‚àí EX12 + E X22 ‚àí EX22 + 2 (E [X1 X2 ‚àí EX1 EX2 ])
= V arX1 + V arX2 + 2Cov (X1 , X2 ) .

Ejemplo. Calculemos la varianza de X1 ‚àí X2 :
h
i


 
 
2
E (X1 ‚àí X2 ) = E X12 + X22 ‚àí 2X1 X2 = E X12 + E X22 ‚àí 2E [X1 X2 ]

h
i
2
2
V ar (X1 ‚àí X2 ) = E (X1 ‚àí X2 ) ‚àí E [X1 ‚àí X2 ]
 
 
2
= E X12 + E X22 ‚àí 2E [X1 X2 ] ‚àí (EX1 ‚àí EX2 )
 2
 2
= E X1 + E X2 ‚àí 2E [X1 X2 ] ‚àí EX12 ‚àí EX22 + 2EX1 EX2
 
 
= E X12 ‚àí EX12 + E X22 ‚àí EX22 ‚àí 2 (E [X1 X2 ‚àí EX1 EX2 ])
= V arX1 + V arX2 ‚àí 2Cov (X1 , X2 ) .

Podemos generalizar estos ejemplos en el siguiente resultado. Sea una suma de N ‚àívariables, X =

PN

i=1

Œ±i ¬∑Xi .

Entonces,

V ar [X] =

N X
N
X

Œ±i ¬∑ Œ±j ¬∑ Cov (Xi , Xj ) ,

i=1 j=1

donde Cov (Xi , Xi ) = V ar (Xi ), para i = 1, ..., N .
Prof. Dr. Antonio Jos√© S√°ez Castillo

117

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

La demostraci√≥n es bien sencilla. Como XÃÑ =

V ar [X] = E

h
"

=E

PN

X ‚àí XÃÑ
N
X

i=1

Œ±i ¬∑ EXi ,

2 i
!

Œ±i ¬∑ Xi ‚àí XÃÑi

i=1

=

N X
N
X



N
X

!#
Œ±i ¬∑ Xi ‚àí XÃÑi



i=1

Œ±i ¬∑ Œ±j ¬∑ E



Xi ‚àí XÃÑi



Xj ‚àí XÃÑj



i=1 j=1

=

N X
N
X

Œ±i ¬∑ Œ±j ¬∑ Cov (Xi , Xj )

i=1 j=1

Fij√©monos que, en el caso en que las variables sean incorreladas,

V ar [X] =

N X
N
X

Œ±i ¬∑ Œ±j ¬∑ Cov (Xi , Xj ) =

i=1 j=1

N
X

Œ±i2 ¬∑ V ar [Xi ] ,

i=1

ya que

(
Cov [X, Y ] =

0 si i 6= j

.

V ar [Xi ] si i = j

5.4.2. Vector de medias y matriz de varianzas-covarianzas de un vector
0

Dado un vector de N ‚àívariables, X = (X1 , ..., XN ) , se dene su

vector de medias como

Ô£´

Ô£∂
E [X1 ]
Ô£¨
Ô£∑
..
Ô£∑,
=Ô£¨
.
Ô£≠
Ô£∏
E [XN ]

¬µX

y su

matriz de varianzas-covarianzas como
CX = (Ci,j )i,j=1,...,N ,

donde

(
Ci,j =

V ar (Xi ) si i = j
Cov (Xi , Xj ) si i 6= j

.

Esta matriz contiene las varianzas de cada variable del vector en la diagonal y en el elemento (i, j) la covarianza
entre la i‚àí√©sima y la j‚àí√©sima variable.
En forma matricial, la matriz de covarianzas puede denirse como



0
CX N √óN = E (X ‚àí ¬µX )N √ó1 (X ‚àí ¬µX )1√óN .
Por otra parte,


0
CX = E (X ‚àí ¬µX ) (X ‚àí ¬µX ) = E [XX 0 ] ‚àí ¬µX ¬µ0X ,

118

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

donde a la matriz E [XX 0 ] se le suele denominar

matriz de correlaciones o de autocorrelaciones, y se

le nota RX .
Ambas matrices, CX y RX , son matrices sim√©tricas.
La linealidad del operador media facilita r√°pidamente la expresi√≥n del vector de medias y la matriz de
varianzas-covarianzas de combinaciones lineales de vectores, como se recoge en el siguiente resultado. Concretamente, si tenemos el vector aleatorio XN √ó1 con vector de medias ¬µX y matriz de varianzas covarianzas CX
y el vector YM √ó1 = AM √óN ¬∑ XN √ó1 + bM √ó1 , entonces, el vector de medias y la matriz de varianzas covarianzas
de Y vienen dadas por

¬µY = A¬µX + b
CY = ACX A0 .

Ejemplo.

Vamos a ver que la aplicaci√≥n de este resultado facilita bastante determinados c√°lculos. Por

ejemplo, si queremos calcular V ar (X1 + X2 ), podemos tener en cuenta que

X1 + X2 =



1

1

X1



!
,

X2

de manera que

V ar (X1 + X2 ) =



1

1

V arX1

Cov (X1 , X2 )

Cov (X1 , X2 )

V arX2



!

1

!

1

= V arX1 + V arX2 + 2Cov (X1 , X2 ) .
De igual forma, si queremos calcular V ar (5X1 ‚àí 3X2 ) , dado que

5X1 ‚àí 3X2 =



5

‚àí3



X1
X2

!
,

se tiene que

V ar (5X1 ‚àí 3X2 ) =



5

‚àí3



V arX1

Cov (X1 , X2 )

Cov (X1 , X2 )

V arX2

!

5

!

‚àí3

= 25V arX1 + 9V arX2 ‚àí 30Cov (X1 , X2 ) .

5.5. Distribuci√≥n normal multivariante
En el contexto de los modelos de distribuciones de probabilidad para variables aleatorias, la distribuci√≥n
normal constituye el ejemplo m√°s relevante, tanto por la frecuencia de su aplicaci√≥n en casos reales como por
la gran versatilidad de sus propiedades matem√°tica. En el contexto de los vectores aleatorios que estamos
tratando en este cap√≠tulo, nos ocupamos de la versi√≥n multivariante de esta distribuci√≥n. De nuevo podemos
Prof. Dr. Antonio Jos√© S√°ez Castillo

119

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

estar seguros de que se trata del caso m√°s interesante por dos motivos: porque aparece como modelo adecuado
en un gran n√∫mero de fen√≥menos de la naturaleza y porque sus propiedades matem√°ticas on inmejorables.
Un vector formado por N variables aleatorias X = (X1 , ..., XN ) se dice que sigue una distribuci√≥n
0

multivariante o distribuci√≥n conjuntamente normal o conjuntamente gaussiana,

normal

con vector de

medias ¬µX y matriz de varianzas-covarianzas CX , si su funci√≥n de densidad conjunta es de la forma

fX (x) = q

1
N

(2œÄ) det (CX )



1
0
‚àí1
¬∑ exp ‚àí (x ‚àí ¬µX ) ¬∑ CX (x ‚àí ¬µx ) ,
2

donde

CX = (Ci,j )i,j=1,...,N
(
V ar [Xi ] si i = j
Cij =
Cov [Xi , Xj ] si i 6= j
x = (x1 , ..., xN )

0

¬µX = (EX1 , ..., EXN )

0

y se nota X ‚Üí NN (¬µX ; CX ) .
Vamos a destacar algunas de las excelentes propiedades de la distribuci√≥n normal multivariante. Concretamente, nos centraremos en los siguientes resultados:
Cualquier marginal sigue tambi√©n una distribuci√≥n normal.
Cualquier distribuci√≥n condicionada sigue tambi√©n una distribuci√≥n normal.
Cualquier combinaci√≥n lineal de un vector normal es tambi√©n normal.
0

Vamos a concretarlos. En primer lugar, si tenemos un vector XN √ó1 = (X1 , ..., XN ) con distribuci√≥n conjuntamente gaussiana de vector de medias ¬µ y matriz de covarianzas CX , en ese caso, el subconjunto de variables
del vector, (Xi1 , ..., XiM ), con M < N tambi√©n sigue distribuci√≥n conjuntamente gaussiana, de par√°metros
0

(¬µi1 , ..., ¬µiM ) y matriz de covarianzas constituida por las las y las columnas de CX correspondientes a las
variables Xi1 , ..., XiM .

Ejemplo. Sea un vector (X1 , X2 , X3 )0 gaussiano, de vector de medias cero y matriz de covarianzas
Ô£´

Ô£∂

2

1

0

Ô£¨
Ô£≠ 1
0

3

Ô£∑
1 Ô£∏.
1

1

En aplicaci√≥n del resultado anterior, las marginales univariantes siguen las distribuciones siguientes:

X1 ‚Üí N (0, 2) , X2 ‚Üí N (0, 3) , X3 ‚Üí N (0, 1).

120

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Por su parte, las marginales bivariantes siguen las distribuciones siguientes:

0

0

(X1 , X2 ) ‚Üí N2

!
,

0
0

0

(X1 , X3 ) ‚Üí N2

!
,

0
0

0

(X2 , X3 ) ‚Üí N2

!
,

0

2

1

1

3

2

0

0

1

3

1

1

1

!!

!!

!!

En cuanto a las distribuciones condicionales, cualquier subconjunto de variables de un vector gaussiano
condicionado a los valores de cualquier otro subconjunto de variables del propio vector sigue distribuci√≥n
conjuntamente gaussiana. Concretamente, la distribuci√≥n de XN √ó1 condicionada a YM √ó1 = yM √ó1 , siendo
0

(X, Y )(M +N )√ó1 conjuntamente gaussiano, es gaussiana de vector de medias
‚àí1
E [X |Y=y ] = ¬µX
N √ó1 + (CXY )N √óM CY


M √óM

yM √ó1 ‚àí ¬µY
M √ó1



y matriz de varianzas-covarianzas



0
V ar X |Y=y = CX ‚àí CXY CY‚àí1 CXY
,
donde el elemento (i, j) de CXY es Cov (Xi , Yj ).

Ejemplo. Siguiendo con el ejemplo anterior, vamos a considerar la distribuci√≥n de X1
0

condicionada a

0

(X2 , X3 ) = (0.5, 0.25) .
Seg√∫n el resultado, √©sta es gaussiana, de vector de medias

E [X1 |X2 =0.5,

X3 =0.25 ]

=0+



1

0

=2‚àí





3

1

1

1

!‚àí1

0.5 ‚àí 0

!
= 0.125

0.25 ‚àí 0

y matriz de covarianzas (es decir, varianza)

V ar (X1 |X2 =0.5,

X3 =0.25 )

1

0



3

1

1

1

!‚àí1

1
0

!
= 1.5

Ejemplo. Como caso particular, vamos a describir con m√°s detalle el caso bivariante, tanto en lo que
respecta a su densidad como a las distribuciones marginales y condicionadas.
0

Sea por tanto un vector (X, Y )2√ó1 , con distribuci√≥n conjuntamente gaussiana de vector de medias

Prof. Dr. Antonio Jos√© S√°ez Castillo

121

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0

(¬µX , ¬µY ) y matriz de covarianzas
C(X,Y ) =
donde œÅ =

Cov(X,Y )
œÉX œÉY

2
œÉX

œÅœÉX œÉY

œÅœÉX œÉY

œÉY2

!
,


2 2
es el coeciente de correlaci√≥n lineal. Entonces, det C(X,Y ) = œÉX
œÉY 1 ‚àí œÅ2 y
‚àí1
C(X,Y
)

1
=
1 ‚àí œÅ2

1
2
œÉX
‚àí œÉXœÅœÉY

‚àí œÉXœÅœÉY
1
2
œÉY

!
.

Por tanto, la funci√≥n de densidad conjunta es

1
p
2œÄœÉX œÉY 1 ‚àí œÅ2
#)
"
(
2
2
2œÅ (x ‚àí ¬µx ) (y ‚àí ¬µY ) (y ‚àí ¬µY )
(x ‚àí ¬µX )
‚àí1
‚àí
.
¬∑ exp
+
2
2 (1 ‚àí œÅ2 )
œÉX
œÉX œÉY
œÉY2

fX,Y (x, y) =

Esta funci√≥n alcanza su m√°ximo,

en el punto (¬µX , ¬µY ).


2
y N ¬µY , œÉY2 .
Evidentemente, las distribuciones marginales son N ¬µX , œÉX
1‚àö
,
2œÄœÉX œÉY 1‚àíœÅ2

En lo que respecta a las distribuciones condicionadas, aplicando el √∫ltimo resultado tenemos que




œÉX
2
¬µX + œÅ
(y0 ‚àí ¬µY ) ; œÉX
1 ‚àí œÅ2
œÉY



œÉY
(x0 ‚àí ¬µX ) ; œÉY2 1 ‚àí œÅ2 .
Y | X = x0 ‚Üí N ¬µY + œÅ
œÉX

X | Y = y0 ‚Üí N

Obs√©rvese que, curiosamente, la varianza condicionada no depende del valor que condiciona. Esto tendr√°
importantes repercusiones m√°s adelante.

Continuando con las propiedades, una de las m√°s √∫tiles es su invarianza frente a transformaciones lineales.
0

Concretamente, si tenemos un vector aleatorio XN √ó1 = (X1 , ..., XN ) con distribuci√≥n gaussiana, vector de
medias ¬µX y matriz de covarianzas CX , entonces una combinaci√≥n lineal suya,

YM √ó1 = AM √óN ¬∑ XN √ó1 + bM √ó1
tiene distribuci√≥n gaussiana de vector de medias ¬µY = A ¬∑ ¬µX + b y matriz de covarianzas CY = A ¬∑ CX ¬∑ A0 .

Ejemplo.

Sean dos variable aleatoria X1 y X2 con distribuci√≥n conjuntamente gaussiana con medias

2
2
cero, varianzas œÉX
= 4 y œÉX
= 9 y covarianza, cX1 ,X2 = 3. Si estas variables se transforman linealmente
1
2

122

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 5.5: Ejemplos de densidades de la normal bivariantes con ¬µX = ¬µY = 0, œÉX = œÉY = 1 y œÅ = 0, 0.5,
‚àí0.5 y 0.9. (En http://www.ilri.org/InfoServ/Webpub/Fulldocs/Linear_Mixed_Models/AppendixD.htm).
Prof. Dr. Antonio Jos√© S√°ez Castillo

123

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

en las variables

Y1 = X1 ‚àí 2X2
Y2 = 3X1 + 4X2
las nuevas variables tienen distribuci√≥n conjuntamente gaussiana, con medias
0

(¬µY1 , ¬µY2 ) =

1

‚àí2

3

4

!

0

¬∑

!

0

=

0

!

0

y matriz de covarianzas

œÉY2 1

cY1 ,Y2

cY1 ,Y2

œÉY2 2

!
=

1

‚àí2

3

4

!

4

3

3

9

!

1

3

‚àí2

4

!
=

28

‚àí66

‚àí66

252

!

Otra de las m√°s importantes propiedades es que se trata del √∫nico caso en el que independencia e incorrelaci√≥n
son equivalentes. Es decir, si XN √ó1 es un vector con distribuci√≥n conjuntamente gaussiana, entonces sus
componentes son incorreladas si y s√≥lo si son independientes.
La demostraci√≥n es sencilla. Ya sabemos que si son independientes son incorreladas (incluso si la distribuci√≥n
no es conjuntamente gaussiana). Por su parte, para probar que si son incorreladas entonces son independientes
s√≥lo hay que tener en cuenta que si son incorreladas, la matriz de covarianzas es diagonal y la densidad
conjunta puede expresarse como producto de las marginales, ya que



1

fX (x1 , ..., xN ) = q
N
(2œÄ) det (CX )

(

1

=q
N
2
(2œÄ) œÉ12 ...œÉN
=

N
Y


1
0 ‚àí1
exp ‚àí (x ‚àí ¬µX ) CX (x ‚àí ¬µX )
2
N

1X
exp ‚àí
2 i=1



xi ‚àí ¬µi
œÉi

2 )

fXi (xi ) .

i=1
0

0

donde x = (x1 , ..., xN ) , ¬µX = (¬µ1 , ..., ¬µN ) y

Ô£´

CX

124

œÉ12
Ô£¨ .
.
=Ô£¨
Ô£≠ .
0

...
..
.
...

Ô£∂
0
.. Ô£∑
Ô£∑
. Ô£∏.
2
œÉN

Prof. Dr. Antonio Jos√© S√°ez Castillo

Parte III
Inferencia estad√≠stica

125

Cap√≠tulo 6
Distribuciones en el muestreo

Pocas observaciones y mucho razonamiento conducen al error; muchas observaciones y poco
razonamiento, a la verdad.
Alexis Carrel

Resumen.

En este cap√≠tulo se pretende llamar la atenci√≥n acerca de que los par√°metros muestrales son

en realidad variables aleatorias. Se analiza as√≠ la distribuci√≥n de probabilidad de la media muestral y de la
varianza muestral en diversas situaciones.

Palabras clave: distribuciones en el muestreo, t de Student, F de Snedecor.

6.1. Introducci√≥n
Al estudiar el concepto de variable aleatoria, dijimos que viene motivado porque muchas de las variables que
se observan en la vida real, en el ambiente de las Ingenier√≠as en particular, est√°n sujetas a incertidumbre.
Eso quiere decir que si nosotros obtenemos algunas observaciones de esas variables (muestras), los datos
no son iguales. Es m√°s, si obtenemos otras observaciones, las dos muestras tampoco ser√°n ni mucho menos
id√©nticas.
Por tanto, al hablar de distribuciones te√≥ricas de probabilidad, lo que pretend√≠amos era proponer un modelo
que permitiera calcular probabilidades asociadas, no a una muestra en particular de datos, sino a todas las
posibles muestras, con todos los posibles datos de la variable.
Recordemos el ejemplo que pusimos: las distribuciones de probabilidad son como un traje que elegimos para
ponernos cualquier d√≠a durante un periodo de tiempo amplio. En la medida que el traje de una variable,
su distribuci√≥n,

le quede bien,

los resultados que obtengamos mediante el c√°lculo de probabilidades podr√°n

aplicarse a cualquier dato o conjunto de datos de la variable. Pero igualmente, si un traje (una distribuci√≥n
de probabilidad te√≥rica)

no le queda bien

a una variable, los resultados te√≥ricos, obtenidos a partir de una

funci√≥n masa o una funci√≥n de densidad te√≥ricas, pueden no ser realistas respecto a los resultados emp√≠ricos
que se obtengan mediante muestras de la variable.
¬æQu√© nos queda por hacer a lo largo del curso? Dado que, en general, las distribuciones te√≥ricas de probabilidad
dependen de uno o m√°s par√°metros, lo que nos ocupar√° gran parte del resto del curso es tratar de elegir
127

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

adecuadamente esos par√°metros. En el ejemplo de los trajes pod√≠amos pensar que esto es como aprender a
escoger la talla del traje.
En este cap√≠tulo vamos a comenzar con algunas cuestiones te√≥ricas acerca de lo que implica el proceso de
muestreo, previo a la elecci√≥n de los par√°metros y, posteriormente, nos vamos a centrar en resultados que
implica el muestreo de datos de variables que siguen una distribuci√≥n normal.

6.2. Muestreo aleatorio
En multitud de √°mbitos de la vida real es evidente que la mejor forma de aprender algo es a partir de la
experiencia. Eso quiere decir que solemos utilizar aquello que vemos para aprender pautas y conductas que
luego generalizamos.
En Estad√≠stica pasa algo muy similar: necesitamos basarnos en muestras de una variable para poder aprender
de ellas y generalizar, inferir, aspectos referentes a las muestras a toda la poblaci√≥n.
Sin embargo, como en la vida real, en Estad√≠stica tambi√©n debemos ser muy cuidadosos con los datos sobre los
que basamos nuestro aprendizaje. ¬æQu√© pasar√≠a si basamos nuestro aprendizaje en experiencias incorrectas o
poco signicativas?
Para que esto no ocurra debemos basarnos en muestras donde todos los individuos de la poblaci√≥n puedan
verse representados. Por otra parte, es evidente que cuanto mayores sean las muestras m√°s ables deber√≠an
ser nuestras inferencias.
El concepto clave en este planteamiento es el de

muestra aleatoria simple.

Supongamos que estamos obser-

vando una variable aleatoria, X , en una poblaci√≥n determinada. Ya dijimos que una muestra aleatoria simple
de X consiste en la recopilaci√≥n de datos de la variable, mediante la repetici√≥n del experimento al que est√°
asociada, con dos condiciones b√°sicas:
1. Que todos los elementos de la poblaci√≥n tengan las mismas posibilidades de salir en la muestra.
2. Que las distintas observaciones de la muestra sean independientes entre s√≠.
En ese caso, los valores que toma la variable en cada una de las observaciones de una muestra de tama√±o

n, X1 , ..., Xn , son en s√≠ mismos, variables aleatorias independientes que siguen la misma distribuci√≥n de
probabilidad, llamada

distribuci√≥n poblacional.

Esta distribuci√≥n es, en principio, desconocida, por lo

que se intentar√° utilizar la muestra para hacer inferencia sobre ella y, al menos, aproximar la forma de esta
distribuci√≥n.

6.3. Distribuciones en el muestreo
Supongamos que estamos observando una variable aleatoria X , y que obtenemos una muestra aleatoria
simple suya, x11 , ..., x1n . Con esos datos podemos calcular la media de la muestra, xÃÑ1 , y la desviaci√≥n t√≠pica de
la muestra, s1 , por ejemplo.
Pero debemos ser conscientes de lo que signica muestra

x11 , ..., x1n

aleatoria.

El hecho de que hayan salido los valores

es fruto del azar. De hecho, si obtenemos otra muestra, x21 , ..., x2n , obtendremos otra media, xÃÑ2 y

otra desviaci√≥n t√≠pica de la muestra, s2 .

128

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Y si, sucesivamente, obtenemos una y otra muestra, obtendremos una y otra media muestral, y una y otra
desviaci√≥n t√≠pica muestral. Por lo tanto, en realidad, lo que estamos viendo es que la media y la varianza
muestrales (y en general, cualquier par√°metro de una muestra aleatoria simple) son, en realidad, variables
aleatorias que, como tales, deben tener su distribuci√≥n, su media, su varianza...
Vamos a recordar dos deniciones que ya introdujimos al comienzo del curso.
Un

par√°metro muestral

es un par√°metro (media, varianza, ...) referido a una muestra de una variable

aleatoria.
Un

par√°metro poblacional es un par√°metro (media, varianza, ...) referido a la distribuci√≥n poblacional de

una variable aleatoria.
Pues bien, asociados a estos dos conceptos tenemos ahora las siguientes deniciones.
La
El

distribuci√≥n en el muestreo de un par√°metro muestral es su distribuci√≥n de probabilidad.
error estandar de un par√°metro muestral es la desviaci√≥n t√≠pica de su distribuci√≥n en el muestreo.

El problema es que, en general, es bastante dif√≠cil conocer la distribuci√≥n en el muestreo de los par√°metros
muestrales.
Sin embargo, el caso en el que resulta m√°s sencillo hacerlo es probablemente el m√°s importante. Como vamos
a ver, si la variable que observamos sigue una distribuci√≥n normal, podremos conocer de forma exacta las
distribuciones en el muestreo de los dos par√°metros m√°s importantes, la media y la varianza.
¬æY si la variable no es normal? Si lo que pretendemos es estudiar la media y la varianza muestrales, recordemos
que el Teorema Central del L√≠mite nos dice que si una variable es suma de otras variables, su distribuci√≥n es
aproximadamente normal, y la media es suma de las variables de la muestra. Es decir, si la variable no es
normal, todav√≠a podemos tener conanza de que lo que hagamos para variables normales puede ser v√°lido.

6.4. Distribuciones en el muestreo relacionadas con la distribuci√≥n
normal
En este apartado simplemente vamos a presentar una serie de resultados acerca de la distribuci√≥n en el
muestreo, es decir, acerca de las distribuciones de probabilidad, de algunos par√°metros muestrales que pueden
obtenerse asociados a una variable aleatoria normal.
Algunas de estas distribuciones aparecen por primera vez, as√≠ que debemos denirlas previamente. Por otra
parte, sus funciones de densidad son bastante poco tratables. Esto no es ning√∫n problema hoy en d√≠a, gracias
al uso que podemos hacer de los ordenadores para cualquier c√°lculo. Adem√°s, para poder trabajar con ellas
cuando no tenemos un ordenador a mano, existen tablas que pueden ser impresas en papel con muchos valores
de sus funciones de distribuci√≥n.

Nota. Una de las primeras distribuciones en el muestreo ser√° la œá2 . Recordemos que una distribuci√≥n œá2 con
n grados de libertad es una distribuci√≥n Gamma de par√°metros
Prof. Dr. Antonio Jos√© S√°ez Castillo

n
2

y 12 .

129

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Si Z es una variable aleatoria normal estandar y S una œá2 con n grados de libertad, siendo ambas independientes, entonces

sigue una distribuci√≥n llamada t

Z
t= p
S/n

de student con n grados de libertad.

Si S1 y S2 son variables aleatorias con distribuci√≥n œá2 con n1 y n2 grados de libertad independientes, entonces

F =
sigue una distribuci√≥n que se denomina F

S1 /n1
S2 /n2

con n1 y n2 grados de libertad.

Con estas deniciones ya podemos dar las distribuciones en el muestreo de algunos par√°metros muestrales
importantes asociados a la normal:
Sea X1 , ..., Xn una muestra aleatoria simple de una variable N (¬µ, œÉ). Entonces, el par√°metro muestral

t=

XÃÑ ‚àí ¬µ
‚àö
Sn‚àí1 / n

sigue una t de Student con n ‚àí 1 grados de libertad.
Sea una muestra X1 , ..., Xn una muestra aleatoria simple de una variable N (¬µ, œÉ). Entonces, el par√°metro muestral

œá2 =

2
(n ‚àí 1) Sn‚àí1
œÉ2

sigue una œá2 con n ‚àí 1 grados de libertad.
Sean X1 , ..., Xn1 e Y1 , ..., Yn2 muestras aleatorias simples de variables independientes con distribuciones

N (¬µ1 , œÉ) y N (¬µ2 , œÉ). Entonces, el par√°metro muestral

XÃÑ ‚àí YÃÑ ‚àí (¬µ1 ‚àí ¬µ2 )
q
t=
,
Sp n11 + n12
donde

Sp2 =

2
2
1
2
(n1 ‚àí 1) Sn‚àí1
+ (n2 ‚àí 1) Sn‚àí1
,
n1 + n2 ‚àí 2

sigue una t de Student con n1 + n2 ‚àí 2 grados de libertad.
Sean X1 , ..., Xn1 e Y1 , ..., Yn2 muestras aleatorias simples de variables independientes con distribuciones

N (¬µ1 , œÉ) y N (¬µ2 , œÉ). Entonces, el par√°metro muestral
œá2 =

(n1 + n2 ‚àí 2) Sp2
,
œÉ2

sigue una œá2 n1 + n2 ‚àí 2 grados de libertad.
Sean X1 , ..., Xn1 e Y1 , ..., Yn2 muestras aleatorias simples de variables independientes con distribuciones

130

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

N (¬µ1 , œÉ) y N (¬µ2 , œÉ). Entonces, el par√°metro muestral
F =

1
Sn‚àí1

2

/œÉ12

2
Sn‚àí1

2

/œÉ22

sigue una distribuci√≥n F con n1 ‚àí 1 y n2 ‚àí 1 grados de libertad.

Prof. Dr. Antonio Jos√© S√°ez Castillo

131

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

132

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 7
Estimaci√≥n de par√°metros de una
distribuci√≥n

¬ΩDatos, datos, datos! -grit√≥ impacientemente-. No puedo hacer ladrillos sin arcilla.
Sherlock Holmes (A. C. Doyle), en

Resumen.

Las aventuras de los bombachos de cobre

Se describen las t√©cnicas m√°s usuales para estimar la media, la varianza y otros par√°metros

poblacionales mediante valores aislados (estimaci√≥n puntual) o mediante intervalos de conanza.

Palabras clave: estimador puntual, m√©todo de los momentos, m√©todo de m√°xima verosimilitud, intervalo
de conanza, nivel de conanza.

7.1. Introducci√≥n
En Estad√≠stica hay tres formas de inferir un valor a un par√°metro de una poblaci√≥n:
Estimando el valor concreto de ese par√°metro.
Estimando una regi√≥n de conanza para el valor del par√°metro.
Tomando una decisi√≥n sobre un valor hipot√©tico del par√°metro.

Ejemplo. El rendimiento de un equipo de trabajo en una cadena de producci√≥n puede estar representado
por el n√∫mero medio de componentes producidas. Supongamos que un ingeniero pretende proporcionar
informaci√≥n acerca de este promedio en su equipo. Existen varias posibilidades:
Podr√≠a simplemente tratar de estimar el promedio de componentes producidas a trav√©s de un √∫nico
valor estimado.
Podr√≠a proporcionar un intervalo de valores en el que tenga mucha conanza que se encuentra el
valor promedio.
133

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Podr√≠a comparar el valor promedio de su equipo con un valor hipot√©tico para, por ejemplo, demostrar a la empresa que tiene un mejor rendimiento que el promedio general de la empresa.

En este cap√≠tulo nos centraremos en la primera y la segunda forma, que consisten en proporcionar un valor
que creemos que est√° cerca del par√°metro (estimaci√≥n puntual) o en proporcionar un intervalo en el que
conamos que se encuentra el par√°metro desconocido (estimaci√≥n por intervalos de conanza). La tercera
posibilidad se estudiar√° en el cap√≠tulo de contrastes de hip√≥tesis.

7.2. Estimaci√≥n puntual
7.2.1. Denici√≥n y propiedades deseables de los estimadores puntuales
Un estimador puntual, Œ∏ÃÇ, es una regla que nos dice c√≥mo calcular una estimaci√≥n num√©rica de un par√°metro
poblacional desconocido, Œ∏, a partir de los datos de una muestra. El n√∫mero concreto que resulta de un c√°lculo,
para una muestra dada, se denomina

estimaci√≥n puntual.

Ejemplo. Si deseamos obtener estimaciones de la media de una variable aleatoria, lo que parece m√°s l√≥gico
ser√≠a utilizar como estimador la media muestral. Cada media muestral de cada muestra ser√≠a una estimaci√≥n
puntual de la media poblacional.
¬æQu√© ser√≠a deseable que le pasara a cualquier estimador? ¬æQu√© buenas propiedades deber√≠a tener un buen
estimador? Vamos a ver dos de ellas.
En primer lugar, parece l√≥gico pensar que si bien el estimador no proporcionar√° siempre el valor exacto del
par√°metro, al menos deber√° establecer estimaciones que
defecto. Este tipo de estimadores se denominan
Un estimador Œ∏ÃÇ de un par√°metro Œ∏ se dice

se equivoquen

en igual medida por exceso que por

insesgados .

insesgado si
h i
E Œ∏ÃÇ = Œ∏.

Se denomina

sesgo de un estimador a E

 h i


ŒòÃÇ ‚àí Œ∏ .

Observemos que para comprobar si un estimador es insesgado, en principio es necesario conocer su distribuci√≥n
en el muestreo, para poder calcular su esperanza matem√°tica.
Adem√°s de la falta de sesgo, nos gustar√≠a que la distribuci√≥n de muestreo de un estimador tuviera poca
varianza, es decir, que la dispersi√≥n de las estimaciones con respecto al valor del par√°metro poblacional, fuera
baja.
En este sentido, se dene el error
y se nota

134

estandar de un estimador como la desviaci√≥n t√≠pica de dicho estimador,

s.e.

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

El

estimador insesgado de m√≠nima varianza de un par√°metro Œ∏ es el estimador Œ∏ÃÇ que tiene la varianza

m√°s peque√±a de entre todos los estimadores insesgados.
Hay que decir que no siempre es f√°cil encontrar este estimador, y que en ocasiones se admite un ligero sesgo
con tal que la varianza del estimador sea m√≠nima.

7.2.2. Estimaci√≥n de la media de una v.a. La media muestral
Sea una v.a. X , y una muestra aleatoria suya, X1 , ..., XN . Entonces, la media muestral,

XÃÑ =

X1 + ... + XN
N

es un estimador insesgado de E [X] y su error estandar es

œÉX
s.e.(XÃÑ) = ‚àö .
N
El resultado establece algo que pod√≠a haberse intuido desde la denici√≥n de la media o esperanza matem√°tica
de una distribuci√≥n de probabilidad: si tenemos unos datos (mas ) de una v.a., una estimaci√≥n adecuada de
la media de la v.a. es la media de los datos.
Hay que tener mucho cuidado con no confundir la media de la v.a., es decir, la media poblacional, con la
media de los datos de la muestra, es decir, con la media muestral.
Por otra parte, el error estandar hace referencia a œÉX , que es un par√°metro poblacional y, por lo tanto,
desconocido. Lo que se suele hacer es considerar la desviaci√≥n t√≠pica muestral como una aproximaci√≥n de la
poblacional para evaluar este error estandar.

7.2.3. Estimaci√≥n de la varianza de una v.a. Varianza muestral
Sea una v.a. X y una muestra aleatoria simple suya, X1 , ..., XN . Entonces, la varianza muestral,
2
SX,N
‚àí1 =

PN

Xi ‚àí XÃÑ
N ‚àí1

2

i=1

es un estimador insesgado de V ar [X].

Nota. Al hilo del comentario previo que hicimos sobre la media muestral como estimador natural

de la

media, ahora quiz√° sorprenda que en el denominador de la varianza muestral aparezca N ‚àí 1 y no N .
En este sentido, si consideramos el estimador
2
SX,N

Prof. Dr. Antonio Jos√© S√°ez Castillo

PN
=

i=1

Xi ‚àí XÃÑ
N

2
,

135

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

se tratar√≠a de un estimador no insesgado. A este estimador de la varianza se le conoce habitualmente
como

cuasivarianza muestral. Ojo, hay que advertir que en algunos libros la manera de nombrar a la

varianza y a la cuasivarianza muestrales es justo al contrario.

Nota.

2
El que la varianza muestral, SN
, sea un estimador insesgado de la varianza, œÉ 2 , no implica que la
q ‚àí1
2
desviaci√≥n t√≠pica muestral, SN ‚àí1 = SN
‚àí1 , sea un estimador insesgado de œÉ , pero en este caso s√≠ ocurre as√≠.

Ejemplo. Mediante R hemos generado una muestra aleatoria simple de 1000 valores de una distribuci√≥n
N (0, 1). Sabemos, por tanto, que la media (poblacional) de los datos es 0 y que la varianza (poblacional)
es 1. No obstante, vamos a suponer que desconocemos de qu√© distribuci√≥n proceden los datos y vamos a
tratar de

ajustar

una distribuci√≥n te√≥rica partiendo de los valores de la muestra:

x1√ó1000 = (‚àí0.9459, ‚àí0.9557, 0.2711, 0.2603, 1.014, ...)
Para empezar, debemos pensar en una distribuci√≥n adecuada. Para ello puede observarse el histograma
de los datos por si √©ste recuerda la forma de alguna funci√≥n de densidad conocida. En este caso, el
histograma de la muestra aparece en la Figura 7.1, histograma que recuerda claramente la funci√≥n de
densidad de una distribuci√≥n normal.
La pregunta inmediata una vez que se opta por ajustar mediante una distribuci√≥n normal es ¬æqu√© normal?
Es decir, ¬æqu√© media y qu√© varianza se proponen para la distribuci√≥n que queremos ajustar a estos datos?
Una respuesta a esta pregunta la proporcionan los estimadores insesgados que hemos encontrado para
estos par√°metros. Concretamente,

xÃÑ = ‚àí0.0133
y

s999 = 0.9813,
por lo que ajustar√≠amos los datos de la muestra x mediante una distribuci√≥n

N (‚àí0.0133, 0.9813) .
La densidad de esta distribuci√≥n aparece tambi√©n en la Figura 7.1, en trazo continuo, y se observa que
ajusta muy bien la forma del histograma.

136

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

0.0

0.1

0.2

Densidad

0.3

0.4

0.5

Histograma de la muestra

‚àí3

‚àí2

‚àí1

0

1

2

3

Figura 7.1: Histograma para la muestra x1√ó1000 con 30 intervalos y funci√≥n de densidad de la distribuci√≥n
N (‚àí0.0133, 0.9813).

7.2.4. Estimaci√≥n de una proporci√≥n poblacional
Supongamos que deseamos estimar una proporci√≥n p, desconocida, que representa la probabilidad de un
suceso dentro de un espacio muestral. Para ello, se realizan N experimentos asociados al espacio muestral y
se cuenta el n¬∫ de veces que ocurre ese suceso del cu√°l queremos estimar su probabilidad, k . En ese caso, la
proporci√≥n muestral,

pÃÇ =

k
,
N

es un estimador insesgado de p. Adem√°s, su error estandar es

r
s.e.(pÃÇ) =

p(1 ‚àí p)
N

Sobre el error estandar, obs√©rvese de nuevo que, dado que p es desconocido, en realidad la expresi√≥n de s.e.(pÃÇ)
no puede evaluarse. Sin embargo, es bastante com√∫n que si el tama√±o de la muestra, N , es grande, se utilice
el valor de la estimaci√≥n, pÃÇ, en lugar de p en esa expresi√≥n.
De todas formas, obs√©rvese tambi√©n que la funci√≥n f (p) = p(1 ‚àí p) es menor que

r
s.e.(pÃÇ) ‚â§
Es por ello que siempre podemos dar esta cantidad,

1
4

si 0 ‚â§ p ‚â§ 1, luego

1
1
= ‚àö .
4N
2 N

1
‚àö
,
2 N

como cota superior del error estandar.

Ejemplo. Si el n√∫mero de varones en una muestra de 1000 individuos de una poblaci√≥n es 507, podemos
aproximar la verdadera proporci√≥n de varones en toda la poblaci√≥n mediante

pÃÇ =
con un error estandar por debajo de

‚àö1
2 1000

Prof. Dr. Antonio Jos√© S√°ez Castillo

507
= 0.507,
1000
= 0.01581139. La estimaci√≥n del error estandar de la

137

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

estimaci√≥n ser√≠a

p
0.507 √ó 0.493/1000 = 0.01580984: en este caso, las diferencias son inapreciables.

7.2.5. Obtenci√≥n de estimadores puntuales. M√©todos de estimaci√≥n
Hasta ahora hemos puesto un ejemplo acerca de la estimaci√≥n de la media o la varianza de una poblaci√≥n
mediante la media y la varianza muestral. Sin embargo, nosotros hemos visto muchas distribuciones te√≥ricas
que no dependen directamente de la media o la varianza. Por ejemplo, la binomial depende de p, la Gamma
de dos par√°metros, a y Œª, ... ¬æC√≥mo obtener estimadores de estos par√°metros?
Existen diversos m√©todos de estimaci√≥n de par√°metros. Nosotros vamos a ver dos de los m√°s sencillos.

7.2.5.1. M√©todo de los momentos
Vamos a explicar el m√©todo s√≥lo para distribuciones de uno o dos par√°metros poblacionales, que son las
√∫nicas que hemos visto nosotros.
Sea x1 , ..., xn una muestra de una variable aleatoria X :
1. Si la distribuci√≥n de X depende de un s√≥lo par√°metro, Œ∏, la media poblacional de X, E [X] = ¬µ, ser√°
funci√≥n de Œ∏, ¬µ = f (Œ∏). En ese caso, el estimador mediante
  el m√©todo de los momentos de Œ∏, Œ∏ÃÇ, se
obtiene despej√°ndolo (si es posible) de la ecuaci√≥n xÃÑ = f Œ∏ÃÇ .
2. Si la distribuci√≥n de X depende de dos par√°metros, Œ∏1 y Œ∏2 , la media poblacional de X, E [X] = ¬µ, ser√°
funci√≥n de ambos, ¬µ = f (Œ∏1 , Œ∏2 ) e igualmente la varianza poblacional estar√° expresada como funci√≥n
de estos par√°metros, V arX = œÉ 2 = g (Œ∏1 , Œ∏2 ). En ese caso, los estimadores mediante el m√©todo de los
momentos de Œ∏1 y Œ∏2 , Œ∏ÃÇ1 y Œ∏ÃÇ2 , se obtienen despej√°ndolos (si es posible) del sistema de ecuaciones



xÃÑ = f Œ∏ÃÇ1 , Œ∏ÃÇ2


s2n‚àí1 = g Œ∏ÃÇ1 , Œ∏ÃÇ2 .

Ejemplo. En la distribuci√≥n binomial sabemos que EX = np, por lo que p =

EX
n .

Por tanto, dada una

muestra de tama√±o N de la variable, el m√©todo de los momentos propone como estimador de p a

pÃÇ =

xÃÑ
.
n

Por cierto, este estimador coincide con el que hab√≠amos considerado en un principio, que era la proporci√≥n
muestral, es decir, pÃÇ = k/N , pero puede haber alguna confusi√≥n en la notaci√≥n. Veamos porqu√©.
Se supone que tenemos una muestra de tama√±o N de datos de una binomial de par√°metro n, es decir,
P
tenemos n experimentos, N veces, o sea, un total de n √ó N experimentos, con i xi √©xitos. Luego, en
efecto,

pÃÇ =

P
xÃÑ
i xi
=
,
n
n√óN

es decir, la proporci√≥n muestral, cociente del n¬∫ de √©xitos entre el n¬∫ total de experimentos. No debemos
confundirnos con la expresi√≥n k/N que pusimos antes porque N no signica lo mismo en ambos casos.

138

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. En la distribuci√≥n geom√©trica sabemos que EX =

1
p

‚àí 1, de donde p =

1
1+EX ,

luego el m√©todo

de los momentos propone como estimador a

pÃÇ =

1
.
1 + xÃÑ

Ejemplo. En el caso de la binomial negativa tenemos dos par√°metros. Se sabe que
a (1 ‚àí p)
p
a (1 ‚àí p)
V arX =
p2
EX =

De esta expresi√≥n debemos despejar a y p. Dado que

EX
= p,
V arX
se tiene que

a = EX √ó

EX
p
EX 2
= EX √ó V arX
=
1‚àíp
V arX ‚àí EX
1 ‚àí VEX
arX

de donde se proponen como estimadores

pÃÇ =
aÃÇ =

xÃÑ
s2X,N ‚àí1
xÃÑ2
s2X,N ‚àí1 ‚àí xÃÑ

.

7.2.5.2. M√©todo de m√°xima verosimilitud
Este m√©todo obedece a un principio muy l√≥gico: dada una muestra, escojamos como estimaciones aquellos
valores de los par√°metros que hagan

m√°s creibles, m√°s veros√≠miles,

los datos de la muestra.

Para desarrollar el m√©todo debemos tener en cuenta que si tenemos una muestra aleatoria simple de una
variable X , x1 , ..., xn , y la funci√≥n masa o densidad de la variable es p (x), entonces la funci√≥n masa o
densidad de la muestra es

p (x1 , ..., xn ) = p (x1 ) ...p (xn ) .
Esta funci√≥n masa o densidad representa en cierto modo la

credibilidad

de los datos de la muestra.

Dada una variable aleatoria X con funci√≥n masa o funci√≥n de densidad p (x) , que depende de uno
o dos par√°metros, y una muestra aleatoria simple de X , x1 , ..., xn , la verosimilitud de la muestra
es la funci√≥n

L = p (x1 ) ...p (xn ) ,
funci√≥n que depender√° de los par√°metros desconocidos de la variable.
Prof. Dr. Antonio Jos√© S√°ez Castillo

139

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Dada la verosimilitud de una muestra, L,
si L depende de un s√≥lo par√°metro, Œ∏, entonces el estimador m√°ximo-veros√≠mil de Œ∏ se obtiene
resolviendo el problema de m√°ximo siguiente:



Œ∏ÃÇ = arg maÃÅx L .
Œ∏

los estimadores m√°ximo-veros√≠miles de

si L depende de dos par√°metros, Œ∏1 y Œ∏2 , entonces

Œ∏1 y Œ∏2 se obtienen resolviendo el problema de m√°ximo siguiente:





Œ∏ÃÇ1 , Œ∏ÃÇ2 = arg maÃÅx L .
Œ∏1 ,Œ∏2

Nota.

Dado que el m√°ximo de una funci√≥n coincide con el m√°ximo de su logaritmo, suele ser muy √∫til

maximizar el logaritmo de la funci√≥n de verosimilitud en vez de la funci√≥n de verosimilitud.

Ejemplo. Vamos a calcular el estimador m√°ximo veros√≠mil del par√°metro p de una distribuci√≥n B (n, p)
basado en una muestra x1 , ..., xN .
En primer lugar, la funci√≥n de verosimilitud es
N  
Y
n

n‚àíx

i
pxi (1 ‚àí p)
x
i
i=1
!
N  
P
Y
PN
n
nN ‚àí N
i=1 xi
=
.
p i=1 xi (1 ‚àí p)
x
i
i=1

Lx1 ,...,xN (p) =

Su logaritmo resulta

ln Lx1 ,...,xN (p) = ln

N  
Y
n
i=1

!
+

xi

N
X

!
xi

√ó ln p +

nN ‚àí

i=1

N
X

!
xi

ln (1 ‚àí p) .

i=1

Para maximizar esta funci√≥n derivamos respecto a p e igualamos a cero:

PN

i=1

p
de donde

xi

‚àí

PN
nN ‚àí i=1 xi
= 0,
1‚àíp

PN
xÃÑ
p
xÃÑ
i=1 xi
=
=
= n
PN
1‚àíp
n ‚àí xÃÑ
1‚àí
nN ‚àí i=1 xi

Luego el estimador es

pÃÇ =

xÃÑ
n

.

xÃÑ
.
n

Obs√©rvese que coincide con el estimador que obtuvimos por el m√©todo de los momentos.

Ejemplo. Vamos a calcular el estimador m√°ximo veros√≠mil del par√°metro Œª de una distribuci√≥n exp (Œª)
basado en una muestra x1 , ..., xN .

140

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Funci√≥n de verosimilitud:

Lx1 ,...,xN (Œª) =

N
Y

Œªe‚àíŒªxi = ŒªN e‚àíŒª

PN

i=1

xi

.

i=1

Logaritmo de la funci√≥n de verosimilitud:

ln Lx1 ,...,xN (Œª) = N ln Œª ‚àí Œª

N
X

xi .

i=1

Para maximizar esta funci√≥n, derivamos respecto a Œª e igualamos a cero:
N

N X
‚àí
xi = 0,
Œª
i=1
de donde

N
ŒªÃÇ = PN

i=1

xi

=

1
.
xÃÑ

De nuevo el estimador m√°ximo veros√≠mil coincide con el proporcionado por el m√©todo de los momentos.

Ejemplo. En el caso de la distribuci√≥n normal, tenemos dos par√°metros. Veamos c√≥mo proceder en esta
situaci√≥n. Vamos a preocuparnos por los estimadores de la media y de la varianza:
La funci√≥n de verosimilitud:
N
(xi ‚àí¬µ)2
 Y
1
‚àö
Lx1 ,...,xN ¬µ, œÉ 2 =
e‚àí 2œÉ2 =
2œÄœÉ 2
i=1


‚àö

N

1
2œÄœÉ 2

Su logaritmo:



N
N
ln Lx1 ,...,xN ¬µ, œÉ 2 = ‚àí ln (2œÄ) ‚àí
ln œÉ 2 ‚àí
2
2

PN

e‚àí

i=1

Pn
2
i=1 (xi ‚àí¬µ)
2œÉ 2

.

2

(xi ‚àí ¬µ)
.
2œÉ 2

Debemos maximizar esta funci√≥n como funci√≥n de ¬µ y œÉ 2 . Para ello, derivamos respecto de ambas
variables e igualamos a cero:

PN

(xi ‚àí ¬µ)
d
2
ln Lx1 ,...,xN ¬µ, œÉ = i=1 2
=0
d¬µ
œÉ
PN
2

d
N
1 i=1 (xi ‚àí ¬µ)
2
=0
ln
L
¬µ,
œÉ
=
‚àí
+
x
,...,x
1
N
2
dœÉ 2
2œÉ 2
2
(œÉ 2 )
De la primera ecuaci√≥n se sigue
N
X

(xi ‚àí ¬µ) =

i=1

xi ‚àí N ¬µ = 0,

i=1

de donde

PN
¬µÃÇ =

Prof. Dr. Antonio Jos√© S√°ez Castillo

N
X

i=1

N

xi

= xÃÑ.

141

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Modelo
B (n, p)
P (Œª)
Geo (p)
BN (a, p)
exp (Œª)
Gamma (a, Œª)

Estimadores por el
m√©todo de los momentos

aÃÇ =

pÃÇ = nxÃÑ
ŒªÃÇ = xÃÑ
1
pÃÇ = 1+xÃÑ
2
xÃÑ
, pÃÇ =
‚àíxÃÑ

s2X,N ‚àí1

ŒªÃÇ = xÃÑ1
aÃÇ = s2 , ŒªÃÇ =
xÃÑ2

n‚àí1

N (¬µ, œÉ)

Estimadores por el m√©todo
de m√°xima verosimilitud
pÃÇ = nxÃÑ
ŒªÃÇ = xÃÑ
1
pÃÇ = 1+xÃÑ
S√≥lo por m√©todos num√©ricos

xÃÑ
s2X,N ‚àí1

ŒªÃÇ = xÃÑ1
S√≥lo por m√©todos num√©ricos

xÃÑ
s2n‚àí1

¬µÃÇ = xÃÑ, œÉÃÇ = sn‚àí1

¬µÃÇ = xÃÑ, œÉÃÇ = sn

Cuadro 7.1: Estimadores por el m√©todo de los momentos y de m√°xima verosimilitud de los par√°metros de las
distribuciones m√°s usuales.

De la segunda, sustituyendo en ella ¬µ por xÃÑ,

PN

i=1

(xi ‚àí xÃÑ)

2
(œÉ 2 )

de donde
2

œÉÃÇ =

Nota.

PN

i=1

2

=

N
,
œÉ2

2

(xi ‚àí xÃÑ)
= s2n .
N

De nuevo hay que llamar la atenci√≥n sobre el hecho de que hemos buscado un estimador, de

m√°xima verosimilitud, de œÉ 2 , no de œÉ . Sin embargo, no es muy dif√≠cil demostrar que el estimador de
m√°xima verosimilitud de œÉ en la distribuci√≥n normal es la cuasidesviaci√≥n t√≠pica muestral, sn .

7.2.6. Tabla resumen de los estimadores de los par√°metros de las distribuciones
m√°s comunes
En toda esta secci√≥n, supongamos que tenemos una muestra x1 , ..., xN de una variable aleatoria X . Los
estimadores seg√∫n el m√©todo de los momentos y de m√°xima verosimilitud de los par√°metros seg√∫n las distribuciones que hemos descrito aparecen en el Cuadro 7.1.

7.3. Estimaci√≥n por intervalos de conanza
Sea x1 , ..., xN una muestra de una determinada v.a. X cuya distribuci√≥n depende de un par√°metro desconocido

Œ∏. Un

intervalo de conanza para Œ∏ con un nivel de signicaci√≥n Œ±, I (x1 , ..., xN ) , es un intervalo real

que depende de la muestra, pero que no depende de Œ∏ tal que

P [Œ∏ ‚àà I (x1 , ..., xN )] = 1 ‚àí Œ±.
Al valor 1 ‚àí Œ± tambi√©n se le llama

142

nivel de conanza.
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

|

|

|

|

|

|

|

|
|

|

|

|

|
|
|

|

|

|
|

|
|

|

|

|

|

|

|

|

|
40

|
40

40

|

|

|

|

|

|

|

|

|

|

|
|

|
|

|

|

|

|

|

|
|

|
|

|
|
|

|
|

30

|

30

|

30

|

|
|

|

|
|

|
|
|

|
|

|

Index

|

|

|

Index

|

Index

|
|

|

|
|
|

|

|

|
|

|

|

20

|

20

|

20

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|
|

|

|
10

10

|

10

|
|

|

|

|
|

|
|
|

|

|

|
|

|
|

|

|

|
|

|

|

|

0.0

0.2

0.4

0.6

Confidence Interval

0

|

0

|

0

|

‚àí0.2

|

|
|
|

|

‚àí0.4

Confidence intervals based on z distribution

50

Confidence intervals based on z distribution

50

50

Confidence intervals based on z distribution

‚àí0.6

‚àí0.4

‚àí0.2

0.0

0.2

0.4

0.6

‚àí1.0

Confidence Interval

‚àí0.5

0.0

0.5

1.0

Confidence Interval

Figura 7.2: Distintos intervalos de conanza para una media a un 68 % (izquierda), a un 90 % (centro) y
a un 99 % (derecha). Puede observarse que aumentar el nivel de conanza hace m√°s amplios los intervalos.
Tambi√©n puede observarse que no todos los intervalos contienen a la media poblacional (0), pero que el n¬∫
de √©stos malos intervalos disminuye conforme aumentamos el nivel de conanza.

Obs√©rvese que la losof√≠a de cualquier intervalo de conanza es proporcionar, bas√°ndonos en los datos, una
regi√≥n donde tengamos un determinado nivel de conanza en que el par√°metro se encuentra. Como en el
caso de los estimadores puntuales, el intervalo de conanza es aleatorio, ya que depende de los datos de
una muestra. Adem√°s, se da por hecho que existe la posibilidad de que el

verdadero

par√°metro Œ∏ no quede

encerrado dentro del intervalo de conanza, cosa que ocurrir√≠a con probabilidad Œ±.

Nota. Al respecto de la interpretaci√≥n del nivel de conanza, tenemos que decir que, dado que desde el
comienzo del curso hemos adoptado una interpretaci√≥n frecuentista de la probabilidad, un intervalo de
conanza al 95 %, por ejemplo, garantiza que si tomamos 100 muestras el par√°metro poblacional estar√°
dentro del intervalo en aproximadamente 95 intervalos construidos.
Sin embargo, esta interpretaci√≥n es absurda en la pr√°ctica, porque nosotros no tenemos 100 muestras,
sino s√≥lo una.
Nosotros tenemos los datos de una muestra. Con ellos construimos un intervalo de conanza. Y ahora s√≥lo
caben dos posibilidades: o el par√°metro est√° dentro del intervalo o no lo est√°. El par√°metro es constante,
y el intervalo tambi√©n. ¬ΩNo podemos repetir el experimento! Es por ello que se habla de intervalos
conanza ,

interpretando que tenemos una

Prof. Dr. Antonio Jos√© S√°ez Castillo

conanza

de

del 95 % en que el par√°metro estar√° dentro.

143

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

7.3.1. Intervalos de conanza para la media
Sea X una v.a. con distribuci√≥n normal de media ¬µ desconocida y varianza œÉ 2 conocida. Sea una muestra

x = (x1 , ..., xN ) de X , y xÃÑ la media muestral asociada. Entonces,



œÉ
œÉ
P ¬µ ‚àà xÃÑ ‚àí z1‚àí Œ±2 ‚àö , xÃÑ + z1‚àí Œ±2 ‚àö
= 1 ‚àí Œ±,
N
N

donde z1‚àí Œ±2 a es tal que FZ z1‚àí Œ±2 = 1 ‚àí Œ±2 , siendo Z ‚Üí N (0, 1) .
a El valor de z Œ± debe buscarse en la tabla de la normal o calcularse con ayuda del ordenador.
1‚àí
2

Es decir, la media se encuentra en el intervalo



œÉ
œÉ
xÃÑ ‚àí z1‚àí Œ±2 ‚àö , xÃÑ + z1‚àí Œ±2 ‚àö
N
N
con un (1 ‚àí Œ±) % de conanza.
No obstante, hay que reconocer que en la pr√°ctica es poco probable que se desconozca el valor de la media
y s√≠ se conozca el de la varianza, de manera que la aplicaci√≥n de este teorema es muy limitada. El siguiente
resultado responde precisamente a la necesidad de extender el anterior cuando se desconoce el valor de la
varianza.
Sea X una v.a. con distribuci√≥n normal de media ¬µ y varianza œÉ 2 , ambas desconocidas. Sea una muestra

x = (x1 , ..., xN ) de X , la media muestral xÃÑ y la varianza muestral s2X,N ‚àí1 . Entonces,
Ô£Æ

s

Ô£Æ

P Ô£∞¬µ ‚àà Ô£∞xÃÑ ‚àí t1‚àí Œ±2 ;N ‚àí1

s2X,N ‚àí1
N

s
, xÃÑ + t1‚àí Œ±2 ;N ‚àí1

s2X,N ‚àí1
N

Ô£πÔ£π
Ô£ªÔ£ª = 1 ‚àí Œ±,

donde tŒ±;N a es el valor tal que FTN (tŒ±;N ) = Œ±, siendo TN una v.a. con distribuci√≥n T de Student con N

grados de libertad.

a El valor de t Œ± debe buscarse en la tabla de la t o calcularse con ayuda del ordenador
1‚àí
2

Es decir, conamos en un (1 ‚àí Œ±) % en que el intervalo

s

Ô£Æ
Ô£∞xÃÑ ‚àí t1‚àí Œ± ;N ‚àí1
2

s2X,N ‚àí1
N

s
, xÃÑ + t1‚àí Œ±2 ;N ‚àí1

s2X,N ‚àí1

Ô£π

N

Ô£ª

contiene a la media, que es desconocida.

Ejemplo.

Mediante R hab√≠amos simulado 1000 valores de una distribuci√≥n N (0, 1). La media y la

desviaci√≥n t√≠pica muestrales de esos 1000 valores resultaron ser xÃÑ = ‚àí0.0133 y s999 = 0.9813. Por tanto,
el intervalo de conanza que se establece al 95 % de conanza para la media es



144

0.9813
‚àí0.0133 ‚àì 1.96 ‚àö
1000


= (‚àí0.074, 0.0475)

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Obs√©rvese que, en efecto, la verdadera media, ¬µ = 0, est√° en el intervalo de conanza.

Los dos resultados que acabamos de enunciar se basan en que se conoce la distribuci√≥n exacta de la muestra,
normal, lo que permite deducir que la media muestral sigue tambi√©n, y de forma exacta, una distribuci√≥n
normal de media ¬µ y varianza

œÉ2
N .

Sin embargo, gracias al teorema central del l√≠mite se sabe que sea cual

sea la distribuci√≥n de las variables de la muestra aleatoria simple, la media muestral sigue aproximadamente
una distribuci√≥n normal de media ¬µ y varianza

œÉ2
N ,

ya que se obtiene como suma de v.a. independientes con

la misma distribuci√≥n. Por lo tanto, podemos obtener un intervalo de conanza

aproximado

para cualquier

media de cualquier distribuci√≥n, como se recoge en el siguiente resultado.

Sea X una v.a. con distribuci√≥n cualquiera de media ¬µ, desconocida, y con varianza, œÉ 2 . Sea una muestra

x = (x1 , ..., xN ) de X y la media muestral, xÃÑ. Entonces, si N es sucientemente elevado (N > 30 es suciente),



œÉ
œÉ
' 1 ‚àí Œ±.
P ¬µ ‚àà xÃÑ ‚àí z1‚àíŒ±/2 ‚àö , xÃÑ + z1‚àíŒ±/2 ‚àö
N
N
En esta expresi√≥n, si œÉ es desconocida, puede sustituirse por la desviaci√≥n t√≠pica muestral, sn‚àí1 .

Ejemplo. Para dimensionar el tama√±o del buer de un modem ADSL es necesario estimar el promedio
de paquetes de datos por milisegundo que recibe el modem.
Se considera que el tiempo (en milisegundos) que transcurre entre paquete y paquete sigue una distribuci√≥n exponencial de par√°metro Œª. Obs√©rvese que la media de esta distribuci√≥n es ¬µ =

1
Œª,

tiempo medio

entre paquetes, por lo que Œª es precisamente el promedio de paquetes por milisegundo que recibe el
modem. Por lo tanto, el objetivo es estimar el par√°metro Œª, que es el que se utilizar√° para dimensionar
el modem.
Mediante un snier acoplado al modem para capturar datos del tr√°co, se toman datos de los tiempos
entre paquetes de 1001 paquetes, por lo que se tienen 1000 datos de tiempos entre paquetes. La media
de estos tiempos resulta ser xÃÑ = 2.025, siendo la desviaci√≥n t√≠pica muestral de 1.921.
En primer lugar, vamos a calcular un intervalo de conanza (al 95 %) para la media de la distribuci√≥n,

¬µ:


sn‚àí1
1.921
sn‚àí1
= 2.025 ‚àì 1.96 √ó ‚àö
= (1.906, 2.144).
xÃÑ ‚àí z0.975 ‚àö , xÃÑ + z0.975 ‚àö
n
n
1000
Finalmente, dado que Œª =

1
¬µ,

el intervalo de conanza al 95 % de Œª es

1
1
2.144 , 1.906



= (0.466, 0.525) .

A t√≠tulo informativo, el valor que se considera en el dimensionamiento del modem es un m√∫ltiplo (el
doble, por ejemplo) del extremo superior del intervalo, en este caso 0.525.

Prof. Dr. Antonio Jos√© S√°ez Castillo

145

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

7.3.2. Intervalos de conanza para una proporci√≥n
Sea p la probabilidad desconocida de un determinado evento, que llamaremos √©xito, que puede ocurrir en
un determinado experimento. Supongamos que tenemos una muestra de N realizaciones independientes del
experimento, y sea pÃÇ =

k
N

la proporci√≥n de √©xitos en la muestra. Entonces, si N es sucientemente elevado

(N > 30), se tiene que

"

r

P p‚àà

pÃÇ ‚àí z1‚àíŒ±/2

pÃÇ (1 ‚àí pÃÇ)
, pÃÇ + z1‚àíŒ±/2
N

r

pÃÇ (1 ‚àí pÃÇ)
N

!#
' 1 ‚àí Œ±.

Ejemplo. La Junta de Andaluc√≠a pretende implantar un programa de ayuda a familias con familiares
dependientes. Dado que la mayor parte de los Servicios Sociales son competencia de los municipios, la
Junta proporcionar√° los medios econ√≥micos, pero ser√°n √©stos los encargados de ejecutar el programa.
Los Servicios Sociales de cualquier municipio asumen que, por errores inevitables, no todas las familias
a las que subvencionan reunen los requisitos exigidos, pero la Junta les responsabiliza de que esto no
ocurra en m√°s del 4 % de ellas. Si se supera este porcentaje, penalizar√° al municipio.
En un municipio se muestrean 200 familias y se detecta que 12 de ellas (6 %) no cumplen las condiciones
exigidas. ¬æDebe la Junta sancionar al municipio?
Si nos jamos s√≥lo en el valor de la estimaci√≥n puntual, 6 %, s√≠ deber√≠a hacerlo, pero no ser√≠a justo: 12
errores en una muestra de 200 pueden no ser una evidencia suciente de que el porcentaje superara el
4 %.
Consideremos un un intervalo de conanza para la proporci√≥n de errores (5 % de signicaci√≥n) con los
datos obtenidos:

r
0.06 ‚àì 1.96

0.06(1 ‚àí 0.06)
= (0.027, 0.093).
200

Por tanto, no hay evidencias de que el porcentaje sea superior al 4 % y no debe sancionarse al municipio.

7.3.3. Intervalos de conanza para la varianza
An√°logamente, pueden darse intervalos de conanza para la varianza con la media conocida o desconocida,
pero s√≥lo cuando la v.a. observada sigue una distribuci√≥n gaussiana. Ambos casos se recogen en el siguiente
resultado.
Sea X una v.a. con distribuci√≥n gaussiana de media ¬µ (desconocida) y varianza œÉ 2 . Sea una muestra

x = (x1 , ..., xN ) de X y la media muestral xÃÑ. Entoncesa :
"P
P

N
2
i=1 (Xi ‚àí xÃÑ)
œá21‚àí Œ± ;N ‚àí1
2

< œÉ2 <

PN

2

i=1 (Xi ‚àí xÃÑ)
œá2Œ± ;N ‚àí1

#
= 1 ‚àí Œ±.

2

a El valor de œá2
œá2
y debe buscarse en las tablas de la distribuci√≥n œá2 u obtenerse mediante el ordenador.
Œ±/2;N ‚àí1 1‚àíŒ±/2;N ‚àí1

146

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros


En esta expresi√≥n, œá2Œ±;N corresponde con aquel valor tal que Fœá2 œá2Œ±;N = Œ±, donde œá2 sigue una distribuci√≥n

cuadrado con N grados de libertad.
Nota. Un intervalo de conanza para la desviaci√≥n t√≠pica puede obtenerse trivialmente como la raiz cuadrada

œá

del intervalo de conanza para la varianza.

Ejemplo. En el ejemplo donde consideramos 1000 valores simulados de una N (0, 1) ten√≠amos que xÃÑ =
‚àí0.0133 y s999 = 0.9813. Por tanto, teniendo en cuenta que
N
X

2

(Xi ‚àí xÃÑ) = 999 √ó s2999 ,

i=1

el intervalo de conanza para la varianza al 95 % que proporciona el teorema es



961.9867
961.9867
,
1.0885 √ó 103 913.3010


= (0.8838, 1.0533) .

Obs√©rvese que œÉ = 1 pertenece al intervalo de conanza al 95 %.
Puede que alguno de vosotros est√© pensando cu√°l puede ser el inter√©s de las estimaciones puntuales y, sobre
todo, mediante intervalos de conanza de la varianza. Probablemente todos tenemos muy claro qu√© es una
media, incluso una proporci√≥n, pero quiz√° se nos escape la importancia pr√°ctica del concepto de varianza.
En este sentido, hay que decir que en el √°mbito de la Ingenier√≠a la varianza se utiliza much√≠simo en lo que
se conoce como

control de calidad.

Los japoneses son, en esto, los pioneros y quiz√° los mejores expertos. A

ellos se les atribuye un principio b√°sico del control de calidad en cualquier proceso b√°sico de producci√≥n:

reducci√≥n de la varianza es la clave del √©xito en la producci√≥n.

la

Pensemos en cualquier proceso de fabricaci√≥n gen√©rico. En √©l se tratar√° de obtener un producto sujeto a unas
especicaciones concretas. Sin embargo, el error inherente a cualquier proceso experimental provocar√°:
1. Un aumento o una disminuci√≥n estructurales del producto con respecto a un valor objetivo. Esto podr√≠a
detectarse como un sesgo en la media de lo producido con respecto al valor objetivo.
2. Unas diferencias m√°s o menos importantes en los productos resultantes, que podr√≠an ser evaluadas
mediante la varianza.
De esas dos posibles problem√°ticas, la m√°s compleja, sin duda es la segunda. Probablemente no es un grave
problema

calibrar

la m√°quina que produce para que la media se sit√∫e en el valor objetivo, pero ser√° sin duda

m√°s complejo modicarla para que produzca de forma m√°s homog√©nea, reduciendo as√≠ la varianza.

7.3.4. Otros intervalos de conanza
Se pueden establecer intervalos de conanza para la diferencia entre las medias de dos variables aleatorias,
para la diferencia entre proporciones o para el cociente de varianzas, entre otros par√°metros de inter√©s.
Asimismo, se pueden obtener intervalos de conanza unilaterales para cualquiera de los par√°metros que hemos
mencionado, es decir, intervalos acotados s√≥lo a un lado, frente a los intervalos

bilaterales

que hemos visto

aqu√≠.
Prof. Dr. Antonio Jos√© S√°ez Castillo

147

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

No obstante, no vamos a detallarlos aqu√≠, aunque su interpretaci√≥n es an√°loga a la de los intervalos de conanza
que hemos visto. Cualquier paquete de software estad√≠stico puede facilitar estos intervalos sin dicultad.

7.4. Resoluci√≥n del ejemplo de los niveles de plomo
Recordemos que al principio del curso plante√°bamos un problema que aparece en un art√≠culo publicado en
Journal of Environmental Engineering

en 2002, titulado Leachate from Land Disposed Residential Cons-

truction Waste, en el que se presenta un estudio de la contaminaci√≥n en basureros que contienen desechos de
construcci√≥n y desperdicios de demoliciones. Dec√≠amos all√≠ que De

un sitio de prueba se tomaron 42 muestras

de lixiado, de las cuales 26 contienen niveles detectables de plomo. Una ingeniera desea obtener a partir de
esos datos una estimaci√≥n de la probabilidad de que una muestra de un basurero contenga niveles detectables
de plomo. No obstante, es consciente de que esa estimaci√≥n estar√° basada en esa muestra, que es de s√≥lo 42
datos, luego querr√° tambi√©n obtener una estimaci√≥n del error que est√° cometiendo al hacer la estimaci√≥n.
Finalmente, se plantea si con la estimaci√≥n y el error de √©sta, podr√° obtener un rango donde la verdadera probabilidad se encuentre con un alto nivel de conanza.

Ahora estamos en condiciones de resolver este

problema.
En primer lugar, tenemos que obtener una estimaci√≥n de la proporci√≥n de muestras (o probabilidad) que
contienen niveles detectables de plomo. Hemos visto que un estimador insesgado de m√≠nima varianza, que
adem√°s coincide con el estimador de m√°xima verosimilitud, de la proporci√≥n es la proporci√≥n muestral. En
nuestro caso, por tanto, podemos estimar la proporci√≥n en pÃÇ = 26
42 = 0.6190.. Adem√°s, podemos estimar el
q
0.6190(1‚àí0.6190)
= 0.0749 y, en cualquier caso, decir que este
error est√°ndar de esta estimaci√≥n en s.e.(pÃÇ) =
42
error estandar ser√° inferior a
estandar inferior a un 7.71 %.

‚àö1
2 42

= 0.0771. En resumen, tenemos una estimaci√≥n del 61.90 % con un error

Por √∫ltimo, en funci√≥n de esta estimaci√≥n y de su error estandar, puede armar con un 95 % de conanza
que el intervalo

0.6190 ‚àì 1.96 √ó 0.0749 = (0.4722, 0.7658)
contendr√° a la verdadera proporci√≥n de muestras con niveles detectables de plomo. Esta √∫ltima armaci√≥n
pone de maniesto que dar un intervalo de conanza con un nivel de signicaci√≥n aceptablemente bajo (5 %)
conduce a un intervalo muy amplio, lo que equivale a decir que a√∫n hay bastante incertidumbre con respecto
a la proporci√≥n que estamos estimando. Por ello, deber√≠amos recomendarle a la ingeniera que aumente el
tama√±o de la muestra.

148

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 8
Contrastes de hip√≥tesis param√©tricas

La gran tragedia de la ciencia: la destrucci√≥n de una bella hip√≥tesis por un antiest√©tico conjunto
de datos.
Thomas H. Huxley.
La Estad√≠stica puede probar todo, incluso la verdad.
N. Moynihan

Resumen. En este cap√≠tulo explicamos qu√© se entiende por contraste de hip√≥tesis estad√≠stica y aprendemos
a realizar contrastes de este tipo a partir de datos, referidos a alg√∫n par√°metro poblacional desconocido.

Palabras clave: contraste de hip√≥tesis, error tipo I, error tipo II, estad√≠stico de contraste, p-valor, nivel de
signicaci√≥n, nivel de conanza.

8.1. Introducci√≥n
Como apunt√°bamos en la introducci√≥n del cap√≠tulo anterior, las llamadas

pruebas o contrastes de hip√≥-

tesis se utilizan para inferir decisiones que se reeren a un par√°metro poblacional bas√°ndose en muestras de
la variable. Vamos a comenzar a explicar el funcionamiento de un contraste de hip√≥tesis con un ejemplo.

Ejemplo. Los cient√≠cos recomiendan que para prever el calentamiento global, la concentraci√≥n de gases
de efecto invernadero no debe exceder las 350 partes por mill√≥n. Una organizaci√≥n de protecci√≥n del medio
ambiente quiere determinar si el nivel medio, ¬µ, de gases de efecto invernadero en una regi√≥n cumple con
las pautas requeridas, que establecen un l√≠mite m√°ximo de 350 partes por mill√≥n. Para ello tomar√° una
muestra de mediciones diarias de aire para decidir si se supera el l√≠mite, es decir, si ¬µ > 350 o no. Por
tanto, la organizaci√≥n desea encontrar apoyo para la hip√≥tesis ¬µ > 350, llamada

hip√≥tesis alternativa,

obteniendo pruebas en la muestra que indiquen que la hip√≥tesis contraria, ¬µ = 350 (o ¬µ ‚â§ 350), llamada

hip√≥tesis nula, es falsa.

Dicho de otra forma, la organizaci√≥n va a someter a juicio a la hip√≥tesis nula ¬µ ‚â§ 350. Partir√° de
inocencia,

su

suponiendo que es cierta, es decir, suponiendo que, en principio, no se superan los l√≠mites de
149

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

presencia de gases de efecto invernadero, y s√≥lo la rechazar√° en favor de H1 si hay pruebas evidentes en
los datos de la muestra para ello.
La decisi√≥n de rechazar o no la hip√≥tesis nula en favor de la alternativa deber√° basarse en la informaci√≥n

que da la muestra, a trav√©s de alguna medida asociada a ella, que se denomina estad√≠stico de contraste.
Por ejemplo, si se toman 30 lecturas de aire y la media muestral es mucho mayor que 350, lo l√≥gico ser√°
rechazar la hip√≥tesis nula en favor de ¬µ > 350, pero si la media muestral es s√≥lo ligeramente mayor que
350 o menor que 350, no habr√° pruebas sucientes para rechazar ¬µ ‚â§ 350 en favor de ¬µ > 350.
La cuesti√≥n clave es en qu√© momento se decide rechazar la hip√≥tesis nula en favor de la alternativa. En
nuestro ejemplo, en qu√© momento podemos decir que la media muestral es sucientemente mayor que
350. El conjunto de estos valores del estad√≠stico de contraste, que permiten rechazar ¬µ = 350 en favor de

¬µ > 350 se conoce como

regi√≥n de rechazo.

A la luz de este ejemplo, vamos a tratar de denir de forma general los conceptos que acabamos de introducir.
Un contraste

de hip√≥tesis es una prueba que se basa en los datos de una muestra de una variable aleatoria
mediante la cu√°l podemos rechazar una hip√≥tesis sobre un par√°metro de la poblaci√≥n, llamada hip√≥tesis
nula (H0 ), en favor de una hip√≥tesis contraria, llamada hip√≥tesis alternativa (H1 ).

La prueba se basa en una transformaci√≥n de los datos de la muestra, lo que se denomina

contraste.

estad√≠stico de

Se rechazar√° la hip√≥tesis nula en favor de la alternativa cuando el valor del estad√≠stico de contraste se sit√∫e
en una determinada regi√≥n, llamada

regi√≥n de rechazo.

La hip√≥tesis H0 se suele expresar como una igualdada , del tipo H0 : Œ∏ = Œ∏0 , donde Œ∏ es un par√°metro de una
poblaci√≥n y Œ∏0 es un valor hipot√©tico para ese par√°metro. Por su parte, H1 puede tener tener dos formas:

H1 : Œ∏ > Œ∏0 , en cuyo caso se habla de contraste unilateral a la derecha o de una cola a la derecha o de
un extremo a la derecha, o H1 : Œ∏ < Œ∏0 , en cuyo caso se habla de contraste unilateral a la izquierda
o de una cola a la izquierda o de un extremo a la izquierda.
H1 : Œ∏ 6= Œ∏0 , en cuyo caso se habla de contraste bilateral o de dos colas o de dos extremos.
a De todas formas, tambi√©n es frecuente expresar H0 como negaci√≥n exacta de H1 , en cuyo caso s√≠ puede ser una desigualdad
no estricta. Matem√°ticamente no hay diferencias en estas dos posibilidades.

Uno de los aspectos m√°s importantes y que se suele prestar a mayor confusi√≥n se reere a qu√© hip√≥tesis
considerar como H0 y cu√°l como H1 . Una regla pr√°ctica para hacerlo correctamente puede ser la siguiente:
1. Si estamos intentando probar una hip√≥tesis, √©sta debe considerarse como la hip√≥tesis alternativa.
2. Por el contrario, si deseamos desacreditar una hip√≥tesis, debemos incluir √©sta como hip√≥tesis nula.

Ejemplo. Para una determinada edicaci√≥n se exige que los tubos de agua tengan una resistencia media
a la ruptura, ¬µ, por encima de 30 kg por cent√≠metro.

150

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Como primera situaci√≥n, supongamos que un proveedor quiere facilitar un nuevo tipo de tubo para
ser utilizado en esta edicaci√≥n. Lo que deber√° hacer es poner a trabajar a sus ingenieros, que
deben realizar una prueba para decidir si esos tubos cumplen con las especicaciones requeridas.
En ese caso, deben proponer un contraste que incluya como hip√≥tesis nula H0 : ¬µ ‚â§ 30 frente a la
alternativa H1 : ¬µ > 30. Si al realizar el contraste de hip√≥tesis se rechaza H0 en favor de H1 , el
tubo podr√° ser utilizado, pero si no se puede rechazar H0 en favor de H1 , no se tienen sucientes
garant√≠as sobre la calidad del tubo y no ser√° utilizado.
Como segunda situaci√≥n, un proveedor lleva suministrando su tipo de tubo desde hace a√±os, sin que
se hayan detectado, en principio, problemas con ellos. Sin embargo, un ingeniero que trabaja para
el gobierno controlando la calidad en las edicaciones viene teniendo sospechas de que ese tipo de
tubo no cumple con las exigencias requeridas. En ese caso, si quiere probar su hip√≥tesis, el ingeniero
deber√° considerar un contraste de la hip√≥tesis nula H0 : ¬µ ‚â• 30 frente a H1 : ¬µ < 30. Dicho de
otra forma, s√≥lo podr√° contrastar su hip√≥tesis si encuentra datos emp√≠ricos que permitan rechazar
esa hip√≥tesis nula en favor de su alternativa, que demuestren con un alto nivel de abilidad que el
proveedor que estaba siendo aceptado ahora no cumple con los requisitos.

De hecho, es important√≠simo que desde el principio tengamos claro qu√© tipo de decisiones puede proporcionarnos un contraste de hip√≥tesis. Aunque ya las hemos comentado, vamos a insistir en ellas. Son las dos
siguientes:
1. Si el valor del estad√≠stico de contraste para los datos de la muestra cae en la regi√≥n de rechazo, podremos
armar

con un determinado nivel de conanza que los datos de la muestra permiten rechazar la

hip√≥tesis nula en favor de la alternativa.
2. Si el valor del estad√≠stico de contraste para los datos de la muestra no cae en la regi√≥n de rechazo, no
podremos armar

con el nivel de conanza exigido que los datos de la muestra permiten rechazar

la hip√≥tesis nula en favor de la alternativa.
La clave radica en que entendamos desde el principio que la hip√≥tesis nula carece de conanza. Es asumida
s√≥lo como punto de partida, pero ser√° abandonada cuando los datos emp√≠ricos muestren evidencias claras
en su contra y a favor de la alternativa. La carga de la prueba de hip√≥tesis radica siempre en la hip√≥tesis
alternativa, que es la √∫nica hip√≥tesis en la que podremos garantizar un determinado nivel de conanza.

8.2. Errores en un contraste de hip√≥tesis
El contraste de una hip√≥tesis estad√≠stica implica, por tanto, una toma de decisi√≥n, a favor de H0 o en contra
de H0 y en favor de H1 . Esto implica que podemos equivocarnos al tomar la decisi√≥n de dos formas.

error tipo I o falso negativo a rechazar la hip√≥tesis nula cuando es cierta, y su probabilidad se
nota por Œ±, llamado nivel de signicaci√≥n.

Se llama

Se llama

nivel de conanza a la probabilidad de aceptar la hip√≥tesis nula cuando es cierta, es decir, 1 ‚àí Œ±.

Prof. Dr. Antonio Jos√© S√°ez Castillo

151

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Estado real
Decisi√≥n en
el contraste

H0
H1

H0
Decisi√≥n correcta
Error tipo I

H1
Error tipo II
Decisi√≥n correcta

Cuadro 8.1: Esquematizaci√≥n de los errorres tipo I y tipo II.

Se llama

error tipo II o falso positivo a aceptar la hip√≥tesis nula cuando es falsa, y su probabilidad se

nota por Œ≤.
Se llama

potencia a la probabilidad de rechazar la hip√≥tesis nula cuando es falsa, es decir, 1 ‚àí Œ≤.

¬æCu√°l de los dos errores es m√°s grave? Probablemente eso depende de cada contraste, pero en general, lo que
se pretende es acotar el error tipo I y tratar de minimizar el error tipo II, es decir, tratar de elegir contrastes
lo m√°s potentes posibles garantizando que la probabilidad del error tipo I es inferior a un determinado nivel.

Ejemplo. Un fabricante de minicomputadoras cree que puede vender cierto paquete de software a m√°s
del 20 % de quienes compran sus computadoras. Se seleccionaron al azar 10 posibles compradores de la
computadora y se les pregunt√≥ si estaban interesados en el paquete de software. De estas personas, 4
indicaron que pensaban comprar el paquete. ¬æProporciona esta muestra sucientes pruebas de que m√°s
del 20 % de los compradores de la computadora adquirir√°n el paquete de software?
Si p es la verdadera proporci√≥n de compradores que adquirir√°n el paquete de software, dado que deseamos
demostrar p > 0.2, tenemos que H0 : p = 0.2 y H1 : p > 0.2.
Sea X : n√∫mero de posibles compradores de la muestra, en cuyo caso, X ‚Üí B (10, p). Utilizaremos el
valor de X como estad√≠stico del contraste, rechazando H0 si X es grande.
Supongamos que establecemos como regi√≥n de rechazo x ‚â• 4. En ese caso, dado que en la muestra x = 4,
rechazar√≠amos H0 en favor de H1 , llegando a la conclusi√≥n de que el fabricante tiene raz√≥n.
Pero, ¬æcu√°l es el nivel de conanza de este contraste? Calculemos la probabilidad de error tipo I. Para
ello, en el Cuadro 8.2 aparece la distribuci√≥n de probabilidad del estad√≠stico de contraste que hemos
elegido, suponiendo que H0 es cierta, ya que debemos calcular

Œ± = P [Rechazar H0 |H0

es cierta ]
‚àí2

= 0.08808 + 2.6424 √ó 10

= P [X ‚â• 4|p=0.2 ]

+ 5.505 √ó 10‚àí3 + 7.8643 √ó 10‚àí4

+ 7.3728 √ó 10‚àí5 + 4.096 √ó 10‚àí6 + 1.024 √ó 10‚àí7
= 0.12087,
luego el nivel de conanza del contraste es del (1 ‚àí 0.12087) √ó 100 % = 87.913 %. La conclusi√≥n ser√≠a que

a la luz de los datos podemos armar con un 87.913 % de conanza que p > 0.2.

¬æY si queremos un nivel de conanza mayor, es decir, una probabilidad de error tipo I menor? Debemos
reducir la regi√≥n de rechazo. Si ponemos como regi√≥n de rechazo x ‚â• 5, ya no podremos rechazar H0 en

152

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

x
0
1
2
3
4
5
6
7
8
9
10

P [X = x]
10
0
10
= 0.10737
0 0.2 0.8
10
1
9
0.2
0.8
=
0.26844
1
10
2
8
0.2
0.8
=
0.30199
2
10
3
7
0.2
0.8
=
0.20133
3
10
4
6
0.2
0.8
=
0.08808
4 5 5
10
0.2
0.8
=
2.6424
√ó 10‚àí2
5 
10
6
4
‚àí3
6 0.2 0.8 = 5.505 √ó 10
10
7
3
‚àí4
7 0.2 0.8 = 7.8643 √ó 10
10
8
2
‚àí5
8 0.2 0.8 = 7.3728 √ó 10
10
9
1
‚àí6
9 0.2 0.8 = 4.096 √ó 10
10
10
0
‚àí7
10 0.2 0.8 = 1.024 √ó 10



Regi√≥n de
aceptaci√≥n

Regi√≥n
de
rechazo

Cuadro 8.2: Funci√≥n masa del estad√≠stico de contraste suponiendo cierta H0 , es decir, suponiendo que p = 0.2.

favor de H1 , ya que x = 4. Adem√°s, ahora

Œ± = 2.6424 √ó 10‚àí2 + 5.505 √ó 10‚àí3 + 7.864 3 √ó 10‚àí4
+ 7.3728 √ó 10‚àí5 + 4.096 √ó 10‚àí6 + 1.024 √ó 10‚àí7
= 3.2793 √ó 10‚àí2 ,

luego el nivel de conanza ser√≠a 1 ‚àí 3.2793 √ó 10‚àí2 √ó 100 % = 96.721 %, y la conclusi√≥n ser√≠a que

luz de los datos no podemos armar que p > 0.2 con un 96.721 % de conanza.

a la

El estudio de Œ≤ es algo m√°s complicado y no lo abordaremos.

8.3. p-valor de un contraste de hip√≥tesis
Hist√≥ricamente, la forma m√°s com√∫n de actuar en un contraste de hip√≥tesis pasa por elegir un nivel de
signicaci√≥n (bajo), que determina un l√≠mite para el error tipo I que estamos dispuestos a asumir. Ese nivel
de signicaci√≥n determina toda la regi√≥n de rechazo y, examinando si el valor del estad√≠stico cae en ella,
podemos concluir si rechazamos o no la hip√≥tesis nula en favor de la alternativa con el nivel de conanza
requerido.
Existe, sin embargo, otra forma de actuar que ha tenido un auge enorme desde que las computadoras se han
convertido en una herramienta al alcance de cualquiera. Bajo esta forma de actuar, calcularemos el valor del
estad√≠stico de contraste y valoraremos c√≥mo es de extremo este valor bajo la distribuci√≥n en el muestreo de
la hip√≥tesis nula. Si es m√°s extremo que el nivel de signicaci√≥n deseado, se rechazar√° la hip√≥tesis nula en
favor de la alternativa. Esta medida de cu√°n extremo es el valor del estad√≠stico se llama

p-valor.

8.3.1. Denici√≥n de p-valor
De forma general, supongamos que queremos contrastar una hip√≥tesis estad√≠stica simple del tipo H0 : Œ∏ = Œ∏0 ,
frente a alguna de las alternativas siguientes: H1 : Œ∏ 6= Œ∏0 , H1 : Œ∏ > Œ∏0 o H1 : Œ∏ < Œ∏0 . Supongamos adem√°s
Prof. Dr. Antonio Jos√© S√°ez Castillo

153

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

que el contraste se realiza mediante un estad√≠stico que notaremos S , y que el valor del estad√≠stico para la
muestra es s.
El

p-valor asociado al contraste se dene como el m√≠nimo nivel de signicaci√≥n con el que la hip√≥tesis nula

ser√≠a rechazada en favor de la alternativa.

Ejemplo. En el Ejemplo 8.2 hemos visto c√≥mo podemos rechazar la hip√≥tesis nula con un 87.913 % de
conanza, pero no con un 96.721 %. Dicho de otra forma, podemos rechazar la hip√≥tesis nula con un
nivel de signicaci√≥n del 12.087 %, pero no con un nivel de signicaci√≥n del 3.279 %. Esto implica que el
p-valor estar√° justo entre estos dos √∫ltimos valores.

Dado que normalmente se elige como nivel de signicaci√≥n m√°ximo Œ± = 0.05, se tiene que la regla de decisi√≥n
en un contraste con ese nivel de signicaci√≥n, dado el p-valor, ser√≠a la siguiente:
Si p < 0.05, rechazamos H0 en favor de H1 con m√°s de un 95 % de conanza.
Si p ‚â• 0.05, no podemos rechazar H0 en favor de H1 con al menos un 95 % de conanza.
Sin embargo, esta regla de decisi√≥n, que es la m√°s habitual, es demasiado reduccionista si no se proporciona
el valor exacto del p-valor. La raz√≥n es que no es lo mismo rechazar una hip√≥tesis con

al menos

un 95 % de

conanza si el p-valor es 0.049 que si es 0.001. Hay que proporcionar siempre el p-valor de un contraste, ya
que eso permite a cada lector decidir por s√≠ mismo.
En resumen, el p-valor permite utilizar cualquier otro nivel de signicaci√≥n, ya que si consideramos un nivel
de signicaci√≥n Œ±:
Si p < Œ±, rechazamos H0 en favor de H1 con m√°s de un (1 ‚àí Œ±) √ó % de conanza.
Si p ‚â• Œ±, no podemos rechazar H0 en favor de H1 con al menos un (1 ‚àí Œ±) √ó % de conanza.
Como conclusi√≥n, siempre que hagamos un contraste de hip√≥tesis, debemos facilitar el p-valor asociado.
Como nota nal sobre el concepto de p-valor, es importante se√±alar que, al contrario de lo que err√≥neamente
se piensa en demasiadas ocasiones, el p-valor no es la probabilidad de la hip√≥tesis nula. Mucha gente piensa
esto porque es cierto que cuando el p-valor es peque√±o es cuando se rechaza la hip√≥tesis nula. Sin embargo,
para empezar, no tiene sentido plantearnos la

probabilidad

de la hip√≥tesis nula, ya que √©sta, o es cierta, o es

falsa: desde una perspectiva cl√°sica de la probabilidad, se habla de la probabilidad de un suceso porque a
veces ocurre y a veces no, pero en este caso no podemos pensar as√≠, ya que la hip√≥tesis nula o se da o no se
da. En realidad, el p-valor lo que da es un indicio de la certidumbre que tenemos, de la conanza en que la
hip√≥tesis nula sea verdad, teniendo en cuenta los datos de la muestra. Esta interpretaci√≥n tiene m√°s que ver
con la interpretaci√≥n subjetiva de la probabilidad de la que hablamos al principio de curso.
Hay que decir que, en relaci√≥n a esta interpretaci√≥n subjetiva de la probabilidad, existe una visi√≥n de la
Estad√≠stica, llamada Estad√≠stica Bayesiana, en la que el p-valor s√≠ puede entenderse como la probabilidad
de la hip√≥tesis nula, pero entendiendo que medimos la probabilidad de la hip√≥tesis nula, no porque pueda
ocurrir o no ocurrir en funci√≥n del azar, sino porque tenemos incertidumbre sobre ella.

154

Prof. Dr. Antonio Jos√© S√°ez Castillo

0.4
0.3
0.2

0.2

0.3

0.4

Apuntes de Estad√≠stica para Ingenieros

0.1

Regi√≥n de aceptaci√≥n

0.1

Regi√≥n de aceptaci√≥n

Œ±

Œ±

0.0

1‚àíŒ±

0.0

1‚àíŒ±

‚àí3

‚àí2

‚àí1

0

1

2

3

‚àí3

‚àí2

‚àí1

0

1

2

3

0.2

0.3

0.4

Figura 8.1: Regiones de rechazo en contrastes unilaterales a la izquierda y a la derecha.

0.1

Regi√≥n de aceptaci√≥n

Œ± 2

Œ± 2

0.0

1‚àíŒ±

‚àí3

‚àí2

‚àí1

0

1

2

3

Figura 8.2: Regi√≥n de rechazo en un contraste bilateral.

8.3.2. C√°lculo del p-valor
Para comprender c√≥mo se calcula el p-valor de un contraste es necesario distinguir entre contrastes unilaterales
o de una cola frente a contrastes bilaterales o de dos colas.
Como ya comentamos, los contrastes del tipo H0 : Œ∏ = Œ∏0 , frente a H1 : Œ∏ 6= Œ∏0 son

contrastes bilaterales

o de dos colas, ya que el rechazo de la hip√≥tesis nula en favor de la alternativa puede producirse porque el
estad√≠stico de contraste toma valores muy altos o muy bajos. Por contra, los contrastes del tipo H0 : Œ∏ = Œ∏0 ,
frente a H1 : Œ∏ > Œ∏0 o H1 : Œ∏ < Œ∏0 son

contrastes unilaterales o de una cola,

ya que el rechazo de la

hip√≥tesis nula en favor de la alternativa puede producirse s√≥lo si el estad√≠stico de contraste toma valores muy
altos (cuando H1 : Œ∏ > Œ∏0 , llamado

contraste a la izquierda).

contraste a la derecha) o muy bajos (cuando H1

Prof. Dr. Antonio Jos√© S√°ez Castillo

: Œ∏ < Œ∏0 ,

llamado
155

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Por tanto, teniendo en cuenta la denici√≥n de p-valor, su c√°lculo se realiza de la siguiente forma:
Si el contraste es unilateral a la izquierda (H1 : Œ∏ < Œ∏0 ),

p = P [S ‚â§ s/H0 ] .
Si el contraste es unilateral a la derecha (H1 : Œ∏ > Œ∏0 ),

p = P [S > s/H0 ] .
Si el contraste es bilateral (H1 : Œ∏ 6= Œ∏0 ),

p = 2 √ó mƒ±ÃÅn {P [S ‚â§ s/H0 ] , P [S > s/H0 ]} .
Hay que decir que el uso del p-valor se ha extendido hasta convertirse en el m√©todo m√°s habitual de toma
de las decisiones desde que el uso de los ordenadores y de los software de c√°lculo est√°n a disposici√≥n de la
mayor√≠a de los usuarios. Hoy en d√≠a casi nadie hace Estad√≠stica a

mano,

y pr√°cticamente todos los programas

estad√≠sticos proporcionan el p-valor como dato para la toma de las decisiones.
En lo que resta del tema lo que vamos a hacer es enunciar distintos contrastes de hip√≥tesis para la media, la
varianza o la proporci√≥n de una poblaci√≥n y para comparar las medias, las varianzas y las proporciones en
dos poblaciones distintas. No nos vamos a centrar en los detalles de c√≥mo se deducen sino s√≥lo en c√≥mo se
utilizan en la pr√°ctica.
De todas formas, es importante hacer una aclaraci√≥n: cuando los datos proceden de una distribuci√≥n normal,
es muy sencillo obtener la distribuci√≥n del estad√≠stico del contraste, gracias a los resultados que vimos en
el cap√≠tulo de distribuciones en el muestreo. Sin embargo, si los datos no proceden de variables normales,
esta cuesti√≥n es much√≠simo m√°s dif√≠cil. Afortunadamente, si el tama√±o de la muestra es grande, el Teorema
Central del L√≠mite garantiza que los par√°metros que se basan en sumas basadas en las muestras siguen
aproximadamente una distribuci√≥n normal. Es por ello que en cada tipo de contraste que vamos a describir
a continuaci√≥n se distinguen aquellos que se basan en muestras grandes y los que se basan en muestras
reducidas, que s√≥lo podr√°n ser utilizados si la variable es normal.
En cada caso, vamos a acompa√±ar el contraste con un ejemplo que comentaremos extensamente.

8.4. Contraste para la media de una poblaci√≥n
Vamos a suponer que tenemos una muestra x1 , ..., xn de una variable aleatoria con media poblacional ¬µ.
Notaremos xÃÑ a la media muestral y s2n‚àí1 a la varianza muestral.

8.4.1. Con muestras grandes (n ‚â• 30)
El Cuadro 8.3 incluye un resumen del procedimiento para el contraste. En √©l, zp es el valor de una N (0, 1)
tal que P [Z < zp ] = p.
A modo de ejemplo, podemos pensar en que los arque√≥logos utilizan el hecho conocido de que los h√∫meros
de los animales de la misma especie tienden a tener aproximadamente las mismas razones longitud/anchura

156

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo de prueba
Hip√≥tesis
Estad√≠stico
Rechazo
p-valor
Supuestos

A la izquierda
H0 : ¬µ = ¬µ0
H1 : ¬µ < ¬µ 0

z < zŒ±
P [Z < z]

Bilateral
H0 : ¬µ = ¬µ0
H1 : ¬µ 6= ¬µ0
xÃÑ‚àí¬µ‚àö
0
z = sn‚àí1
/ n
|z| > z1‚àíŒ±/2
2P [Z > |z|]
n ‚â• 30

A la derecha
H0 : ¬µ = ¬µ0
H1 : ¬µ > ¬µ0

z > z1‚àíŒ±
P [Z > z]

Cuadro 8.3: Contraste para la media con muestras grandes
9.23
12.57
8.42
9.59
11.37

10.38
8.71
7.84
8.63
10.06

9.76
9.16
9.16
7.48
8.09

7.58
10.80
9.40
7.75
9.19

9.99
9.86
9.03
8.92
10.79

9.46
7.61
9.00
12.85
9.82

10.18
8.98
9.25
11.01
9.37

9.08
10.81
10.39
8.19
9.66

7.09
9.05
8.50
7.44
9.75

9.25
9.39
9.51
11.66
9.66

Cuadro 8.4: Datos del ejemplo de las especies
para tratar de discernir si los h√∫meros f√≥siles que encuentran en un yacimiento corresponden o no a una nueva
especie.
Supongamos que una especie com√∫n en la zona donde se enclava un yacimiento, la

Bichus localis,

tiene una

raz√≥n media longitud/anchura de 9. Los arque√≥logos encargados del yacimiento han hallado 50 h√∫meros
f√≥siles, cuyos datos aparecen en el Cuadro 8.4. ¬æTienen los arque√≥logos indicios sucientes para concluir que
han descubierto en el yacimiento una especie distinta de la

Bichus localis ?

En primer lugar, observemos que no nos han especicado ning√∫n nivel de signicaci√≥n en el enunciado. En
este caso, lo habitual es considerar Œ± = 0.05. En caso de que la decisi√≥n sea muy relevante, elegir√≠amos un
nivel m√°s bajo.
A continuaci√≥n debemos plantear las hip√≥tesis del contraste. En principio, la zona de la excavaci√≥n indica que
la especie del yacimiento deber√≠a ser la especie

Bichus localis,

salvo que demostremos lo contrario, es decir,

la hip√≥tesis nula es H0 : ¬µ = 9, donde por ¬µ estamos notando la media de la raz√≥n longitud/anchura del
h√∫mero de la especie del yacimiento. Como hip√≥tesis alternativa nos planteamos que se trate de otra especie,
es decir H1 : ¬µ 6= 9. Se trata, por tanto, de un contraste de dos colas.
Para realizarlo, debemos calcular en primer lugar el estad√≠stico de contraste. √âste, a su vez, requiere del
c√°lculo de la media y de la desviaci√≥n t√≠pica muestral de los datos. Estos valores son, respectivamente, 9.414
y 1.239. Por tanto,

z=

9.414 ‚àí 9
‚àö = 2.363.
1.239/ 50

Ahora tenemos que plantearnos si este valor del estad√≠stico nos permite rechazar la hip√≥tesis nula en favor
de la alternativa o no. Podemos hacerlo de dos formas:
1. Obteniendo la regi√≥n de rechazo. Dado que z1‚àí0.05/2 = 1.96, la regi√≥n de rechazo es |z| > 1.96. Vemos
que, en efecto, 2.363 > 1.96, por lo que podemos rechazar la hip√≥tesis nula en favor de la alternativa
con un 95 % de conanza, concluyendo con ese nivel de conanza que se trata de una nueva especie.
Nos queda, sin embargo, la duda de saber qu√© hubiera pasado de tomar un nivel de signicaci√≥n m√°s
exigente; por ejemplo, Œ± = 0.01.
Prof. Dr. Antonio Jos√© S√°ez Castillo

157

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipo de prueba
Hip√≥tesis
Estad√≠stico
Rechazo
p-valor
Supuestos

A la izquierda
H0 : ¬µ = ¬µ0
H1 : ¬µ < ¬µ 0

Bilateral
A la derecha
H0 : ¬µ = ¬µ0
H0 : ¬µ = ¬µ0
H1 : ¬µ 6= ¬µ0
H1 : ¬µ > ¬µ0
xÃÑ‚àí¬µ‚àö
0
t = sn‚àí1 / n
t < tŒ±;n‚àí1
|t| > t1‚àíŒ±/2;n‚àí1
t > t1‚àíŒ±;n‚àí1
P [Tn‚àí1 < t]
2P [Tn‚àí1 > |t|]
P [Tn‚àí1 > t]
Distribuci√≥n de probabilidad aproximadamente normal

Cuadro 8.5: Contraste para la media con muestras peque√±as
2. Mediante el p-valor. Tenemos que

p = 2 √ó P [Z > |2.363|] = 0.018.
Dado que es inferior al 5 %, podemos rechazar la hip√≥tesis nula en favor de la alternativa con un 95 % de
conanza, concluyendo con ese nivel de conanza que la raz√≥n media longitud/anchura de los h√∫meros
del yacimiento es distinta de la del
un 99 % de conanza (1 % de

Bichus localis,

pero no podr√≠amos llegar a hacer esa armaci√≥n con

signicaci√≥n)1 .

8.4.2. Con muestras peque√±as (n < 30)
La principal diferencia es que, al no poder utilizar el Teorema Central del L√≠mite por tratarse de muestras
peque√±as, debemos a√±adir como hip√≥tesis la normalidad de los datos. En ese caso, la distribuci√≥n en el
muestreo del estad√≠stico ya no es normal, sino t-student. El resumen aparece en el Cuadro 8.5. En ella, tp;v
es el valor de una t de Student con v grados de libertad tal que P [Tv < tp;v ] = p.
Vamos a aplicar el test en la siguiente situaci√≥n. El diario Sur publicaba una noticia el 5 de noviembre de 2008
donde se indicaba que

los niveles de concentraci√≥n de benceno, un tipo de hidrocarburo cancer√≠geno que se

encuentra como vapor a temperatura ambiente y es indisoluble en agua, no superan el m√°ximo permitido por
la Directiva Europea de Calidad del Aire, cinco microgramos por metro c√∫bico. √âsta es la principal conclusi√≥n
del estudio elaborado por un equipo de la Escuela Andaluza de Salud P√∫blica

en el Campo de Gibraltar. La

noticia s√≥lo indicaba que el estudio se basaba en una muestra, dando el valor medio muestral en varias zonas
del Campo de Gibraltar, pero no el tama√±o ni la desviaci√≥n t√≠pica muestral.
Para realizar el ejemplo, nosotros vamos a imaginar unos datos correspondientes a una muestra de 20 hogares
donde se midi√≥ la concentraci√≥n de benceno, arrojando una media muestral de 5.1 microgramos por metro
c√∫bico y una desviaci√≥n t√≠pica muestral de 1.7. Estoy seguro de que, en ese caso, el peri√≥dico habr√≠a sacado
grandes titulares sobre la contaminaci√≥n por benceno en los hogares del Campo de Gibraltar pero, ¬æpodemos
armar que, en efecto, se superan los l√≠mites de la Directiva Europea de Calidad del Aire?
En primer lugar, de nuevo no nos indican un nivel de signicaci√≥n con el que realizar la prueba. Escogemos,
en principio, Œ± = 0.05.
Tenemos que tener cuidado, porque el planteamiento de la prueba, tal y como se nos ha planteado, ser√°
contrastar la hip√≥tesis nula H0 : ¬µ = 5 frente a H1 : ¬µ > 5, en cuyo caso, un error tipo I se traduce en
concluir que se viola la normativa cuando en realidad no lo hace, lo cu√°l es grave porque genera alarma
injusticada en la poblaci√≥n, mientras que el error tipo II, el que no controlamos con el Œ±, es concluir que
1 Debe quedar claro que, estad√≠sticamente, lo que hemos demostrado es que la raz√≥n media es distinta de 9. Son los arque√≥logos
los que deciden que eso implica una nueva especie.

158

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

se cumple la normativa cuando en realidad no lo hace, ¬Ωlo cual es grav√≠simo para la poblaci√≥n! Con esto
quiero incidir en una cuesti√≥n importante respecto a lo que se nos pide que demostremos: se nos dice que
nos planteemos si se superan los l√≠mites de la normativa, en cuyo caso H1 debe ser ¬µ > 5, pero en realidad,
deber√≠amos plantearnos la pregunta de si podemos estar seguros de que se est√° por debajo de los l√≠mites
m√°ximos permitidos, es decir, deber√≠amos probar H1 : ¬µ < 5.
Centr√°ndonos exclusivamente en lo que se nos pide en el enunciado, tenemos que H1 : ¬µ > 5 determina que
se trata de una prueba unilateral a la derecha. El estad√≠stico de contraste es

t=

5.1 ‚àí 5
‚àö = 0.263.
1.7/ 20

1. Si queremos concluir con la regi√≥n de rechazo, √©sta est√° formada por los valores t > t0.95;19 = 1.729,
luego, dado que 0.263 < 1.729, no podemos armar con un 95 % de conanza que se est√© incumpliendo
la normativa.
2. El p-valor es a√∫n m√°s informativo. Su valor es p = P [T19 > 0.263] = 0.398, por lo que tendr√≠amos
que llegar hasta casi un 40 % de signicaci√≥n para rechazar la hip√≥tesis nula en favor de la alternativa
armando que se incumple la normativa.
Por lo tanto, tal y como est√° planteado el problema, no podemos armar que se est√© incumpliendo la normativa
(con un 5 % de signicaci√≥n), por m√°s que un valor muestral de la media, 5.1, parezca indicar que s√≠. Lo
que yo recomendar√≠a a los responsables del cumplimiento la normativa es que aumentaran el tama√±o de la
muestra, ya que, por ejemplo, si esos mismos datos correspondieran a 1000 hogares en vez de a 20, s√≠ se
podr√≠a armar con un 95 % de conanza que se incumple la normativa.

8.5. Contraste para la diferencia de medias de poblaciones independientes
Sean dos muestras, x1 , ..., xn1 e y1 , ..., yn2 , de v.a. independientes con medias ¬µ1 y ¬µ2 y varianzas œÉ12 y œÉ22 .
2
2
Sean xÃÑ, yÃÑ , s1n‚àí1 y s2n‚àí1 medias y varianzas muestrales.

8.5.1. Con muestras grandes (n1 , n2 ‚â• 30)
El resumen del procedimiento para el contraste aparece en el Cuadro 8.6.
Vamos a considerar un ejemplo donde aplicar el contraste. Imaginemos que un ingeniero inventa un nuevo
m√©todo de producci√≥n con el que cree que pueden reducirse los tiempos de producci√≥n. Para comprobarlo,
produce 50 unidades con el nuevo proceso y 30 con el antiguo, contabilizando el tiempo (en segundos) que se
tarda en producir cada unidad. En el Cuadro 8.7 aparece un resumen de los resultados.
¬æProporcionan estas muestras pruebas sucientes para concluir que el promedio de tiempo de producci√≥n
disminuye con el nuevo proceso? Pru√©bese con Œ± = 0.05.
Llamemos ¬µ1 al tiempo medio de producci√≥n bajo el nuevo proceso y ¬µ2 al tiempo medio de producci√≥n bajo
el antiguo proceso. Nos piden que contrastemos H0 : ¬µ1 = ¬µ2 frente a H1 : ¬µ1 < ¬µ2 o, lo que es lo mismo,

H1 : ¬µ1 ‚àí ¬µ2 < 0: se trata, por tanto, de un test unilateral a la izquierda.
Prof. Dr. Antonio Jos√© S√°ez Castillo

159

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipo de
prueba
Hip√≥tesis

Unilateral a
la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

Estad√≠stico
de contraste
Regi√≥n de
rechazo
p-valor
Supuestos

Unilateral
a la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 > D0

Bilateral

H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0
z=

(xÃÑ‚àíyÃÑ)‚àíD0
s

(s1n‚àí1 )
n1

z < zŒ±

2

2

+

(s2n‚àí1 )
n2

|z| > z1‚àíŒ±/2

z > z1‚àíŒ±

P [Z < z]
2P [Z > |z|]
P [Z > z]
n1 , n2 ‚â• 30. Muestreo independiente y aleatorio

Cuadro 8.6: Contraste para la diferencia de medias con muestras grandes
Proceso nuevo
n1 = 50
y¬Ø1 = 1255
s1 = 215

Proceso antiguo
n2 = 30
y¬Ø2 = 1330
s2 = 238

Cuadro 8.7: Datos del ejemplo del nuevo proceso de producci√≥n
El estad√≠stico es

1255 ‚àí 1330
= ‚àí1.41.
z=q
2152
2382
+
50
30

Para tomar la decisi√≥n podemos obtener la regi√≥n cr√≠tica o el p-valor:
1. La regi√≥n de rechazo es z < z0.05 = ‚àí1.65. Dado que z = ‚àí1.41 no cae en esta regi√≥n, no podemos
rechazar la hip√≥tesis nula en favor de la alternativa con Œ± = 0.05, es decir, no tenemos un 95 % de
conanza en que el nuevo proceso haya disminuido el tiempo medio de producci√≥n. No obstante, esta
respuesta deja abierta la pregunta, si no un 95 % de conanza, ¬æcu√°nta?.
2. Dado que el p-valor es p = P [Z < ‚àí1.41] = 0.079 > 0.05, no podemos rechazar la hip√≥tesis nula en
favor de la alternativa con el nivel de signicaci√≥n Œ± = 0.05.
Hay que decir que no hemos podido probar lo que se sospechaba, que el nuevo proceso reduc√≠a el tiempo
medio de producci√≥n, pero los datos apuntan en esta direcci√≥n. Desde el punto de vista estad√≠stico, deber√≠amos
recomendar al ingeniero que aumente el tama√±o de las muestras porque es posible que en ese caso s√≠ pueda
probar esa hip√≥tesis.

8.5.2. Con muestras peque√±as (n1 < 30 o n2 < 30) y varianzas iguales
El resumen aparece en el Cuadro 8.8. A prop√≥sito de la hip√≥tesis de la igualdad de las varianzas, √©sta debe
basarse en razones no estad√≠sticas. Lo habitual es que se suponga que son iguales porque el experto que est√°
realizando el contraste tiene razones experimentales para hacerlo, razones ajenas a la estad√≠stica.
Vamos a considerar como ejemplo el de un ingeniero que desea comparar dos equipos de trabajo para analizar
si se comportan de forma homog√©nea. Para ello realiza una prueba de destreza entre los trabajadores de
ambos equipos: 13 del equipo 1 y 15 del equipo 2, cuyas puntuaciones aparecen en el Cuadro 8.9. ¬æHay
indicios sucientes de que existan diferencias entre las puntuaciones medias de los dos equipos? (Œ± = 0.05).

160

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo

A la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

Hip√≥tesis
Estad√≠stico
de contraste

t=

(xÃÑ‚àíyÃÑ)‚àíD0
r 
s2p n1 + n1
1

Regi√≥n de
Rechazo
p-valor

Bilateral
A la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0
H1 : ¬µ1 ‚àí ¬µ2 > D0
2
2
1
(n1 ‚àí1)(sn‚àí1 ) +(n2 ‚àí1)(s2n‚àí1 )
2
 , sp =
n1 +n2 ‚àí2

2

|t| > t1‚àíŒ±/2;n1 +n2 ‚àí2

t < tŒ±;n1 +n2 ‚àí2

t > t1‚àíŒ±;n1 +n2 ‚àí2

P [Tn1 +n2 ‚àí2 < t]
2P [Tn1 +n2 ‚àí2 > |t|]
P [Tn1 +n2 ‚àí2 > t]
Muestreo independiente y aleatorio. Variables normales.
œÉ12 = œÉ22

Supuestos

Cuadro 8.8: Contraste para la igualdad de medias con muestras peque√±as
Equipo 1
Equipo 2

59
71

73
63

74
40

61
34

92
38

60
48

84
60

54
75

73
47

47
41

102
44

75
86

33
53

68

39

Cuadro 8.9: Datos de las puntuaciones de los dos equipos de trabajo
Nos piden que contrastemos la igualdad de las medias (H0 : ¬µ1 = ¬µ2 ), frente a la alternativa H1 : ¬µ1 6= ¬µ2 ,
por lo que se trata de un contraste bilateral.
En primer lugar, obtenemos los estad√≠sticos muestrales de ambos equipos. Las medias son, respectivamente,
68.2 y 53.8, mientras que las desviaciones t√≠picas muestrales son 18.6 y 15.8. Con estos valores podemos
calcular s2p :

s2p =

12 √ó 18.6 + 14 √ó 15.8
= 294.09.
13 + 15 ‚àí 2

Con este valor ya podemos calcular el estad√≠stico de contraste:

t= q

68.2 ‚àí 53.8
1
294.09( 13
+

= 2.22.
1
15 )

Aunque no hemos dicho nada al respecto, vamos a suponer que las varianzas son iguales. Esto no parece
descabellado si admitimos que las condiciones en que trabajan ambos equipos determinan que no debe haber
diferencias en la variabilidad de sus puntuaciones. Esta hip√≥tesis debe ser admitida y propuesta por el experto
(en este caso, el ingeniero) que maneja los datos.
Para obtener la conclusi√≥n, como siempre, vamos a obtener la regi√≥n de rechazo y valorar el p-valor:
1. La regi√≥n de rechazo es |t| > t0.975;26 = 2.055. Dado que t = 2.22 cae en esa regi√≥n, podemos rechazar
la igualdad de las medias con un 95 % de conanza.
2. Dado que el p-valor, p = 2P [T26 > 2.22] = 0.035 es inferior a 0.05, podemos rechazar la igualdad de las
medias con un 95 % de conanza. De hecho, podr√≠amos llegar a un 96.5 %.

8.5.3. Con muestras peque√±as, varianzas distintas y mismo tama√±o muestral
El resumen del contraste se recoge en el Cuadro 8.10

8.5.4. Con muestras peque√±as, varianzas distintas y distinto tama√±o muestral
El resumen aparece en el Cuadro 8.11, donde v se redondea al entero m√°s cercano.
Prof. Dr. Antonio Jos√© S√°ez Castillo

161

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipo de prueba
Hip√≥tesis

Unilateral a
la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0

Estad√≠stico
de contraste
Regi√≥n de
rechazo
p-valor
Supuestos

Unilateral
a la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 > D0

Bilateral

(xÃÑ‚àíyÃÑ)‚àíD0

t=

r 
1
n

2

2

(s1n‚àí1 ) +(s2n‚àí1 )



|t| > t1‚àíŒ±/2;2(n‚àí1)

t < tŒ±;2(n‚àí1)

t > t1‚àíŒ±;2(n‚àí1)

P [TŒ±;2(n‚àí1) < t]
2P [TŒ±;2(n‚àí1) > |t|]
P [TŒ±;2(n‚àí1) > t]
Las dos muestras se recogen de forma independiente y aleatoria
Ambas variables siguen distribuciones aproximadamente normales
Las muestras tienen el mismo tama√±o, n1 = n2 = n

Cuadro 8.10: Contraste para la igualdad de medias con muestras peque√±as varianzas distintas y mismo
tama√±o muestral
Tipo de prueba
Hip√≥tesis
Estad√≠stico
de contraste

Unilateral a
la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

t=

Supuestos

n1

(xÃÑ‚àíyÃÑ)‚àíD0
s

2

(s1n‚àí1 )
n1

Regi√≥n
de rechazo
p-valor

Unilateral
a la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0
H1 : ¬µ1 ‚àí ¬µ2 > D0
2
2 !2
(s1n‚àí1 ) + (s2n‚àí1 )
Bilateral

( s2 )
+ n‚àí1

2

n2

,v =

Ô£´
Ô£¨
Ô£≠

(

Ô£´

)

n1 ‚àí1

t < tŒ±;v

n2

2 Ô£∂2
s1
n‚àí1
Ô£∑
Ô£∏
n1

|t| > t1‚àíŒ±/2;v

Ô£¨
Ô£≠

+

(s2n‚àí1 )
n2

2 Ô£∂2
Ô£∑
Ô£∏

n2 ‚àí1

t > t1‚àíŒ±;v

P [Tv < t]
2P [Tv > |t|]
P [Tv > t]
Las dos muestras se recogen de forma independiente y aleatoria
Ambas variables siguen distribuciones aproximadamente normales

Cuadro 8.11: Contraste para la igualdad de medias con muestras peque√±as, varianzas distintas y distinto
tama√±o muestral

8.6. Contraste para la diferencia de medias de poblaciones apareadas
Tenemos una misma poblaci√≥n en la que seleccionamos una muestra de n individuos. En cada uno de ellos
observamos dos variables, X e Y . Estas variables no son independientes: las muestras est√°n

apareadas,

(x1 , y1 ) , ..., (xn , yn ). Para comparar ambas variables se considera una nueva variable, D = X ‚àí Y . Notamos
2
d¬Ø a la media muestral de x1 ‚àí y1 , ..., xn ‚àí yn y sdn‚àí1 a su varianza muestral.

8.6.1. Con muestras grandes (n ‚â• 30)
El resumen aparece en el Cuadro 8.12.

8.6.2. Con muestras peque√±as (n < 30)
El resumen aparece en el Cuadro 8.13. Veamos un ejemplo.
Una empresa farmace√∫tica est√° investigando un medicamento que reduce la presencia en sangre de un com-

162

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo
Hip√≥tesis

A la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

Bilateral
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0
¬Ø
0
‚àö
z = sdd‚àíD
/ n

A la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 > D0

z < zŒ±
P [Z < z]

|z| > z1‚àíŒ±/2
2P [Z > |z|]
n ‚â• 30

z > z1‚àíŒ±
P [Z > z]

Estad√≠stico
Rechazo
p-valor
Supuestos

n‚àí1

Cuadro 8.12: Contraste para la igualdad de medias en poblaciones apareadas con muestra grande
Tipo

Estad√≠stico

Bilateral
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 6= D0
¬Ø
0
‚àö
t = sdd‚àíD
/ n

Rechazo
p-valor
Supuestos

t < tŒ±;n‚àí1
|t| > t1‚àíŒ±/2;n‚àí1
t > t1‚àíŒ±;n‚àí1
P [Tn‚àí1 < t]
2P [Tn‚àí1 > |t|]
P [Tn‚àí1 > t]
D = X ‚àí Y , es aproximadamente normal

Hip√≥tesis

A la izquierda
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 < D0

A la derecha
H0 : ¬µ1 ‚àí ¬µ2 = D0
H1 : ¬µ1 ‚àí ¬µ2 > D0

n‚àí1

Cuadro 8.13: Contraste para la igualdad de medias en poblaciones apareadas y muestra peque√±a
ponente no deseado2 . Antes de sacarlo al mercado necesita un estudio de casos-controles que demuestre su
ecacia.
El estudio de casos controles consiste en encontrar un n√∫mero determinado de parejas de personas con
caracter√≠sticas siol√≥gicas parecidas; en este caso, la m√°s importante de estas caracter√≠sticas ser√≠a que las
parejas caso-control tengan al inicio del estudio el mismo o muy parecido nivel de presencia en sangre del
componente no deseado: en cada una de esas parejas, una act√∫a como caso, tomando la medicaci√≥n en estudio,
y la otra como control, tomando un producto inocuo llamado placebo. Ninguna de las dos personas, ni siquiera
el m√©dico o el farmace√∫tico que controla el proceso, sabe qui√©n es el caso y qui√©n el control. S√≥lo quien recopila
y analiza los resultados, sin contacto alguno con el paciente, tiene esos datos. Esta metodolog√≠a se conoce
como

doble ciego

y evita que el conocimiento de que se est√° administrando la medicina provoque un efecto

en s√≠ mismo. Los datos aparecen en el Cuadro 8.14.
Un an√°lisis costo-benecio de la empresa farmace√∫tica muestra que ser√° benecioso sacar al mercado el
producto si la disminuci√≥n media del componente perjudicial es de al menos 2 puntos. Realicemos una nueva
prueba para ayudar a la compa√±√≠a a tomar la decisi√≥n correcta. Los datos son la disminuci√≥n de presencia
en sangre del componente no deseado despu√©s de tomar el medicamento o el placebo.
Empecemos por la notaci√≥n. Vamos a llamar muestra 1 a la del medicamento y muestra 2 a la del placebo.
Con esta notaci√≥n, nos piden que contrastemos H0 : ¬µ1 ‚àí¬µ2 = 2 frente a H1 : ¬µ1 > ¬µ2 +2, o equivalentemente,

H1 : ¬µ1 ‚àí ¬µ2 > 2. En ese caso, el estad√≠stico de contraste es
t=

3.21 ‚àí 2
‚àö = 3.375
1.134/ 10

y el p-valor asociado es p = P [T9 > 3.375] = 0.004. Vemos que la signicaci√≥n determina un p-valor inferior,
por ejemplo, a Œ± = 0.05, por lo que podemos concluir con ese nivel de signicaci√≥n que la mejora es superior,
en media, a 2 puntos y, por tanto, el medicamento es rentable.
2 Podr√≠a

ser colesterol, √°cido √∫rico, ...

Prof. Dr. Antonio Jos√© S√°ez Castillo

163

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Pareja
1
2
3
4
5
6
7
8
9
10

Medicamento
32.10
36.10
32.30
29.50
34.30
31.90
33.40
34.60
35.20
32.70

Placebo
27.10
31.50
30.40
26.90
29.90
28.70
30.20
31.80
33.60
29.90

Diferencia
5.00
4.60
1.90
2.60
4.40
3.20
3.20
2.80
1.60
2.80

Cuadro 8.14: Datos del ejemplo de la compa√±√≠a farmace√∫tica
Tipo de prueba
Hip√≥tesis
Estad√≠stico
de contraste
p-valor
Regi√≥n
de rechazo
Supuestos

Unilateral a
la izquierda
H0 : p = p0
H1 : p < p0

Bilateral

H0 : p = p 0
H1 : p 6= p0
z=

Unilateral
a la derecha
H0 : p = p0
H1 : p > p0

q pÃÇ‚àíp0

p0 (1‚àíp0 )
n

P [Z < z]

2P [Z > |z|]

P [Z > z]

z < zŒ±

|z| > z1‚àíŒ±/2

z > z1‚àíŒ±

np0 , n (1 ‚àí p0 ) ‚â• 10

Cuadro 8.15: Contraste para una proporci√≥n

8.7. Contraste para la proporci√≥n en una poblaci√≥n
En esta ocasi√≥n tenemos una poblaci√≥n donde una proporci√≥n dada presenta una determinada caracter√≠stica,
que denominamos

√©xito,

y cuya probabilidad es p. Deseamos hacer inferencia sobre esta proporci√≥n. Para

ello seleccionamos una muestra aleatoria simple de tama√±o n y contabilizamos la proporci√≥n de √©xitos en la
muestra, pÃÇ. El resumen del contraste aparece en el Cuadro 8.15.
Vamos a considerar un primer ejempo relativo a la relaci√≥n entre el g√©nero y los accidentes de tr√°co. Se
estima que el 60 % de los conductores son varones. Por otra parte, un estudio realizado sobre los datos de 120
accidentes de tr√°co muestra que en ellos el 70 % de los accidentes fueron provocados por un var√≥n conductor.
¬æPodemos, con esos datos, conrmar que los hombres son m√°s peligrosos al volante?
Si notamos por p a la proporci√≥n de varones causantes de accidentes de tr√°co, la pregunta se responder√°
armativamente si logramos contrastar la hip√≥tesis H1 : p > 0.6. El valor del estad√≠stico es

0.7 ‚àí 0.6
= 2.236.
z=q
0.6√ó0.4
120

Por su parte, la regi√≥n de rechazo ser√≠a |z| > 1.96 para un Œ± = 0.05, luego en efecto, podemos concluir que la
proporci√≥n de varones causantes de accidentes es superior a la proporci√≥n de varones conductores en general.
El p-valor, de hecho, es 0.013.
Vamos a analizar con mucho detalle otro ejemplo sobre igualdad de proporciones. De todas formas, lo que
quiero enfatizaros con el ejemplo no est√° relacionado en s√≠ con el hecho de que se reera a una proporci√≥n.
Una marca de nueces arma que, como m√°ximo, el 6 % de las nueces est√°n vac√≠as. Se eligieron 300 nueces

164

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

al azar y se detectaron 21 vac√≠as. Con un nivel de signicaci√≥n del 5 %, ¬æse puede aceptar la armaci√≥n de
la marca?

En primer lugar, pedir un nivel de signicaci√≥n del 5 % es equivalente a pedir un nivel de conanza del
95 % ... ¬æsobre qu√©? Nos preguntan si se puede aceptar la armaci√≥n de la marca

con un nivel de

signicaci√≥n del 5 %, es decir, con un nivel de conanza del 95 %. Eso implica que queremos
probar con amplias garant√≠as que la marca no miente, y la √∫nica forma de hacerlo es poner su hip√≥tesis
(p < 0.06) en la hip√≥tesis alternativa. Por tanto, tendr√≠amos H0 : p ‚â• 0.06 frente a lo que arma la
marca, H1 : p < 0.06.

Ahora bien, j√©monos que la proporci√≥n muestral de nueces vac√≠as es pÃÇ = 21/300 = 0.07. Es decir, nos
piden que veamos si una proporci√≥n muestral de 0.07 da suciente conanza (95 % para ser exactos) de
que p < 0.06... ¬ΩNo da ninguna! Ni siquiera hace falta hacer el contraste con n√∫meros. Jam√°s podremos
rechazar la hip√≥tesis nula en favor de la hip√≥tesis de la marca, es decir, en absoluto podemos armar
lo que dice la marca, p < 0.06, con un 95 % de conanza. De todas formas, por si hay alg√∫n incr√©dulo,
0.07‚àí0.06
el estad√≠stico de contraste ser√≠a z = ‚àö
= 0.729. La regi√≥n de rechazo, dado que es un test a la
0.06√ó0.94
300

izquierda, ser√≠a z < z0.05 = ‚àí1.645. Como vemos, el valor del estad√≠stico de contraste est√° en la cola de
la derecha y la regi√≥n de rechazo en la de la izquierda. Por eso dec√≠a antes que es imposible rechazar la
hip√≥tesis nula en favor de la alternativa, independientemente del nivel de conanza requerido.
Hasta ahora hemos demostrado que la marca no puede armar que la proporci√≥n de nueces vac√≠as es
inferior al 6 % con un 95 % de conanza. De hecho, no lo puede armar con ning√∫n nivel de conanza,
porque los datos tomados proporcionan una estimaci√≥n de 0.07 que va justo en contra de su hip√≥tesis.
Pero vamos a suponer que nos ponemos gallitos y decimos:  es

m√°s, podr√≠a demostrar que hay eviden-

cias emp√≠ricas que proporcionan un 95 % de conanza en que la compa√±√≠a miente, siendo en realidad
la proporci√≥n de nueces vac√≠as superior al 6 % .

Ahora somos nosotros los que armamos otra cosa:

armamos p > 0.06 con un 95 % de conanza, lo que equivale a decir que hemos planteado un nuevo
contraste de hip√≥tesis en el que H0 : p ‚â§ 0.06 frente a H1 : p > 0.06. Las cuentas est√°n casi hechas, ya
que el valor del estad√≠stico de contraste es el mismo, z = 0.729, mientras que la regi√≥n de rechazo es

z > z0.95 = 1.645. Ahora el valor del estad√≠stico, es decir, la informaci√≥n que nos dan los datos (21 de
300 nueces vac√≠as), s√≠ es coherente con la hip√≥tesis alternativa, de ah√≠ que est√© en la misma cola que la
regi√≥n de rechazo... ¬Ωpero no cae en ella!. Por lo tanto, no tenemos sucientes evidencias en los datos
para rechazar la hip√≥tesis nula en favor de la alternativa con un 95 % de conanza, as√≠ que no podemos
demostrar con ese nivel de conanza que la marca miente.
En resumen, aunque parezca parad√≥jico, no tenemos sucientes evidencias en los datos para armar
que la compa√±√≠a dice la verdad, pero tampoco para demostrar que miente. La diferencia entre ambas
hip√≥tesis radica en que no tenemos ninguna conanza en la armaci√≥n de la compa√±√≠a, y s√≠ alguna
conanza en la armaci√≥n contraria. ¬æCu√°nta conanza tenemos en la armaci√≥n contraria p > 0.06?
Ese valor viene dado por el p-valor, P [Z > 0.729] = 0.233, que determina que el nivel de conanza en

p > 0.06 es (1 ‚àí 0.233) √ó 100 % = 72.9 %.
Finalmente, alguien podr√≠a pensar,  ¬æy

entonces qu√© hacemos? .

Desde el punto de vista estad√≠stico

lo √∫nico que podemos recomendar es aumentar el tama√±o de la muestra, es decir, romper m√°s de 300
nueces para tomar la decisi√≥n. Aparentemente, la informaci√≥n recogida con 300 nueces parece indicar
Prof. Dr. Antonio Jos√© S√°ez Castillo

165

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipo de
prueba
Hip√≥tesis

Unilateral a
la izquierda
H0 : p1 ‚àí p2 = D0
H1 : p1 ‚àí p2 < D0

Estad√≠stico
de contraste
Regi√≥n
de rechazo
p-valor
Supuestos

Bilateral

H0 : p1 ‚àí p2 = D0
H1 : p1 ‚àí p2 6= D0

z=

r

pÃÇ1 ‚àípÃÇ2 ‚àíD0

,
1
1
n +n

pÃÇ(1‚àípÃÇ)

1

pÃÇ =

Unilateral
a la derecha
H0 : p1 ‚àí p2 = D0
H1 : p1 ‚àí p2 > D0

n1 pÃÇ1 +n2 pÃÇ2
n1 +n2

2

z < zŒ±

|z| > z1‚àíŒ±/2

z > z1‚àíŒ±

P [Z < z]

2P [Z > |z|]
Al menos 10 √©xitos y 10 fracasos

P [Z > z]

Cuadro 8.16: Contraste para la diferencia de proporciones
que la marca miente. De hecho, si la proporci√≥n muestral de 0.07 proviniera de una muestra de 1600
nueces en vez de 300, s√≠ hubi√©ramos podido demostrar con un 95 % de conanza que la marca miente.

8.8. Contraste para la diferencia de proporciones
En esta ocasi√≥n partimos de dos poblaciones dentro de las cuales hay proporciones p1 y p2 de individuos con
la caracter√≠stica √©xito. Pretendemos comparar estas proporciones mediante la toma de muestras de tama√±o n1
y n2 . Notaremos pÃÇ1 y pÃÇ2 las proporciones de √©xitos en las muestras. Supondremos de nuevo que las muestras
son grandes para poder aplicar el Teorema Central del L√≠mite a la hora de trabajar con el estad√≠stico de
contraste. El resumen del contraste aparece en el Cuadro 8.16.
Vamos a considerar un estudio3 con datos reales, aunque algo anticuados, referente a la relaci√≥n entre los
accidentes de tr√°co y el consumo de alcohol, realizado por la DGT en la Comunidad Aut√≥noma de Navarra
en 1991.
Se realizaron pruebas de alcoholemia en 274 conductores implicados en accidentes de tr√°co con heridos,
de los cuales, 88 dieron positivo. Por su parte, la Guardia Civil de Tr√°co realiz√≥ en la misma zona 1044
controles de alcoholemia al azar, de los cuales 15 dieron positivo.
Lo que la DGT quiere demostrar es que el alcohol es causante de los accidentes de tr√°co. Sin embargo,
desde el punto de vista estad√≠stico s√≥lo podemos contrastar la hip√≥tesis de que la proporci√≥n de positivos en
la prueba de alcoholemia es mayor en el grupo de conductores implicados en accidentes de tr√°co.
Notemos por p1 y p2 a las verdaderas proporciones en el grupo de implicados en accidentes y en el grupo
de conductores no implicados. Se nos pide contrastar H0 : p1 = p2 frente a H1 : p1 > p2 . El estad√≠stico de
contraste es

z=q

88
274
88+15
274+1044 (1

‚àí

‚àí

15
1044

88+15
1
274+1044 )( 274

= 904.29.
+

1
1044 )

Est√° claro que el valor del estad√≠stico es bestial, sin necesidad de valorar la regi√≥n de rechazo, que ser√≠a

z > z0.95 = 1.645, luego podemos rechazar la hip√≥tesis nula en favor de la alternativa con, al menos, el 95 %
de conanza. El p-valor, p = P [Z > 904.29] = 0 indica que la conanza es, de hecho, bastante mayor.
No puedo resistirme a concluir el ejemplo sin recordar que lo que la DGT realmente querr√° dar a entender
es que el alcohol es el causante de los accidentes de tr√°co, pero que eso no puede ser demostrado con el
contraste.
3 http://www.dgt.es/educacionvial/imagenes/educacionvial/recursos/dgt/EduVial/50/40/index.htm

166

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo de prueba
Hip√≥tesis
Estad√≠stico
de contraste
Rechazo
p-valor
Supuestos

Unilateral a
la izquierda
H0 : œÉ 2 = œÉ02
H1 : œÉ 2 < œÉ02

Bilateral

H0 : œÉ 2 = œÉ02
H1 : œÉ 2 6= œÉ02
œá2 =

Unilateral
a la derecha
H0 : œÉ 2 = œÉ02
H1 : œÉ 2 > œÉ02

(n‚àí1)s2n‚àí1
œÉ02

œá2 < œá2Œ±/2;n‚àí1 o
œá2 > œá21‚àíŒ±;n‚àí1
œá2 > œá21‚àíŒ±/2;n‚àí1
P [œá2n‚àí1 < œá2 ]
2min(P [œá2n‚àí1 < œá2 ], P [œá2n‚àí1 > œá2 ])
P [œá2n‚àí1 > œá2 ]
Distribuci√≥n de probabilidad aproximadamente normal
œá2 < œá2Œ±;n‚àí1

Cuadro 8.17: Contraste para la varianza

8.9. Contraste para la varianza de una poblaci√≥n
De nuevo consideremos que tenemos una variable aleatoria X con varianza œÉ 2 y que tomamos una muestra de
tama√±o n, cuya varianza muestral notamos por s2n‚àí1 . Vamos a tratar de hacer inferencia sobre œÉ 2 . El problema
es que ahora no podemos aplicar el Teorema Central del L√≠mite, por lo que s√≥lo utilizar los contrastes cuando


la variable X es normal. œá2p;v es el valor de una œá2 de v grados de libertad tal que P œá2 < œá2p;v = p.
Las empresa Sidel arma que su m√°quina de llenado HEMA posee una desviaci√≥n t√≠pica en el llenado de
contenedores de 500ml de producto homog√©neo inferior a 0.8 gr.4 Vamos a suponer que el supervisor de control
de calidad quiere realizar una comprobaci√≥n al respecto. Recopila para ello una muestra del llenado de 50
contenedores, obteniendo una varianza muestral de 0.6 ¬æEsta informaci√≥n proporciona pruebas sucientes de
que la desviaci√≥n t√≠pica de su proceso de llenado es realmente inferior a 0.8gr.?
Planteamos, en primer lugar, las hip√≥tesis del contraste. Se nos pide que contrastemos H0 : œÉ = 0.8 o,
equivalentemente, H0 : œÉ 2 = 0.64 frente a la alternativa H1 : œÉ 2 < 0.64. Se trata, por tanto, de un test
unilateral a la izquierda. El estad√≠stico de contraste es

œá2 =

49 √ó 0.6
= 45.938.
0.64

Ahora concluimos a trav√©s de la regi√≥n de rechazo (elegimos Œ± = 0.05) y del p-valor:
1. Dado que œá20.05;9 = 33.930, y œá2 = 45.938 > œá20.05;9 = 33.930, no podemos concluir con al menos un
95 % de conanza que, en efecto, la desviaci√≥n t√≠pica de la cantidad de llenado es inferior a 0.8gr.
2. Dado que el p-valor es p = P [œá249 < 45.938] = 0.4, bastante alto, tenemos muy serias dudas acerca de
que, en efecto, la desviaci√≥n t√≠pica sea realmente inferior a 0.8gr.

Ojo: antes de que la empresa Sidel se enfade con nosotros, no olvidemos que los datos son imaginarios: s√≥lo
son reales las especicaciones t√©cnicas de œÉ < 0.8gr.

8.10. Contraste para el cociente de varianzas
Tenemos dos muestras, x1 , ..., xn1 y y1 , ..., yn2 , de dos variables aleatorias independientes con varianzas œÉ12 y

œÉ22 . Notaremos (s1n‚àí1 )2 y (s2n‚àí1 )2 a las varianzas muestrales. De nuevo s√≥lo podremos considerar el contraste
4 http://www.sidel.com/es/products/equipment/the-art-of-lling/hema-gw
Prof. Dr. Antonio Jos√© S√°ez Castillo

167

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Tipo
Hip√≥tesis

Unilateral a
la izquierda

H0 :
H1 :

œÉ12
œÉ22
œÉ12
œÉ22

=1

H0 :

<1

H1 :

Estad√≠stico
Rechazo
p-valor
Supuestos

Unilateral
a la derecha

Bilateral

f=

œÉ12
=1
œÉ22
œÉ12
6= 1
œÉ22
2
(s1n‚àí1 )

H0 :
H1 :

œÉ12
œÉ22
œÉ12
œÉ22

=1
>1

2

(s2n‚àí1 )

f < fŒ±/2;n1 ‚àí1,n2 ‚àí1 o
f > f1‚àíŒ±;n1 ‚àí1,n2 ‚àí1
f > f1‚àíŒ±/2;n1 ‚àí1,n2 ‚àí1
P [Fn1 ‚àí1,n2 ‚àí1 < f ] 2min(P [Fn1 ‚àí1,n2 ‚àí1 < f ], P [Fn1 ‚àí1,n2 ‚àí1 > f ]) P [Fn1 ‚àí1,n2 ‚àí1 > f ]
Las dos muestras se recogen de forma independiente y aleatoria
Ambas variables siguen distribuciones aproximadamente normales
f < fŒ±;n1 ‚àí1,n2 ‚àí1

Cuadro 8.18: Contraste para el cociente de varianzas
si ambas variables son normales. El resumen del contraste aparece en el Cuadro 8.18. En √©l, fp;v1 ,v2 es el
valor de una F de v1 y v2 grados de libertad5 tal que P [F < fp;v1 ,v2 ] = p.
Para practicar sobre el contraste, consideremos que se han realizado 20 mediciones de la dureza en la escala
Vickers de acero con alto contenido en cromo y otras 20 mediciones independientes de la dureza de una
soldadura producida sobre ese metal. Las desviaciones est√°ndar de las muestras de dureza del metal y de
dureza de la soldadura sobre √©ste fue de 12.06¬µHV y 11.41¬µHV , respectivamente. Podemos suponer que
las durezas corresponden a variables normales e independientes. ¬æPodemos concluir que la dureza del metal
b√°sico es m√°s variable que la dureza medida en la soldadura?
Vamos a llamar a la dureza sobre el acero, X , y a la dureza sobre la soldadura, Y . Se nos pide que contrastemos
2
2
H0 : œÉ X
= œÉY2 frente a la alternativa H1 : œÉX
> œÉY2 o, equivalentemente, H1 :
una prueba unilateral a la derecha. El estad√≠stico de contraste es

f=

2
œÉX
2
œÉY

> 1. Se trata, por tanto, de

12.062
= 1.1172.
11.412

Vamos a tomar un nivel de signicaci√≥n de Œ± = 0.05. La regi√≥n cr√≠tica viene delimitada por el valor f0.95;19,19 =

2.168. Dado que f = 1.1172 < f0.95;19,19 = 2.168, no podemos concluir al nivel de signicaci√≥n Œ± = 0.05 que
la dureza del metal b√°sico sea m√°s variable que la dureza medida en la soldadura.
El p-valor, por su parte, es p = P [F19,19 > 1.1172] = 0.4058.

8.11. Contraste para las medias de m√°s de dos poblaciones independientes. ANOVA
En algunas de las secciones anteriores hemos conseguido contrastes de hip√≥tesis para valorar si existen diferencias signicativas entre dos grupos independientes. Lo que nos planteamos aqu√≠ es extender estos contrastes
para poder comparar no s√≥lo dos sino tres o m√°s grupos. Se da por hecho, por tanto, que existe un

factor

que separa los valores de la variable en varios grupos (dos o m√°s).
Concretamente, supongamos m muestras independientes unas de otras, cada una de ellas con un tama√±o

ni 6 . Supongamos tambi√©n que cada una de las muestras provienen de poblaciones con distribuci√≥n normal
5 De
6 No

168

cara al uso de las tablas hay una propiedad bastante √∫til: fp;v1 ,v2 = 1/f1‚àíp;v2 ,v1
es necesario, aunque s√≠ deseable, que todas las muestras tengan el mismo tama√±o.
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

de medias ¬µi y varianzas todas iguales, œÉ 2 .
Lo que planteamos es contrastar

H0 : ¬µ1 = ... = ¬µm
frente a

H1 : no todas las medias son iguales.
Obs√©rvese que la alternativa no dice

que todas las medias sean distintas

sino tan s√≥lo que al menos dos de

ellas sean diferentes.
Denotemos por xi1 , ..., xini a la muestra i‚àí√©sima, y xÃÑi y s2i,ni ‚àí1 a su media y su varianza muestral, con

i = 1, ..., m.
Este contraste se denomina ANOVA como acr√≥nimo de

Analysis of Variance,

ya que, como vamos a ver, se

basa en analizar a qu√© se debe la variabilidad total que presentan los datos, si al azar o a las diferencias entre
las poblaciones de las que proceden las muestras.
Supongamos que

juntamos

todas las muestras, obteniendo una √∫nica muestra global de tama√±o

N=

m
X

ni ,

i=1

y calculamos su media,

Pm Pni
i=1

xÃÑ =
Ahora, vamos a preguntarnos por las

j=1

xij

N

.

fuentes de variaci√≥n de los datos :

1. En primer lugar, los datos var√≠an globalmente respecto a la media total. Una medida de esta variaci√≥n
es la

suma de los cuadrados totales,

SCT =

ni
m X
X

xij ‚àí xÃÑ

2

.

i=1 j=1

2. Por otro lado, puede haber diferencias entre las medias de cada grupo y la media total. Podemos medir
estas diferencias con la

suma de los cuadrados entre-grupos:
SCE =

m
X

2

ni (xÃÑi ‚àí xÃÑ) .

i=1

Si la hip√≥tesis nula fuera cierta, s√≥lo habr√≠a peque√±as diferencias
muestra, en cuyo caso, la

SCE

muestrales

entre las medias de cada

ser√≠a peque√±a. Si fuera falsa, habr√≠a muchas diferencias entre las medias

y con respecto a la media total, en cuyo caso

SCE

ser√≠a grande.

3. Por √∫ltimo, debido a la variabilidad inherente a toda muestra, los datos de cada muestra van a variar respecto a su media particular. Como medida de esta variaci√≥n consideramos la

cuadrados dentro de los grupos o intra-grupos:
SCD =

ni
m X
X
i=1 j=1

Prof. Dr. Antonio Jos√© S√°ez Castillo

xij ‚àí xÃÑi

2

=

m
X

suma de los

(ni ‚àí 1) s2i,ni ‚àí1 .

i=1

169

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

La clave en estas consideraciones lo constituye la siguiente igualdad, conocida como

de la varianza:

teorema de partici√≥n

SCT = SCE + SCD.
Teniendo en cuenta este resultado, el ANOVA consiste en ver si
de

SCD.

SCE

es signicativamente grande respecto

Para ello basta considerar que, suponiendo que la hip√≥tesis nula es cierta:

SCT
œÉ2

sigue una œá2 con N ‚àí 1 grados de libertad.

SCE
œÉ2

sigue una œá2 con m ‚àí 1 grados de libertad.

SCD
œÉ2

sigue una œá2 con N ‚àí m grados de libertad.

As√≠, el estad√≠stico de contraste del test es

F =

SCE
m‚àí1
SCD
N ‚àím

,

que, suponiendo que la hip√≥tesis nula es cierta, sigue una F de Snedecor con m ‚àí 1 y N ‚àí m grados de
libertad.
Por lo tanto, el test podemos resumirlo de la siguiente forma:
1. Calculamos

Pm Pni
xÃÑ =

y con ella

SCE =

m
X

i=1

j=1

N
2

ni (xÃÑi ‚àí xÃÑ) =

i=1

2. Calculamos

SCD =

ni
m X
X

xij

m
X

ni xÃÑ2i ‚àí N xÃÑ2 .

i=1

xij ‚àí xÃÑi

2

=

i=1 j=1

m
X

(ni ‚àí 1) s2i,ni ‚àí1 .

i=1

3. Calculamos el estad√≠stico del test:

F =

SCE
m‚àí1
SCD
N ‚àím

.

4. Tomamos la decisi√≥n:
a)

Si F ‚â§ Fm‚àí1,N ‚àím;1‚àíŒ± , no rechazamos la hip√≥tesis nula en favor de la alternativa con un nivel de
signicaci√≥n Œ±.

b)

Si F > Fm‚àí1,N ‚àím;1‚àíŒ± , rechazamos la hip√≥tesis nula en favor de la alternativa con un nivel de
signicaci√≥n Œ±.

Ejemplo. En un experimento se prepararon ujos de soldadura con 4 composiciones qu√≠micas diferentes.
Se hicieron 5 soldaduras con cada composici√≥n sobre la misma base de acero, midiendo la dureza en la
escala de Brinell. El Cuadro 8.19 siguiente resume los resultados.
Vamos a contrastar si existen diferencias signicativas entre las durezas, suponiendo que estas siguen
distribuciones normales todas ellas con la misma varianza.

170

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Composici√≥n
A
B
C
D

Media muestral
253.8
263.2
271.0
262.0

Desviaci√≥n t√≠pica muestral
9.7570
5.4037
8.7178
7.4498

Cuadro 8.19: Datos del ejemplo de ANOVA

En primer lugar, observemos que los tama√±os muestrales son iguales: n1 = ... = n4 = 5.
Por otra parte, tenemos:

xÃÑ =

5 √ó 253.8 + 5 √ó 263.2 + 5 √ó 271.0 + 5 √ó 262.0
= 262.5
20

2

2

SCE = 5 √ó (253.8 ‚àí 262.5) + ... + 5 √ó (262.0 ‚àí 262.5) = 743.4

SCD = (5 ‚àí 1) 9.75702 + ... + (5 ‚àí 1) 7.44982 = 1023.6.
Por tanto,

F =

743.4
4‚àí1
1023.6
20‚àí4

= 3.8734.

Por su parte, el valor de F3,16;0.95 es 3.2389, de manera que podemos armar que existen diferencias
signicativas entre las durezas de los 4 compuestos, con un 95 % de conanza.

8.12. El problemas de las pruebas m√∫ltiples. M√©todo de Bonferroni
¬æQu√© ocurre si en un estudio tenemos que realizar m√°s de una prueba de hip√≥tesis? Cada prueba lleva consigo
un determinado nivel de conanza y, por tanto, una probabilidad de equivocarnos rechazando una hip√≥tesis
nula que es cierta (error tipo I). Cuantas m√°s pruebas hagamos, m√°s probabilidades tenemos de cometer un
error en la decisi√≥n rechazando una hip√≥tesis nula cierta o, dicho de otra forma, menor conanza tendremos.
El m√©todo de Bonferroni es uno de los m√©todos m√°s simples para tratar de corregir este problema asociado
a las pruebas m√∫ltiples. Se trata de corregir los p-valores de todas las pruebas que se est√©n realizando
simult√°neamente, multiplic√°ndolos por el n¬∫ total de pruebas, antes de tomar la decisi√≥n.

Ejemplo. En Biolog√≠a Molecular se estudia la relaci√≥n que puede tener el nivel de expresi√≥n de un gen
con la posibilidad de padecer un tipo de c√°ncer. Un investigador consigue analizar el nivel de expresi√≥n de
10 genes en una muestra de pacientes y realiza 10 contrastes de hip√≥tesis donde la hip√≥tesis alternativa de
cada uno de ellos dice que un gen est√° relacionado con la posibilidad de padecer ese c√°ncer. Los p-valores
obtenidos son los siguientes:

(0.1, 0.01, 0.21, 0.06, 0.32, 0.24, 0.45, 0.7, 0.08, 0.0003)

Prof. Dr. Antonio Jos√© S√°ez Castillo

171

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

En principio, tendr√≠amos evidencias de que el 2¬∫ y el √∫ltimo gen est√°n signicativamente relacionados con
ese tipo de c√°ncer. Sin embargo, debemos corregir el efecto de la realizaci√≥n de las 10 pruebas simult√°neas.
Aplicando el m√©todo de Bonferroni, debemos multiplicar por 10 los p-valores. En ese caso, el segundo
gen ya no puede ser considerado estad√≠sticamente signicativo para el riesgo de padecer el c√°ncer (0.01 √ó

10 > 0.05); por el contrario, dado que 0.0003 √ó 10 < 0.05, el √∫ltimo gen sigue siendo considerado
signicativamente relacionado con el c√°ncer.

8.13. Resoluci√≥n del ejemplo del del di√°metro de los cojinetes
Recordemos el planteamiento:

Un ingeniero industrial es responsable de la producci√≥n de cojinetes de bolas y

tiene dos m√°quinas distintas para ello. Le interesa que los cojinetes producidos tengan di√°metros similares,
independientemente de la m√°quina que los produce, pero tiene sospechas de que est√° produciendo alg√∫n problema de falta de calibraci√≥n entre ellas. Para analizar esta cuesti√≥n, extrae una muestra de 120 cojinetes que
se fabricaron en la m√°quina A, y encuentra que la media del di√°metro es de 5.068 mm y que su desviaci√≥n
est√°ndar es de 0.011 mm. Realiza el mismo experimento con la m√°quina B sobre 65 cojinetes y encuentra que
la media y la desviaci√≥n est√°ndar son, respectivamente, 5.072 mm y 0.007 mm. ¬æPuede el ingeniero concluir
que los cojinetes producidos por las m√°quinas tienen di√°metros medios signicativamente diferentes?

En este caso, afortunadamente tenemos un tama√±o muestral que va a permitir obviar la hip√≥tesis de normalidad. Vemos que se plantea un supuesto que puede ser analizado a trav√©s de la media, en concreto, comparando
la media de ambas m√°quinas. Si llamamos X al di√°metro de la m√°quina A e Y al di√°metro de la m√°quina
B, tenemos que contrastar H0 : ¬µX = ¬µY frente a H1 : ¬µX 6= ¬µY .
El estad√≠stico de contraste es

5.068 ‚àí 5.072
= ‚àí3.013.
z=q
0.0072
0.0112
+
120
65

El p-valor asociado es 2 √ó P [Z < ‚àí3.361] = 0.002, luego tenemos evidencias de que, en efecto, el di√°metro
medio de ambas m√°quinas es distinto.

172

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 9
Contrastes de hip√≥tesis no param√©tricas

Todos aprendemos de la experiencia, y la lecci√≥n en esta ocasi√≥n es que nunca se debe perder
de vista la alternativa.
Sherlock Holmes (A. C. Doyle), en Las Aventuras de Black Peter

Resumen. Continuando con los contraste de hip√≥tesis, presentamos en este cap√≠tulo nuevos contrastes que
permitir√°n decidir si un ajuste mediante una distribuci√≥n te√≥rica es v√°lido y valorar si existe relaci√≥n entre
variables cualitativas.

Palabras clave: bondad de ajuste, test œá2

de bondad de ajuste, test de bondad de ajuste de Kolmogorov-

Smirno, test œá2 de independencia.

9.1. Introducci√≥n
Todos los contrastes que hemos descrito en el cap√≠tulo anterior se basan, directa o indirectamente (a trav√©s
del teorema central del l√≠mite) en que los datos se ajustan a la distribuci√≥n normal, haciendo inferencia de
una u otra forma sobre sus par√°metros. En este cap√≠tulo vamos a considerar contrastes que no necesitan
de tal hip√≥tesis, por lo que no se enuncian como contrastes sobre alg√∫n par√°metro desconocido: de ah√≠ que
formen parte de los llamados contrastes

no param√©tricos o contrastes de hip√≥tesis no param√©tricas.

9.2. Contrastes de bondad de ajuste
Gracias a lo estudiado en el apartado correspondiente a la estimaci√≥n puntual de par√°metros ahora somos
capaces de ajustar una distribuci√≥n a unos datos mediante alg√∫n m√©todo de estimaci√≥n (momentos, m√°xima
verosimilitud, ...). Sin embargo, hasta ahora no disponemos de ninguna herramienta capaz de

juzgar

si ese

ajuste es bueno o malo, o c√≥mo de bueno es. De hecho, en la relaci√≥n de problemas correspondiente dejamos
abierta esta cuesti√≥n, ya que s√≥lo pudimos valorar esta

bondad del ajuste

mediante representaciones gr√°cas,

lo que s√≥lo nos dio una visi√≥n parcial del problema, que puede ser muy subjetiva.
Los dos contrastes de hip√≥tesis que vamos a describir ahora van a permitir contrastar como hip√≥tesis nula

H0 : la distribuci√≥n se ajusta adecuadamente a los datos,
173

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Resultado
1
2
3
4
5
6
Total

Observados
105
107
89
103
111
85
600

Esperados
100
100
100
100
100
100
600

Cuadro 9.1: Frecuencias observadas y esperadas en 600 lanzamientos del dado.
frente a la alternativa

H1 : la distribuci√≥n no se ajusta adecuadamente a los datos,
facilitando adem√°s un p-valor que permitir√°, adem√°s, comparar la bondad de distintos ajustes.
Decir, por √∫ltimo, que aunque estos dos contrastes de hip√≥tesis pueden aplicarse a cualquier tipo de variables
est√°n especialmente indicados para variables de tipo discreto o cualitativo en el caso del primero de ellos (test

œá2 de bondad de ajuste) y para variables de tipo continuo en el segundo (test de Kolmogorov-Smirnov).

9.2.1. Test œá2 de bondad de ajuste
Ejemplo. Supongamos que un tahur del Missisipi quiere probar un dado para ver si es adecuado para
jugar honestamente con √©l. En ese caso, si notamos por pi a la probabilidad de que en el lanzamiento del
dado resulte el valor i = 1, 2, ..., 6, el tahur quiere probar la hip√≥tesis

H0 : p1 = ... = p6 =

1
6

frente a la alternativa de H1 que alg√∫n pi sea distinta de 16 .
Para realizar la prueba, lanzar√° el dado 600 veces, anotando el n√∫mero de veces que se da cada resultado.
Estas cantidades se denominan

frecuencias observadas.

Por otra parte, si el dado fuera justo (hip√≥tesis H0 ), en 600 lanzamientos deber√≠an darse aproximadamente
100 de cada resultado posible. √âstas frecuencias se denominan

frecuencias esperadas.

El tahur tomar√° la decisi√≥n con respecto al dado a partir de la comparaci√≥n de las frecuencias observadas
y las esperadas (ver Cuadro 9.1). ¬æQu√© decidir√≠as t√∫ a la luz de esos datos?

A continuaci√≥n, vamos a describir el test œá2 , que permite realizar pruebas de este tipo. Como hemos comentado
en la introducci√≥n, con ella podremos

juzgar

ajustes de los que hemos logrado en el cap√≠tulo de estimaci√≥n

puntual, pero tambi√©n podremos utilizarla en ejemplos como el que acabamos de ver, en el que el experto
est√° interesado en contrastar datos experimentales con respecto a una distribuci√≥n te√≥rica que le resulta de
inter√©s.
En primer lugar y de forma m√°s general, supongamos que tenemos una muestra de tama√±o N de una v.a.
discreta o cualitativa, X , ajustada a un modelo dado por una distribuci√≥n.

174

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Consideremos una partici√≥n del conjunto de valores que puede tomar la variable: S1 , ..., Sr . En principio,
esta partici√≥n podr√≠an ser simplemente todos y cada uno de los valores que toma la variable X , pero, como
veremos, es posible que tengamos que agrupar algunos de ellos.
Seguidamente, consideremos la probabilidad, seg√∫n la distribuci√≥n dada por el ajuste que queremos evaluar,
de cada una de estas partes,

pi = P [X ‚àà Si /H0 ] > 0.
De igual forma, calculemos Oi , el n√∫mero de observaciones de la muestra que caen en cada conjunto Si .
La idea del test es comparar el n√∫mero de observaciones Oi que caen realmente en cada conjunto Si con el
n√∫mero esperado de observaciones que deber√≠an caer en Si si el ajuste es el dado por nuestro modelo, que
ser√≠a N √ó pi . Para ello, una medida que compara estas dos cantidades viene dada por

D=

r
2
X
(Oi ‚àí N √ó pi )
.
N √ó pi
i=1

Si, para una muestra dada, esta v.a. toma un valor d muy alto, indica que los valores observados

no cuadran

con el ajuste que hemos propuesto (con lo cu√°l se rechazar√≠a la hip√≥tesis nula en favor de la alternativa);
si, por el contrario, toma un valor d bajo, indica que nuestro ajuste corresponde bien con los datos de la
muestra, por lo que es

aceptable

la hip√≥tesis nula.

El problema nal es decidir cu√°ndo el valor de la v.a. D, d, es lo sucientemente alto como para que nos
resulte inaceptable el ajuste. Para decidirlo hay que tener en cuenta que cuando N es razonablemente alto y
la hip√≥tesis H 0 es cierta, la distribuci√≥n de probabilidad de D es œá2 con r ‚àí k ‚àí 1 grados de libertad, es decir,
N >>

D/H0 ‚Üí œá2r‚àík‚àí1 ,
donde k es el n√∫mero de par√°metros que han sido estimados en el ajuste. Teniendo en cuenta este resultado,
se calcula bajo esta distribuci√≥n la probabilidad de que se de un valor todav√≠a m√°s alto que d (el p-valor, por
tanto),

p = P [D > d/H0 ] .
Si esta probabilidad es inferior al 5 %, se rechaza la hip√≥tesis nula en favor de la alternativa con un 95 % de
conanza. Dicho de otra forma, se acepta la hip√≥tesis nula s√≥lo si el valor de D entra dentro del 95 % de
resultados m√°s favorables a ella.
Esquem√°ticamente, el proceso es el siguiente:
1. Se enuncia el test:

H0 : los datos siguen la distribuci√≥n dada por nuestro ajuste
H1 : los datos no siguen la distribuci√≥n dada por nuestro ajuste
2. Si en la muestra se dan los valores x1 , ..., xm , se calculan las frecuencias esperadas seg√∫n el ajuste
propuesto de cada valor xi , N √ó P [X = xi ], i = 1, ..., m. Si alguna de estas frecuencias es inferior
a 5, se agrupa con alguna de la m√°s cercana hasta que sumen una frecuencia mayor o igual a 5. Se
construye as√≠ la partici√≥n del conjunto de valores posibles para X , S1 , ...Sr , cuyas frecuencias esperadas
Prof. Dr. Antonio Jos√© S√°ez Castillo

175

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

xi
Frec. obs.

0
42

1
28

2
13

3
5

4
7

5
3

6
2

Cuadro 9.2: Frecuencias observadas en la muestra de tiempos entre llegadas.
son todas mayores o iguales a 5. En realidad, esto es s√≥lo una recomendaci√≥n que puede relajarse: si
alguna frecuencia esperada es s√≥lo ligeramente inferior a 5, no es especialmente grave.
3. Se calculan las frecuencias observadas de cada Si , y lo notamos como Oi .
4. Se calcula el estad√≠stico del test en la muestra

d=

r
2
X
(Oi ‚àí N √ó pi )
.
N √ó pi
i=1

5. Se calcula el p-valor asociado al valor del estad√≠stico,

p = P [D > d/H0 ] ,
seg√∫n una distribuci√≥n œá2 con r ‚àí k ‚àí 1 grados de libertad.
6. Se toma la decisi√≥n (para un nivel de conanza del 95 %):
a)

Si p < 0.05, se rechaza la hip√≥tesis nula en favor de la alternativa, con un 95 % de conanza.

b)

Si p ‚â• 0.05, se concluye que no hay evidencias en contra de armar que los datos se ajustan a la
distribuci√≥n dada.

Ejemplo.

Los datos que se presentan en el Cuadro 9.2 constituyen una muestra aleatoria simple del

tiempo en ms. que transcurre entre la llegada de paquetes transmitidos por un determinado protocolo.
En la tabla aparecen los valores junto al n√∫mero de veces que han sido observados en la muestra.
Se sospecha que una distribuci√≥n geom√©trica puede ajustar bien esos datos. Vamos a realizar ese ajuste
y contrastar si es aceptable mediante el test de la chi-cuadrado.
En primer lugar, para ajustar una distribuci√≥n geom√©trica debemos estimar el par√°metro de la misma.
Vamos a hacerlo de forma sencilla por el m√©todo de los momentos. El valor de la media de la distribuci√≥n
es $EX= de donde p =

1
1+EX .

Por tanto, nuestro estimador ser√°

pÃÇ =

1
.
1 + xÃÑ

Por su parte,

xÃÑ =

0 √ó 42 + 1 √ó 28 + 2 √ó 13 + 3 √ó 5 + 4 √ó 7 + 5 √ó 3 + 6 √ó 2
= 1.24,
100

luego $

176

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

As√≠ pues, deseamos contrastar en qu√© medida el ajuste de una Geo (0.4464) es v√°lido para los datos de
la muestra. Es decir, deseamos contrastar H0 : X ‚Üí Geo (0.4464) frente a la alternativa H1 : X 9

Geo (0.4464) .
Vamos a calcular cu√°les son las probabilidades te√≥ricas seg√∫n esa distribuci√≥n de los valores observados
en la muestra:
0

P [X = 0] = 0.4464 √ó (1 ‚àí 0.4464) = 0.4464

1

P [X = 1] = 0.4464 √ó (1 ‚àí 0.4464) = 0.2471

2

P [X = 2] = 0.4464 √ó (1 ‚àí 0.4464) = 0.1368

3

P [X = 3] = 0.4464 √ó (1 ‚àí 0.4464) = 0.0757

4

P [X = 4] = 0.4464 √ó (1 ‚àí 0.4464) = 0.0419

5

P [X = 5] = 0.4464 √ó (1 ‚àí 0.4464) = 0.0232

6

P [X = 6] = 0.4464 √ó (1 ‚àí 0.4464) = 0.0128

P [X > 6] = 1 ‚àí (0.4464 + 0.2471 + 0.1368 + 0.0757 + 0.0419 + 0.0232 + 0.0128) = 0.0159
Ahora tenemos que construir la partici√≥n de los valores de la variable que, como sabemos, son 0,1,... Hay
que tener en cuenta que debemos procurar que las frecuencias esperadas sean superiores o iguales a 5.
Como hay 100 observaciones, ser√° necesario agrupar los valores 4 en adelante en un solo conjunto. Vamos
a resumir este planteamiento en el Cuadro 9.3 donde, adem√°s, aparecen los residuos al cuadrado entre
las frecuencias observadas y esperadas, necesarios para calcular el estad√≠stico del test.
El valor de √©ste se calcula a partir de los resultados de la tabla de la siguiente manera:

d=

6.9696 0.0841 0.4624 6.6049 6.8644
+
+
+
+
= 1.7973.
44.64
27.71
13.68
7.57
9.38

Finalmente, el p-valor se calcula como P [D > 1.7973] , donde D sigue una œá25‚àí1‚àí1 , es decir, una Gamma
de par√°metros (5 ‚àí 1 ‚àí 1)/2 y 1/2. Por tanto,

ÀÜ

‚àû

p ‚àí valor =
1.7973

1
2

 32 ‚àí1 ‚àí 1 x
e 2

dx = 0.61552.
3
Œì 2

1
2x

Al ser superior (muy superior, de hecho) a 0.05, podemos armar que no hay evidencias en los datos de
la muestra en contra de que √©stos sigan una distribuci√≥n Geo (0.4464).

Prof. Dr. Antonio Jos√© S√°ez Castillo

177

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n
2

xi

Oi

N √ó pi

(Oi ‚àí N √ó pi )

0
1
2
3
‚â•4

42
28
13
5
12

44.64
27.71
13.68
7.57
9.38

(42 ‚àí 44.64) = 6.969 6
2
(28 ‚àí 27.71) = 0 .0841
2
(13 ‚àí 13.68) = 0.462 4
2
(5 ‚àí 7.57) = 6.604 9
2
(12 ‚àí 9.38) = 6.864 4

2

Cuadro 9.3: Frecuencias observadas, frecuencias esperadas y residuos.

9.2.2. Test de Kolmogorov-Smirno
En este caso el test es aplicable sobre todo a variables de tipo continuo. Se basa en la comparaci√≥n de la
funci√≥n de distribuci√≥n te√≥rica propuesta por el modelo cuyo ajuste estamos evaluando con la funci√≥n de
distribuci√≥n emp√≠rica de los datos.
Concretamente, si tenemos X1 , ..., XN una muestra de una v.a. X , si notamos por F (x) a la funci√≥n de
distribuci√≥n del modelo propuesto y por SN (x) a la funci√≥n de distribuci√≥n emp√≠rica asociada a la muestra,
el estad√≠stico que se utiliza para este contraste viene dado por

DN = Sup |F (x) ‚àí SN (x)| .
x

A la hora de calcular este m√°ximo debemos tener en cuenta que la variable x es de tipo continuo.
La hip√≥tesis nula a contrastar es

H0 : los datos de la muestra se ajustan a la distribuci√≥n dada por F (x) ,
frente a la hip√≥tesis alternativa

H1 : los datos de la muestra no se ajustan a la distribuci√≥n dada por F (x) .
Se rechazar√° la hip√≥tesis nula en favor de la alternativa cuando el p-valor asociado al valor que tome DN sea
inferior a 0.05.
Esquem√°ticamente, el proceso en el desarrollo del test puede resumirse en los siguientes pasos:
1. Ordenamos los valores de la muestra de menor a mayor: x(1) , ..., x(N ) .
2. Construimos la funci√≥n de distribuci√≥n emp√≠rica, que en cada valor de la muestra viene dado por

SN x(i) = Ni .
3. El valor del estad√≠stico se calcula como

dN = maÃÅx

1‚â§i‚â§N





 

		
maÃÅx F x(i) ‚àí SN x(i)  , F x(i) ‚àí SN x(i‚àí1)  .

4. Se rechazar√° la hip√≥tesis nula en favor de la alternativa si p = P [DN > dN ] < 0.05, con un (1 ‚àí p) √ó

100 % de conanza.
La distribuci√≥n de probabilidad de DN , necesaria para calcular el p-valor, no es muy conocida. Adem√°s,
para evaluar esta probabilidad hay que tener en cuenta el n√∫mero de par√°metros de la distribuci√≥n en el

178

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

ajuste. Una metodolog√≠a adecuada para ello es conocida como M√©todos de Monte Carlo, aunque excede los
contenidos de estos apuntes. Debo advertir que muchos de los paquetes estad√≠sticos m√°s habituales pueden
inducir a error en el c√°lculo de este p-valor, ya que proporcionan por defecto aqu√©l correspondiente a un
ajuste en el que no se estime ning√∫n par√°metro en la distribuci√≥n bajo la hip√≥tesis nula, dando lugar a una
sobreestimaci√≥n de dicho p-valor.
1.4647
0.2333

0.4995
0.0814

0.7216
0.3035

0.1151
1.7358

0.2717
0.9021

0.7842
0.0667

3.9898
0.0868

0.1967
0.8909

0.8103
0.1124

0.4854
0.0512

Cuadro 9.4: Datos de la muestra.

Ejemplo. Los datos que aparecen en el Cuadro 9.4 corresponden al tiempo en sec. entre conexiones a
un servidor. Nos planteamos si una distribuci√≥n exponencial es adecuada para su ajuste.
En primer lugar hemos de decidir cu√°l es el ajuste propuesto. El estimador m√°ximo veros√≠mil del par√°metro Œª de una exponencial coincide con el estimador del m√©todo de los momentos, ŒªÃÇ =

1
m1 .

En este

caso, ŒªÃÇ = 1/0.6902 = 1. 448 9.
Para calcular el valor del estad√≠stico del contraste, debemos evaluar la funci√≥n de distribuci√≥n de una

exp (1.4489),
F (x) = 1 ‚àí e‚àí1.4489x , x ‚â• 0
con la funci√≥n de distribuci√≥n emp√≠rica. El Cuadro 9.5 muestra ambas funciones de distribuci√≥n. De ella
se deduce que el valor del estad√≠stico de contraste es 0.172 72. El p-valor asociado (calculado por M√©todos
de Monte Carlo con R) toma el valor

P [D20 > 0.172 72] = 0.5707.
Por tanto, no hay en los datos evidencia en contra de asumir que siguen una distribuci√≥n exp (1.4489).
La Figura 9.1 muestra en una vertiente gr√°ca la bondad del ajuste y el punto donde se alcanza la
distancia m√°xima entre las funci√≥n de distribuci√≥n te√≥rica y emp√≠rica.

x(i)
0.0512
0.0667
0.0814
0.0868
0.1124
0.1151
0.1967
0.2333
0.2717
0.3035


F x(i)
7.1499 √ó 10‚àí2
9.2119 √ó 10‚àí2
0.11125
0.11818
0.15029
0.1536
0.24798
0.28682
0.32542
0.3558

i
20

i‚àí1
20

0.05
0.1
0.15
0.2
0.25
0.3
0.25
0.4
0.45
0.5

0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45

x(i)
0.4854
0.4995
0.7216
0.7842
0.8103
0.8909
0.9021
1.4647
1.7358
3.9898


F x(i)
0.50505
0.51506
0.64849
0.67897
0.69089
0.72496
0.72938
0.88023
0.91914
0.99691

i
20

i‚àí1
20

0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1

0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95

Cuadro 9.5: Tabla asociada al Test de Kolmogorov-Smirnov.

Prof. Dr. Antonio Jos√© S√°ez Castillo

179

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.5

1

1.5

2

2.5

3

3.5

4

Figura 9.1: Funciones de distribuci√≥n te√≥rica y emp√≠rica. Valor donde se da el estad√≠stico de KolmogorovSmirnof.

9.3. Contraste de independencia œá2
Si nos damos cuenta, desde el cap√≠tulo de estad√≠stica descriptiva nos hemos centrado exclusivamente en
variables de tipo cuantitativo.
Sin embargo, en numerosas ocasiones el objeto de estudio viene determinado, no por una cantidad, sino
por una cualidad o un estado no cuanticable. Es por ello que vamos a considerar un contraste relativo a
variables de tipo cualitativo, concretamente, para valorar si dos de estas variables est√°n o no signicativamente
relacionadas.

Ejemplo.

¬æEst√° relacionada la ideolog√≠a pol√≠tica con el g√©nero del votante? Es decir, nos planteamos si

el que una persona se declare de izquierdas o de derechas depende de si es var√≥n o mujer. Existen dos
variables cualitativas o caracter√≠sticas que dividen a la poblaci√≥n. Lo que nos interesa es si esa divisi√≥n
est√° o no relacionada. ¬æSer√°n m√°s conservadoras las mujeres?

Consideremos en general una poblaci√≥n en la que cada individuo se clasica de acuerdo con dos caracter√≠sticas,
designadas como X e Y . Supongamos que los posibles valores de X son x1 , ..., xr y los posibles valores de Y
son y1 , ..., ys .
Denotemos por pij a la proporci√≥n de individuos de la poblaci√≥n cuyas caracter√≠sticas son simult√°neamente

xi e yj . Denotemos adem√°s, como pi. a la proporci√≥n de individuos con caracter√≠stica xi y p.j a la proporci√≥n
de individuos con caracter√≠stica yj . En t√©rminos de probabilidades, tendremos que si se elige un individuo al
azar,

P [X = xi , Y = yj ] = pij

P [X = xi ] = pi. =

s
X

pij

j=1

180

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

P [Y = yj ] = p.j =

r
X

pij .

i=1

Lo que pretendemos contrastar es si las dos caracter√≠sticas son independientes, es decir, si para todo i y para
todo j ,

P [X = xi , Y = yj ] = P [X = xi ] √ó P [Y = yj ] ,
es decir, si

pij = pi. √ó p.j .
As√≠ pues, podemos enunciar el contraste como

H0 : pij = pi. √ó p.j para todo i = 1, ..., r; j = 1, ..., s
frente a

H1 : pij 6= pi. √ó p.j para alg√∫n valor de i y j .
Para llevar a cabo el contraste tomaremos una muestra de la poblaci√≥n de tama√±o n. Denotemos por nij los

individuos de esa muestra que toman simult√°neamente el valor xi y el valor yj (frecuencias observadas),
Ps
Pr
ni. = j=1 nij los individuos de la muestra que toman el valor xi y n.j = i=1 nij los que toman el valor

yj .
De esta forma,

pÃÇij =

nij
n

pÃÇi. =

ni.
n

pÃÇ.j =

n.j
n

ser√° un estimador basado en la muestra de pij ,

ser√° un estimador basado en la muestra de pi. y

ser√° un estimador basado en la muestra de p.j .
Por otra parte, si la hip√≥tesis nula fuera cierta, el n√∫mero de individuos en la muestra, de tama√±o n, que
toman simult√°neamente los valores xi y yj ser√≠a

eij = n √ó pi . √ó p.j .
Basado en la muestra, los valores

eÃÇij = n √ó pÃÇi. √ó pÃÇ.j
ni. √ó n.j
=
n
(frecuencias

esperadas) ser√≠an sus estimadores.

Finalmente, el estad√≠stico del contraste se basa en comparar los valores reales en la muestra de nij con
los valores eÃÇij que se dar√≠an si la hip√≥tesis nula fuera cierta, es decir, si las caracter√≠sticas X e Y fueran
Prof. Dr. Antonio Jos√© S√°ez Castillo

181

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

independientes. El valor del estad√≠stico es

d=

r X
s
2
X
(nij ‚àí eÃÇij )
.
eÃÇij
i=1 j=1

Suponiendo que la hip√≥tesis nula es cierta, la distribuci√≥n del estad√≠stico del contraste es œá2 con (r ‚àí 1) (s ‚àí 1)
grados de libertad, por lo que decidiremos en funci√≥n del p-valor asociado,

p = P [D > d/H0 ] ,
donde D ‚Üí œá2(r‚àí1)(s‚àí1) o bien:
Rechazaremos H0 con nivel de signicaci√≥n Œ± si d > œá2(r‚àí1)(s‚àí1);1‚àíŒ± .
No rechazaremos H0 con nivel de signicaci√≥n Œ± si d < œá2(r‚àí1)(s‚àí1);1‚àíŒ± .
Hay que hacer una √∫ltima observaci√≥n: para que en efecto D ‚Üí œá2 con (r ‚àí 1) (s ‚àí 1) es necesario que todas
(o casi todas) las frecuencias esperadas eÃÇij sean mayores o iguales a 5. Si alguna o algunas de ellas no lo
son, la distribuci√≥n œá2 podr√≠a no ser adecuada y el resultado del test incorrecto. Para que esto no ocurra es
recomendable que el tama√±o de la muestra sea grande.

Ejemplo. Se toma una muestra de 300 personas, pregunt√°ndoles si se consideran m√°s de derechas, m√°s
de izquierdas o de centro y anotando su g√©nero. El resultado se resume en la siguiente tabla:
Izquierda

Derecha

Centro

Total

Mujeres

68

56

32

156

Hombres

52

72

20

144

Total

120

128

52

300

Este tipo de tablas se conocen como

tablas de contingencia. Contiene los valores que hemos notado

nij y, en los m√°rgenes inferior y lateral derecho, los valores ni. y n.j .
Vamos a ver si el g√©nero est√° relacionado con la ideolog√≠a. Si no fuera as√≠, si la ideolog√≠a fuera independiente
del g√©nero, se tendr√≠a en una muestra de 300 individuos las frecuencias esperadas ser√≠an
Izquierda
Mujeres
Hombres
Total

182

156
300 300
144
300 300

120
300
120
300

120

Derecha
156
300 300
144
300 300

128
300
128
300

128

Centro
156
300 300
144
300 300

52

Total

52
300
52
300

156
144
300

Izquierda

Derecha

Centro

Total

Mujeres

62.40

66.56

27.04

156

Hombres

57.60

61.44

24.96

144

Total

120

128

52

300

.

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

El valor del estad√≠stico de contraste es, por tanto,
2

2

2

(68 ‚àí 62.40)
(56 ‚àí 66.56)
(32 ‚àí 27.04)
+
+
+
62.40
66.56
27.04
2
2
2
(52 ‚àí 57.60)
(72 ‚àí 61.44)
(20 ‚àí 24.96)
+
+
+
= 6.433.
57.60
61.44
24.96

D=

Por su parte, œá2(2‚àí1)(3‚àí1);0.95 = 5.991, de manera que podemos rechazar la hip√≥tesis nula en favor de la
alternativa, armando con un 95 % de conanza que el genero est√° relacionado con la ideolog√≠a. ¬æEn qu√©
sentido lo estar√°?
Si nos centramos s√≥lo en los de izquierdas, tenemos que el porcentaje de hombres y mujeres es de
68
120

√ó 100 % = 56.667 % y de

52
120

√ó 100 % = 43.333 %, respectivamente.

Si nos centramos s√≥lo en los de derechas, tenemos que el porcentaje de hombres y mujeres es de
56
128

√ó 100 % = 43.75 % y de

72
128

√ó 100 % = 56.25 %, respectivamente.

Finalmente, si nos centramos s√≥lo en los de centro, tenemos que el porcentaje de hombres y mujeres
es de

32
52

√ó 100 = 61.538 % y de

20
52

√ó 100 = 38.462 %, respectivamente.

Lo que parece que ocurre es que las mujeres tienen mayor preferencia por la derecha. Sin embargo, esta
armaci√≥n no se ha contrastado, sino que se basa simplemente en datos descriptivos1 .

9.4. Resoluci√≥n del ejemplo de los accidentes laborales
Redordemos el planteamiento:

En una empresa se sospecha que hay franjas horarias donde los accidentes

laborales son m√°s frecuentes. Para estudiar este fen√≥meno, contabilizan los accidentes laborales que sufren
los trabajadores seg√∫n franjas horarias, durante un a√±o. Los resultados aparecen en la tabla.

Horas del d√≠a
8-10 h.
10-12 h.
13-15 h.
15-17 h.

N√∫mero de accidentes
47
52
57
63

Con esa informaci√≥n, los responsables de seguridad de la empresa deben decidir si hay franjas horarias donde
los accidentes son m√°s probables o si, por el contrario, √©stos ocurren absolutamente al azar.

En primer lugar debemos plantearnos la hip√≥tesis que queremos contrastar. El hecho de que ocurran los
accidentes absolutamente al azar vendr√≠a a decir que la probabilidad de ocurrencia es la misma en cada franja
horaria (puesto que todas ellas tienen la misma amplitud). Por ello, si notamos pi a la probabilidad de que
ocurra un accidente en la i-√©sima franja horaria, nos planteamos como hip√≥tesis nula H0 : p1 = ... = p4 =

1
4

frente a la alternativa de que no todas las probabilidades sean iguales.
Para realizar el contraste podemos considerar un contraste de bondad de ajuste en el que la distribuci√≥n de
probabilidad sea una uniforme discreta, que no tiene par√°metros.
Prof. Dr. Antonio Jos√© S√°ez Castillo

183

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

En este caso, el estad√≠stico de contraste es muy sencillo:

œá2 =

(47 ‚àí 219 √ó (1/4))2
(52 ‚àí 219 √ó (1/4))2
(57 ‚àí 219 √ó (1/4))2
(63 ‚àí 219 √ó (1/4))2
+
+
+
= 2.571.
219 √ó (1/4)
219 √ó (1/4)
219 √ó (1/4)
219 √ó (1/4)

Por su parte, el p-valor es p = P [œá24‚àí0‚àí1 > 2.571] = 0.462, por lo que no tenemos evidencias en estos datos
que hagan pensar en que hay franjas horarias m√°s propicias a los accidentes.

184

Prof. Dr. Antonio Jos√© S√°ez Castillo

Cap√≠tulo 10
Regresi√≥n lineal simple

Un pol√≠tico debe ser capaz de predecir lo que pasar√° ma√±ana, y la semana, el mes y el a√±o
pr√≥ximos. Y tambi√©n debe ser capaz de explicar por qu√© no acert√≥.
Winston Churchill

Resumen. En este cap√≠tulo se describe el modelo de regresi√≥n lineal simple, que asume que entre dos variables
dadas existe una relaci√≥n de tipo lineal contaminada por un error aleatorio. Aprenderemos a estimar dicho
modelo y, a partir de estas estimaciones y bajo determinadas hip√≥tesis, podremos extraer predicciones del
modelo e inferir la fortaleza de dicha relaci√≥n lineal.

Palabras clave: regresi√≥n lineal simple, variable dependiente, variable independiente, error aleatorio, nube
de puntos, principio de m√≠nimos cuadrados, coeciente de correlaci√≥n lineal, coeciente de determinaci√≥n
lineal, bondad del ajuste, predicci√≥n, estimaci√≥n.

10.1. Introducci√≥n
Uno de los aspectos m√°s relevantes que aborda la Estad√≠stica se reere al an√°lisis de las relaciones que se dan
entre dos variables aleatorias. El an√°lisis de estas relaciones est√° muy frecuentemente ligado al an√°lisis de
una variable, llamada variable

dependiente (Y ) , y del efecto que sobre ella tiene otra (u otras) variable(s),
llamada(s) variable(s) independiente(s) (X), y permite responder a dos cuestiones b√°sicas:
¬æEs signicativa la inuencia que tiene la variable independiente sobre la variable dependiente?

Si, en efecto, esa relaci√≥n es signicativa, ¬æc√≥mo es? y ¬æpodemos aprovechar esa relaci√≥n para predecir
valores de la variable dependiente a partir de valores observados de la variable independiente? M√°s a√∫n,
¬æpodemos inferir caracter√≠sticas sobre esa relaci√≥n y con el fen√≥meno que subyace a ella?

Ejemplo. Un equipo de investigadores que trabajan en seguridad en el trabajo est√° tratando de analizar
c√≥mo la piel absorbe un cierto componente qu√≠mico peligroso. Para ello, coloca diferentes vol√∫menes del
compuesto qu√≠mico sobre diferentes segmentos de piel durante distintos intervalos de tiempo, midiendo
al cabo de ese tiempo el porcentaje de volumen absorbido del compuesto. El dise√±o del experimento se ha
185

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

realizado para que la interacci√≥n esperable entre el tiempo y el volumen no inuya sobre los resultados.
Los datos aparecen en el Cuadro 10.1
Lo que los investigadores se cuestionan es si la cantidad de compuesto por un lado y el tiempo de
exposici√≥n al que se somete por otro, inuyen en el porcentaje que se absorbe. De ser as√≠, ser√≠a interesante
estimar el porcentaje de absorci√≥n de personas que se sometan a una exposici√≥n de una determinada
cantidad, por ejemplo, durante 8 horas.
En una primera aproximaci√≥n al problema, podemos observar una representaci√≥n gr√°ca de los datos en
los diagramas de dispersi√≥n o nubes de puntos de la Figura 10.1. ¬æQu√© armar√≠amos? Parece que s√≠ hay
una relaci√≥n lineal m√°s o menos clara (pero no denitiva) entre el tiempo de exposici√≥n y el porcentaje
de absorci√≥n, pero ¬æla hay entre el volumen y el porcentaje de absorci√≥n?

Experimento
1
2
3
4
5
6
7
8
9

Volumen
0.05
0.05
0.05
2.00
2.00
2.00
5.00
5.00
5.00

Tiempo
2
10
24
2
10
24
2
10
24

Porcentaje Absorbido
50.88
49.96
83.66
54.09
68.27
85.65
48.39
64.88
88.01

Cuadro 10.1: Datos sobre el experimento de la absorci√≥n del compuesto

Un modelo de regresi√≥n lineal simple para una variable, Y (variable dependiente), dada otra variable, X

(variable

independiente), es un modelo matem√°tico que permite obtener una f√≥rmula capaz de relacionar

Y con X basada s√≥lo en relaciones lineales, del tipo
Y = Œ≤0 + Œ≤1 X + Œµ.
En esta expresi√≥n:

Y representa a la variable dependiente, es decir, a aquella variable que deseamos estudiar en relaci√≥n
con otras.

X representa a la variable independiente, es decir, aquellas que creemos que puede afectar en alguna
medida a la variable dependiente. La estamos notando en may√∫scula, indicando que podr√≠a ser una
variable aleatoria, pero habitualmente se considera que es una constante que el investigador puede jar
a su antojo en distintos valores.

Œµ representa el error

aleatorio, es decir, aquella cantidad (aleatoria) que provoca que la relaci√≥n entre

la variable dependiente y la variable independiente no sea perfecta, sino que est√© sujeta a incertidumbre.

186

Prof. Dr. Antonio Jos√© S√°ez Castillo

5

80
70
60
50

Porcentaje.Absorbido

80
70
60
50

Porcentaje.Absorbido

Apuntes de Estad√≠stica para Ingenieros

15

0

Tiempo

2

4

Volumen

Figura 10.1: Nube de puntos
Hay que tener en cuenta que el valor de Œµ ser√° siempre desconocido hasta que se observen los valores de X e

Y , de manera que el modelo de predicci√≥n ser√° realmente
YÃÇ = Œ≤0 + Œ≤1 X.
Lo que en primer lugar resultar√≠a deseable de un modelo de regresi√≥n es que estos errores aleatorios ocurran en
la misma medida por exceso que por defecto, sea cual sea el valor de X , de manera que E [Œµ/X=x ] = E [Œµ] = 0
y, por tanto,

E [Y /X=x ] = Œ≤0 + Œ≤1 x + E [Œµ/X=x ]
= Œ≤0 + Œ≤1 x.
Es decir, las medias de los valores de Y para un valor de X dado son una recta.
La Figura 10.2 representa una nube de puntos y la recta de regresi√≥n que los ajusta de unos datos gen√©ricos.
Podemos ver el valor concreto de Œµ = y ‚àí E [Y /X=x ] para un dato, supuesto que hemos obtenido un modelo
de regresi√≥n. En ella se puede ver tambi√©n la interpretaci√≥n de los coecientes del modelo:

Œ≤0 es

la ordenada al origen del modelo, es decir, el punto donde la recta intercepta o corta al eje y.

Œ≤1 representa

la pendiente

de la l√≠nea y, por tanto, puede interpretarse como el incremento de la

variable dependiente por cada incremento en una unidad de la variable independiente.

Prof. Dr. Antonio Jos√© S√°ez Castillo

187

100

105

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

y

Œµi

Œ≤0 + Œ≤1xi

85

90

95

yi

xi
50

60

70

80

90

100

x

Figura 10.2: Diagrama de dispersi√≥n y l√≠nea de las medias hipot√©ticas.

Nota. Es evidente que la utilidad de un modelo de regresi√≥n lineal tiene sentido siempre que la relaci√≥n
hipot√©tica entre X e Y sea de tipo lineal, pero ¬æqu√© ocurre si en vez de ser de este tipo es de otro tipo
(exponencial, logar√≠tmico, hiperb√≥lico...)?
En primer lugar, es absolutamente conveniente dibujar el diagrama de dispersi√≥n antes de comenzar a
tratar de obtener un modelo de regresi√≥n lineal, ya que si la forma de este diagrama sugiere un perl
distinto al de una recta quiz√° deber√≠amos plantearnos otro tipo de modelo.
Y, por otra parte, si se observa que el diagrama de dispersi√≥n es de otro tipo conocido, puede optarse
por realizar un cambio de variable para considerar un modelo lineal. Existen t√©cnicas muy sencillas para
esta cuesti√≥n, pero no las veremos aqu√≠.

10.2. Estimaci√≥n de los coecientes del modelo por m√≠nimos cuadrados
Si queremos obtener el modelo de regresi√≥n lineal

que mejor se ajuste a los datos de la muestra,

deberemos

estimar los coecientes Œ≤0 y Œ≤1 del modelo. Para obtener estimadores de estos coecientes vamos a considerar
un nuevo m√©todo de estimaci√≥n, conocido como

m√©todo de m√≠nimos cuadrados.

Hay que decir que

bajo determinados supuestos que veremos en breve, los estimadores de m√≠nimos cuadrados coinciden con los
estimadores m√°ximo-veros√≠miles de Œ≤0 y Œ≤1 .
El razonamiento que motiva el m√©todo de m√≠nimos cuadrados es el siguiente: si tenemos una muestra de

188

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

valores de las variables independiente y dependiente,

(x1 , y1 ) , ..., (xn , yn ) ,
buscaremos valores estimados de Œ≤0 y Œ≤1 , que notaremos por Œ≤ÃÇ0 y Œ≤ÃÇ1 , de manera que en el modelo ajustado,

yÃÇx = Œ≤ÃÇ0 + Œ≤ÃÇ1 x
minimice la suma de los cuadrados de los errores observados. Recordemos que

E [Y /X=x ] = Œ≤0 + Œ≤1 x,
luego yÃÇx puede interpretarse de dos formas:
1. Como una predicci√≥n del valor que tomar√° Y si X = x.

2. Como una estimaci√≥n del valor medio de Y cuando X = x.
Concretando, lo que buscamos es minimizar la

SSE =

suma de los cuadrados de los errores

n 
X

2
yi ‚àí (Œ≤ÃÇ0 + Œ≤ÃÇ1 xi ) ,

i=1

es decir buscamos






Œ≤ÃÇ0 , Œ≤ÃÇ1 = arg mƒ±ÃÅn SSE .
Œ≤0 ,Œ≤1

Se llama

dada X

recta de regresi√≥n por m√≠nimos cuadrados (o simplemente recta de regresi√≥n) de

Y

a la l√≠nea que tiene la SSE m√°s peque√±a de entre todos los modelos lineales.

La soluci√≥n de ese problema de m√≠nimo se obtiene por el mecanismo habitual: se deriva SSE respecto de Œ≤ÃÇ0
y Œ≤ÃÇ1 , se iguala a cero y se despejan estos. La soluci√≥n es Œ≤ÃÇ1 =

SSxy =
SSxx =

n
X
i=1
n
X

(xi ‚àí xÃÑ) (yi ‚àí yÃÑ) =

SSxy
SSxx
n
X

y Œ≤ÃÇ0 = yÃÑ ‚àí Œ≤ÃÇ1 xÃÑ, donde

xi yi ‚àí nxÃÑyÃÑ

i=1
2

(xi ‚àí xÃÑ) =

i=1

n
X

x2i ‚àí nxÃÑ2 .

i=1

Con esta notaci√≥n, es f√°cil demostrar que

SSE =

n 
X
i=1

2
2
SSxx SSyy ‚àí SSxy
yi ‚àí (Œ≤ÃÇ0 + Œ≤ÃÇ1 xi ) =
SSxx

=SSyy ‚àí

SSxy 2
= SSyy ‚àí SSxy √ó Œ≤ÃÇ1 .
SSxx

Prof. Dr. Antonio Jos√© S√°ez Castillo

189

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

En este sentido, se dene como medida de la calidad del ajuste de la recta de regresi√≥n el

error estandar del

ajuste como

r
se =
s
=

SSE
=
n‚àí2

v

2
uP 
u
t i yi ‚àí Œ≤ÃÇ0 + Œ≤ÃÇ1 x
n‚àí2

SSyy ‚àí Œ≤ÃÇ1 SSxy
.
n‚àí2

Cuanto mayor sea esta cantidad, peor son las predicciones de la recta de regresi√≥n.

Ejemplo. Para los datos sobre el ejemplo de la absorci√≥n del compuesto, vamos a calcular e interpretar
las dos rectas de regresi√≥n posibles.
En primer lugar, vamos a considerar la recta de regresi√≥n para explicar el porcentaje de absorci√≥n (y)
conocido el volumen de sustancia (x):

SSxy = 36.24, SSx = 37.31
luego

Œ≤ÃÇ1 =

SSxy
= 0.97
SSxx

Œ≤ÃÇ0 = yÃÑ ‚àí Œ≤ÃÇ1 xÃÑ = 63.69,
as√≠ que la recta de regresi√≥n ajustada es

yÃÇx = 63.69 + 0.97 √ó x.
La interpretaci√≥n de Œ≤ÃÇ1 = 0.97 es que el porcentaje de absorci√≥n, Y , aumenta en promedio 0.97 por cada
incremento de 1 unidad de volumen de compuesto. La interpretaci√≥n de Œ≤ÃÇ0 = 63.69 ser√≠a la del valor
promedio de Y cuando x = 0, pero es que en este caso este supuesto no tiene sentido, as√≠ que no debe
tenerse en cuenta.
Vamos con la recta de regresi√≥n para explicar el porcentaje de absorci√≥n (y ) en funci√≥n del tiempo de
exposici√≥n (x):

SSxy = 1187.96, SSxx = 744
luego

Œ≤ÃÇ1 =

SSxy
= 1.60
SSxx

Œ≤ÃÇ0 = yÃÑ ‚àí Œ≤ÃÇ1 xÃÑ = 46.82,

190

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 10.3: Nubes de puntos y rectas de regresi√≥n ajustadas en el ejemplo de la absorci√≥n

as√≠ que la recta de regresi√≥n ajustada es

yÃÇx = 46.82 + 1.60 √ó x.
Por cada incremento de una unidad del tiempo de exposici√≥n, el porcentaje de absorci√≥n aumenta en
media 1.60.
Ahora vamos a representar las nubes de puntos de nuevo con sus rectas de regresi√≥n ajustadas. De
esa manera podremos comprobar de una forma gr√°ca c√≥mo de buenas son las rectas en cuanto a su
capacidad de ajuste de los datos. Los resultados aparecen en la Figura 10.3. Podemos ver que el ajuste
es mucho mejor cuando la variable explicativa es el tiempo de absorci√≥n, mientras que si la variable
explicativa es el volumen, la recta no puede pasar cerca de los datos.

Nota. Hay que hacer una observaci√≥n importante que suele conducir a frecuentes errores. La recta de
regresi√≥n para la variable dependiente Y , dada la variable independiente X no es la misma que la recta
de regresi√≥n de X dada Y . La raz√≥n es muy sencilla: para obtener la recta de regresi√≥n de Y dado X
debemos minimizar

n 
X


2
yi ‚àí Œ≤ÃÇ0 + Œ≤ÃÇ1 xi
,

i=1

Prof. Dr. Antonio Jos√© S√°ez Castillo

191

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

mientras que para obtener la recta de regresi√≥n de X dado Y deber√≠amos minimizar
n 
X


2
xi ‚àí Œ≤ÃÇ0 + Œ≤ÃÇ1 yi
,

i=1

en cuyo caso obtendr√≠amos como soluci√≥n

Œ≤ÃÇ1 =

SSxy
SSyy

Œ≤ÃÇ0 = xÃÑ ‚àí Œ≤ÃÇ1 yÃÑ,
siendo la recta de regresi√≥n, xÃÇ = Œ≤ÃÇ0 + Œ≤ÃÇ1 y .
El error que suele cometerse con frecuencia es pensar que si tenemos, por ejemplo, la recta de Y dado

X , la de X dado Y puede obtenerse

despejando.

Es importante que, para terminar este apartado, recordemos que Œ≤ÃÇ0 y Œ≤ÃÇ1 son s√≥lo estimaciones de Œ≤0 y Œ≤1 ,
estimaciones basadas en los datos que se han obtenido en la muestra.
Una forma de hacernos conscientes de que se trata de estimaciones y no de valores exactos (es imposible
conocer el valor exacto de ning√∫n par√°metro poblacional) es proporcionar las estimaciones de los errores
estandar de las estimaciones de Œ≤0 y Œ≤1 . Se conoce que dichas estimaciones son:

s

s2e
SSxx
s 

 
xÃÑ2
1
s.e. Œ≤ÃÇ0 = s2e
+
n SSxx




s.e. Œ≤ÃÇ1 =

Ejemplo.

En el ejemplo de los datos de absorci√≥n hemos estimado los coecientes de las dos rectas

de regresi√≥n del porcentaje de absorci√≥n en funci√≥n del volumen y del tiempo de absorci√≥n. Vamos
a completar ese an√°lisis con el c√°lculo de los errores estandares de esas estimaciones. Los resultados
aparecen resumidos en la siguiente tabla:
Modelo

Œ≤ÃÇ0

 
s.e. Œ≤ÃÇ0

Œ≤ÃÇ1

 
s.e. Œ≤ÃÇ1

% absorcioÃÅn = Œ≤0 + Œ≤1 √ó V olumen

63.69

8.80

0.97

2.83

% absorcioÃÅn = Œ≤0 + Œ≤1 √ó T iempo

46.82

3.16

1.60

0.21

Obs√©rvese que los errores estandar en el modelo en funci√≥n del volumen son mayores proporcionalmente
que en el modelo en funci√≥n del tiempo de absorci√≥n.

10.3. Supuestos adicionales para los estimadores de m√≠nimos cuadrados
Hasta ahora lo √∫nico que le hemos exigido a la recta de regresi√≥n es:

192

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

1. Que las medias de Y para cada valor de x se ajusten

m√°s o menos

a una l√≠nea recta, algo f√°cilmente

comprobable con una nube de puntos. Si el aspecto de esta nube no recuerda a una l√≠nea recta sino a
otro tipo de funci√≥n, l√≥gicamente no haremos regresi√≥n lineal.
2. Que los errores tengan media cero, independientemente del valor de x, lo que, por otra parte, no es una
hip√≥tesis sino m√°s bien un requerimiento l√≥gico al modelo.
Lo que ahora vamos a hacer es a√±adir algunos supuestos al modelo de manera que cuando √©stos se cumplan,
las propiedades de los estimadores de los coecientes del modelo sean muy buenas. Esto nos va a permitir
hacer inferencia sobre estos coecientes y sobre las estimaciones que pueden darse de los valores de la variable
dependiente.
Los supuestos que podemos a√±adir se reeren al error del modelo, la variable Œµ.

Supuesto 1. Tal y como ya hemos dicho, E [/X=x ] = E [] = 0, lo que implica que E [Y /X=x ] = Œ≤0 + Œ≤1 x.
Supuesto 2. La varianza de  tambi√©n es constante para cualquier valor de x dado, es decir, V ar (/X=x ) = œÉ2
para todo x.

Supuesto 3. La distribuci√≥n de probabilidad de  es normal.
Supuesto 4. Los errores  son independientes unos de otros, es decir, la magnitud de un error no inuye en
absoluto en la magnitud de otros errores.
En resumen, todos los supuestos pueden resumirse diciendo que  |X=x ‚Üí N (0, œÉ 2 ) y son independientes entre
s√≠.
Estos supuestos son restrictivos, por lo que deben comprobarse cuando se aplica la t√©cnica. Si el tama√±o de
la muestra es grande, la hip√≥tesis de normalidad de los residuos estar√° bastante garantizada por el teorema
central del l√≠mite. En cuanto a la varianza constante respecto a los valores de x, un incumplimiento moderado
no es grave, pero s√≠ si las diferencias son evidentes.
Existen t√©cnicas espec√≠cas para evaluar en qu√© medida se cumplen estas hip√≥tesis. Tambi√©n existen procedimientos para corregir el incumplimiento de estos supuestos. Estos aspectos ser√°n tratados al nal del
tema.

10.4. Inferencias sobre el modelo
10.4.1. Inferencia sobre la pendiente
Al comienzo del cap√≠tulo nos plante√°bamos como uno de los objetivos de la regresi√≥n el decidir si el efecto de
la variable independiente es o no signicativo para la variable dependiente. Si nos jamos, esto es equivalente
a contrastar si el coeciente Œ≤1 es o no signicativamente distinto de cero. Vamos a profundizar en porqu√© es
as√≠.
Observemos la Figura 10.4. En la nube de puntos y la recta de regresi√≥n ajustada de la izquierda, ¬æobservamos
una relaci√≥n lineal

buena

entre x e y con un buen ajuste de la recta de regresi√≥n? Cabr√≠a pensar que s√≠, pero

Prof. Dr. Antonio Jos√© S√°ez Castillo

193

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 10.4: Nubes de puntos y rectas de regresi√≥n que las ajustan
estar√≠amos equivocados: si la recta de regresi√≥n trata de explicar y en funci√≥n de x, ¬æcu√°nto var√≠a y conforme
var√≠a x? Dado que la pendiente de esa recta es cero o pr√°cticamente cero, por mucho que cambies x, eso
no afecta al valor de y , es decir, ¬Ωx

no inuye nada sobre y!

Sin embargo, en la nube de puntos de la

derecha, a pesar de que aparentemente el ajuste es peor, la recta ajustada s√≠ tiene pendiente distinta de cero,
luego el hecho de que y var√≠e viene dado en buena parte por el hecho de que x var√≠a, y ello ocurre porque la
pendiente de esa recta es distinta de cero. As√≠ pues, no lo olvidemos: decir que dos variables est√°n relacionadas
linealmente equivale a decir que la pendiente de la recta de regresi√≥n que ajusta una en funci√≥n de la otra es
distinta de cero.
Pues bien, dados los supuestos descritos en la secci√≥n anterior, es posible obtener un contraste de este tipo,
tal y como se resumen en el Cuadro 10.2. En ella, si, en efecto, lo que deseamos es contrastar si el efecto de
la variable independiente es o no signicativo para la variable dependiente, el valor de b1 ser√° cero.

Ejemplo. Para los datos del ejemplo sobre la absorci√≥n, part√≠amos del deseo de comprobar si al volumen
y/o el tiempo de exposici√≥n inu√≠an sobre el porcentaje de absorci√≥n. Las nubes de puntos y el ajuste de
la recta ya nos dieron pistas: daba la impresi√≥n de que el tiempo de absorci√≥n s√≠ inu√≠a en el porcentaje
de absorci√≥n, pero no quedaba tan claro si el volumen lo hac√≠a. Es el momento de comprobarlo.
Nos planteamos en primer lugar si el tiempo de exposici√≥n inuye o no sobre el porcentaje de absorci√≥n,
es decir, nos planteamos si en el modelo lineal

P orcentaje de absorcioÃÅn = Œ≤0 + Œ≤1 √ó T iempo de exposicioÃÅn + Œµ

194

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo de prueba
Hip√≥tesis
Estad√≠stico
de contraste
Regi√≥n
de rechazo
p-valor
Supuestos

Unilateral a
la izquierda
H 0 : Œ≤ 1 = b1
H1 : Œ≤1 < b1

Bilateral

H0 : Œ≤1 = b1
H1 : Œ≤1 6= b1

ÀÜ

t = ‚àöŒ≤21 ‚àíb1

se /SSxx

t < tŒ±;n‚àí2

, s2e =

Unilateral
a la derecha
H0 : Œ≤1 = b1
H1 : Œ≤1 > b1

SSyy ‚àíŒ≤ÃÇ1 SSxy
n‚àí2

|t| > t1‚àíŒ±/2;n‚àí2

=

SSE
n‚àí2

t > t1‚àíŒ±;n‚àí2

P [Tn‚àí2 < t]
2P [Tn‚àí2 > |t|]
P [T > t]
Los dados en la Secci√≥n 10.3
Cuadro 10.2: Contraste sobre Œ≤1

el coeciente Œ≤1 es o no cero. Formalmente, nos planteamos H0 : Œ≤1 = 0 frente a H1 : Œ≤1 6= 0:

Œ≤ÃÇ1 = 1.6
s2e =

SSyy ‚àí Œ≤ÃÇ1 SSxy
= 32.82
n‚àí2

t0.975;9‚àí2 = 2.364624, t0.025;30‚àí2 = ‚àí2.364624
1.6 ‚àí 0
t= p
= 7.60,
32.82/744
luego, como cab√≠a esperar, podemos armar a la luz de los datos y con un 95 % de conanza que el
efecto del tiempo de exposici√≥n sobre el porcentaje de absorci√≥n es signicativo. El p-valor, de hecho, es

p = 2P [T7 > 7.60] = 0.000126.
Vamos ahora a analizar si el efecto lineal del volumen sobre el porcentaje de absorci√≥n es signicativo.
Es decir, ahora nos planteamos si en el modelo lineal

P orcentaje de absorcioÃÅn = Œ≤0 + Œ≤1 √ó V olumen + Œµ
el coeciente Œ≤1 es o no cero, es decir, planteamos el contraste de H0 : Œ≤1 = 0 frente a H1 : Œ≤1 6= 0:

Œ≤ÃÇ1 = 0.97
s2e =

SSyy ‚àí Œ≤ÃÇ1 SSxy
= 298.77
n‚àí2

t0.975;9‚àí2 = 2.364624, t0.025;30‚àí2 = ‚àí2.364624
0.97 ‚àí 0
t= p
= 0.34,
298.77/37.31
luego, como cab√≠a esperar, no podemos armar a la luz de los datos y con un 95 % de conanza que el
efecto del volumen sobre el porcentaje de absorci√≥n sea signicativo. El p-valor, de hecho, es p = 2P [T7 >

0.34] = 0.741.
En vista de los resultados, a partir de ahora dejaremos de considerar el efecto del volumen sobre el
porcentaje de absorci√≥n, y s√≥lo tendremos en cuenta el efecto del tiempo de exposici√≥n.

Prof. Dr. Antonio Jos√© S√°ez Castillo

195

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Ejemplo. Un ingeniero qu√≠mico est√° calibrando un espectr√≥metro para medir la concentraci√≥n de CO
en muestras de aire. Esta calibraci√≥n implica que debe comprobar que no hay diferencias

signicativas

entre la concentraci√≥n verdadera de CO (x) y la concentraci√≥n medida por el espectr√≥metro (y ). Para
ello toma 11 muestras de aire en las que conoce su verdadera concentraci√≥n de CO y las compara con la
concentraci√≥n medida por el espectr√≥metro. Los datos son los siguientes (las unidades son ppm):

x

0

10

20

30

40

50

60

70

80

90

100

y

1

12

20

29

38

48

61

68

79

91

97

Lo ideal, lo deseado, ser√≠a que y = x, es decir, que el modelo lineal que explica y en funci√≥n de x tuviera
coecientes Œ≤0 = 0 y Œ≤1 = 1. Por ahora vamos a centrarnos en el primer paso en la comprobaci√≥n de que
el espectr√≥metro est√° bien calibrado, que implica contrastar que Œ≤1 = 1. Para ello,

SSxx = 11000; SSyy = 10506.73; SSxy = 10740
10460
Œ≤ÃÇ1 =
= 0.976
11000
SSyy ‚àí Œ≤ÃÇ1 SSxy
s2e =
= 2.286
n‚àí2
por lo tanto,

0.976 ‚àí 1
= ‚àí1.639.
t= p
1.964/11000

Dado que t1‚àí 0.05
= t0.975;9 = 2.262 y |‚àí1.639| < 2.262, no hay razones para concluir que Œ≤1 6= 1.
2 ;11‚àí2
As√≠ pues, el modelo podr√≠a ser

y = Œ≤0 + x,
aunque lo deseado, insistamos, ser√≠a que fuera

y = x,
es decir, que lo que mida el espectr√≥metro coincida con la cantidad real de CO en el aire. Como hemos
dicho, eso ocurrir√≠a si Œ≤0 = 0, lo que equivale a decir que en ausencia de CO, el espectr√≥metro est√© a
cero.

Adem√°s del contraste de hip√≥tesis, es trivial proporcionar un intervalo de conanza para la pendiente, ya que
conocemos su estimaci√≥n, su error estandar y la distribuci√≥n en el muestreo (t-student, como aparece en el
contraste). Concretamente,

h

 
 i
P Œ≤1 ‚àà Œ≤ÃÇ1 ‚àí t1‚àí Œ±2 ;n‚àí2 √ó s.e. Œ≤ÃÇ1 , Œ≤ÃÇ1 + t1‚àí Œ±2 ;n‚àí2 √ó s.e. Œ≤ÃÇ1
= 1 ‚àí Œ±.

Ejemplo.

En el ejemplo que acabamos de ver sobre la calibraci√≥n del espectr√≥metro, el intervalo de

conanza para Œ≤1 es (0.94, 1.01). Como podemos ver, el valor Œ≤1 = 1 es un valor conable del intervalo,
luego raticamos que no podemos armar que el espectr√≥metro est√© mal calibrado.

196

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Tipo de prueba
Hip√≥tesis

Unilateral a
la izquierda
H 0 : Œ≤ 0 = b0
H1 : Œ≤0 < b0

Estad√≠stico
de contraste
Regi√≥n
de rechazo
p-valor
Supuestos

t=

r
s2e

Bilateral

H0 : Œ≤0 = b0
H1 : Œ≤0 6= b0

Œ≤ÀÜ0 ‚àíb0

,
1
xÃÑ2
n + SSxx

s2e =

SSyy ‚àíŒ≤ÃÇ1 SSxy
n‚àí2

|t| > t1‚àíŒ±/2;n‚àí2

t < tŒ±;n‚àí2

Unilateral
a la derecha
H0 : Œ≤0 = b0
H1 : Œ≤0 > b0

=

SSE
n‚àí2

t > t1‚àíŒ±;n‚àí2

P [Tn‚àí2 < t]
2P [Tn‚àí2 > |t|]
P [T > t]
Los dados en la Secci√≥n 10.3
Cuadro 10.3: Contraste sobre Œ≤0

10.4.2. Inferencia sobre la ordenada en el origen
Este √∫ltimo ejemplo pone de maniesto que tambi√©n puede tener inter√©s realizar contrastes sobre el valor de

Œ≤0 . Para ello, el Cuadro 10.3 describe el procedimiento de un contraste de este tipo.
Finalmente, tengamos en cuenta que podr√≠a ser de inter√©s un contraste conjunto sobre Œ≤0 y Œ≤1 , por ejemplo,
del tipo Œ≤0 = 0, Œ≤1 = 1. Hay que decir que este tipo de contrastes m√∫ltiples superan los contenidos de esta
asignatura. Lo √∫nico que podr√≠amos hacer en un contexto como el nuestro es realizar sendos contrastes sobre

Œ≤0 y Œ≤1 por separado, teniendo en cuenta el nivel de signicaci√≥n de ambos contrastes.

Ejemplo. En el ejemplo anterior, vamos a contrastar si, en efecto, Œ≤0 = 0, lo que equivaldr√° a concluir
que no hay razones para pensar que el espectr√≥metro est√° mal calibrado. Para ello,

Œ≤ÃÇ0 = yÃÑ ‚àí Œ≤ÃÇ1 xÃÑ = 0.636
por lo tanto,

t= q

0.636 ‚àí 0
2.286

1
11

+

502
11000

 = 0.746.

Comoquiera que 0.746 < t0.975;9 = 2.261, tampoco tenemos razones para pensar que Œ≤0 = 0 con un 95 %
de conanza, luego, en resumen, no existen razones para pensar que el espectr√≥metro est√° mal calibrado.

Ejemplo.

Imaginemos que deseamos comprobar experimentalmente que, tal y como predice la ley de

Ohm, la tensi√≥n (V ) entre los extremos de una resistencia y la intensidad de corriente (I ) que circula
por ella se relacionan siguiendo la ley

V = R √ó I,
donde R es el valor de la resistencia. Nosotros vamos a realizar la comprobaci√≥n con una misma resistencia,
variando los valores de la intensidad, por lo que la ecuaci√≥n equivale a

V = Œ≤0 + Œ≤1 √ó I,
siendo Œ≤0 = 0 y Œ≤1 = R. Los datos son los que aparecen en el Cuadro 10.4.
Tenemos que realizar un contraste, H0 : Œ≤0 = 0 frente a H1 : Œ≤0 6= 0 que equivale a contrastar en realidad

Prof. Dr. Antonio Jos√© S√°ez Castillo

197

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Observaci√≥n
1
2
3
4
5
6
7
8
9
10
11

I (mA)
0.16
6.54
12.76
19.26
25.63
31.81
38.21
47.40
54.00
60.80
68.00

V (V)
0.26
1.04
2.02
3.05
4.06
5.03
6.03
7.03
8.06
8.99
10.01

Cuadro 10.4: Datos para la comprobaci√≥n de la Ley de Ohm

que nuestros aparatos de medida est√°n bien calibrados, puesto que la ley de Ohm obliga a que Œ≤0 = 0.
Vamos all√°:

SSxx = 5105.90
SSyy = 107.25
SSxy = 739.49
Œ≤ÃÇ1 = 0.14
Œ≤ÃÇ0 = 0.25
s2e = 0.022
As√≠ pues,

t= q

0.25 ‚àí 0
0.022

1
11

+

33.142
5105.90

 = 3.531.

Dado que t0.975,9 = 2.262, tenemos que rechazar la hip√≥tesis H0 : Œ≤0 = 0, lo que

¬Ωcontradice la ley de

Ohm! Lo que este an√°lisis pone de maniesto es que tenemos alg√∫n problema en nuestras mediciones.

Dejemos un poco de lado este √∫ltimo resultado. Si queremos estimar el valor de la resistencia, una
estimaci√≥n puntual es, como hemos visto, RÃÇ = Œ≤ÃÇ1 = 0.14, y un intervalo de conanza al 95 % de conanza
(omitimos los detalles de los c√°lculos) resulta ser (0.141, 0.149).
Finalmente, podemos tambi√©n proporcionar un intervalo de conanza para la ordenada en el origen, dado
por

h

 
 i
P Œ≤0 ‚àà Œ≤ÃÇ0 ‚àí t1‚àí Œ±2 ;n‚àí2 √ó s.e. Œ≤ÃÇ0 , Œ≤ÃÇ0 + t1‚àí Œ±2 ;n‚àí2 √ó s.e. Œ≤ÃÇ0
= 1 ‚àí Œ±.

Ejemplo.

En el ejemplo del espectr√≥metro, el intervalo de conanza para la ordenada en el origen es

(‚àí1.29, 2.57), luego es conable pensar que Œ≤0 = 0. En suma, hemos comprobado que es posible Œ≤1 = 1 y
Œ≤0 = 0, luego hemos comprobado que la ecuaci√≥n y = x no puede ser rechazada con los datos disponibles,
es decir, que no hay razones para pensar que el espectr√≥metro est√© mal calibrado.

198

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Ejemplo. En el ejemplo de la comprobaci√≥n de la Ley de Ohm, el intervalo de conanza al 95 % para la
ordenada en el origen es (0.09, 0.41). Dado que ese intervalo no incluye al cero, podemos armar con un
95 % de conanza que la recta de regresi√≥n no pasa por el origen, lo que contradice la Ley de Ohm.

10.5. El coeciente de correlaci√≥n lineal
Œ≤ÃÇ1 mide en cierto modo la relaci√≥n que existe entre la variable dependiente y la variable independiente, ya
que se interpreta como el incremento que sufre Y por cada incremento unitario de X . Sin embargo, es una
medida sujeta a la escala de las variables X e Y , de manera que se hace dif√≠cil poder comparar distintos Œ≤ÃÇ10 s
entre s√≠.
En esta secci√≥n vamos a denir el llamado

coeciente de correlaci√≥n lineal,

que ofrece una medida

cuantitativa de la fortaleza de la relaci√≥n lineal entre X e Y en la muestra, pero que a diferencia de Œ≤ÃÇ1 , es
adimensional, ya que sus valores siempre est√°n entre ‚àí1 y 1, sean cuales sean las unidades de medida de las
variables.
Dada una muestra de valores de dos variables (x1 , y1 ) , ..., (xn , yn ), el

muestral r se dene como

r= p

coeciente de correlaci√≥n lineal

‚àö
SSxy
SSxx
=p
Œ≤ÃÇ1 .
SSxx SSyy
SSyy

Como coment√°bamos, la interpretaci√≥n del valor de r es la siguiente:

r cercano o igual a 0 implica poca o ninguna relaci√≥n lineal entre X e Y.
Cuanto m√°s se acerque a 1 √≥ -1, m√°s fuerte ser√° la relaci√≥n lineal entre X e Y .
Si r = ¬±1, todos los puntos caer√°n exactamente en la recta de regresi√≥n.
Un valor positivo de r implica que Y tiende a aumentar cuando X aumenta, y esa tendencia es m√°s
acusada cuanto m√°s cercano est√° r de 1.
Un valor negativo de r implica que Y disminuye cuando X aumenta, y esa tendencia es m√°s acusada
cuanto m√°s cercano est√° r de -1.

Nota.

En la Figura 10.5 aparecen algunos de los supuestos que acabamos de enunciar respecto a los

distintos valores de r. Hay que hacer hincapi√© en que r s√≥lo es capaz de descubrir la presencia de relaci√≥n
de tipo lineal. Si, como en el √∫ltimo gr√°co a la derecha de esta gura, la relaci√≥n entre X e Y no es de
tipo lineal, r no es adecuado como indicador de la fuerza de esa relaci√≥n.

Nota. En la Figura 10.6 aparece un valor at√≠pico entre un conjunto de datos con una relaci√≥n lineal m√°s
que evidente. Por culpa de este dato, el coeciente de correlaci√≥n lineal ser√° bajo. ¬æQu√© debe hacerse en

Prof. Dr. Antonio Jos√© S√°ez Castillo

199

0

20

60

100

Correlaci√≥n lineal positiva fuerte

10000

20

6000

10
0

2000

‚àí10
‚àí20
0

20

60

100

Correlaci√≥n lineal negativa fuerte

0

‚àí30

0

‚àí100

20

40

60

80

‚àí60 ‚àí40 ‚àí20

100

0

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

0

20

60

100

Ausencia de correlaci√≥n lineal

0

20

60

100

Correlaci√≥n parab√≥lica

Figura 10.5: Valores de r y sus implicaciones.

este caso? En general, no se deben eliminar datos de una muestra, pero podr√≠a ocurrir que datos at√≠picos
correspondan a errores en la toma de las muestras, en el registro de los datos o, incluso, que realmente no
procedan de la misma poblaci√≥n que el resto de los datos: en ese caso, eliminarlos podr√≠a estar justicado
de cara a analizar de una forma m√°s precisa la relaci√≥n lineal entre los datos.

Nota.

Correlaci√≥n frente a causalidad. Hay que hacer una advertencia importante acerca de las inter-

pretaciones del coeciente de correlaci√≥n lineal. Es muy frecuente que se utilice para justicar relaciones
causa-efecto, y eso es un grave error. r s√≥lo indica presencia de relaci√≥n entre las variables, pero eso no
permite inferir, por ejemplo, que un incremento de X sea la causa de un incremento o una disminuci√≥n
de Y .

Ejemplo. Para los datos del ejemplo sobre la absorci√≥n, calculemos r e interpret√©moslo.
En el caso del porcentaje de absorci√≥n en funci√≥n del volumen de compuesto,

r= ‚àö

36.24
= 0.129;
37.30 √ó 2126.61

vemos que la relaci√≥n es muy peque√±a; de hecho, comprobamos mediante un contraste de hip√≥tesis sobre

Œ≤1 que era no signicativa.
En el caso del porcentaje de absorci√≥n en funci√≥n del tiempo de absorci√≥n,

r= ‚àö

36.24
= 0.944.
744 √ó 2126.61

Esta relaci√≥n s√≠ resulta ser muy fuerte y en sentido directo. Por eso al realizar el test sobre Œ≤1 , √©ste s√≠
result√≥ ser signicativo.
No podemos olvidar que el coeciente de correlaci√≥n lineal muestral, r, mide la correlaci√≥n entre los valores

200

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

r = 0.27 r^2 = 0.07
Slope = 0.26 Intercept = 3.56
10

End

6

8

LS Line

4

y

Add Point

0

2

Delete Point

0

2

4

6

8

10

Move Point

x

Figura 10.6: Un dato at√≠pico entre datos relacionados linealmente.

de X y de Y en la muestra. Existe un coeciente de correlaci√≥n lineal similar pero que se reere a todos los
posibles valores de la variable. Evidentemente, r es un estimador de este coeciente poblacional.

Dadas dos variables X e Y , el

coeciente de correlaci√≥n lineal poblacional, œÅ, se dene comoa
‚àö
E [(X ‚àí EX) (Y ‚àí EY )]
V arX
‚àö
œÅ=
= ‚àö
Œ≤1 .
V arXV arY
V arY

a Este concepto se estudia tambi√©n en el cap√≠tulo de vectores aleatorios.

Inmediatamente surge la cuesti√≥n de las inferencias. Podemos y debemos utilizar r para hacer inferencias
sobre œÅ. De todas formas, en realidad estas inferencias son equivalentes a las que hacemos sobre Œ≤1 , ya que la
relaci√≥n entre Œ≤1 y œÅ provoca que la hip√≥tesis H0 : Œ≤1 = 0 sea equivalente a la hip√≥tesis H0 : œÅ = 0. Podemos,
por lo tanto, utilizar el contraste resumido en el Cuadro 10.2 para b1 = 0 y teniendo en cuenta que

‚àö
r n‚àí2
t= ‚àö
.
1 ‚àí r2

Ejemplo. Vamos a contrastar H0
El estad√≠stico de contraste es t =

: œÅ = 0 frente a H1 : œÅ 6= 0 de nuevo en el ejemplo de la absorci√≥n.

‚àö
9‚àí2
0.944√ó
‚àö
1‚àí0.9442

= 7.60, que coincide con el valor de t cuando contrastamos

H0 : Œ≤1 = 0, frente a H1 : Œ≤1 6= 0. Vemos que, en efecto, es el mismo contraste.

Prof. Dr. Antonio Jos√© S√°ez Castillo

201

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

10.6. Fiabilidad de la recta de regresi√≥n. El coeciente de determinaci√≥n lineal
Como hemos visto, el coeciente de correlaci√≥n lineal puede interpretarse como una medida de la contribuci√≥n
de una variable a la predicci√≥n de la otra mediante la recta de regresi√≥n. En esta secci√≥n vamos a ver una
medida m√°s adecuada para valorar hasta qu√© punto la variable independiente contribuye a predecir la variable
dependiente.
Recordemos lo que hab√≠amos observado en la Figura 10.4. All√≠ ten√≠amos una recta, la de la izquierda, que
aparentemente era buena, mientras que la de la derecha aparentemente era peor. Sin embargo, ya dijimos que
eso era inexacto. En realidad nosotros no deseamos comprobar exactamente si los puntos est√°n o no en torno
a la recta de regresi√≥n, sino en qu√© medida la recta de regresi√≥n explica Y en funci√≥n de X .
Vamos a entrar en detalles. Necesitamos que la recta explique Y en funci√≥n de X porque Y tiene datos que
presentan una cierta variabilidad: ¬æcu√°nta variabilidad? Cuando denimos la varianza, esa variabilidad la
medimos como

SSyy =

n
X

2

(yi ‚àí yÃÑ) ,

i=1

de tal manera que cuanto m√°s var√≠en los datos de Y mayor ser√° SSyy . Por otra parte, cuando ajustamos por
la recta de regresi√≥n yÃÇx = Œ≤ÃÇ0 + Œ≤ÃÇ1 √ó x, medimos el error que cometemos en el ajuste con

SSE =

n
X

2

(yi ‚àí yÃÇx ) .

i=1

Vamos a ponernos en las dos situaciones l√≠mite que pueden darse en cuanto a la precisi√≥n de una recta de
regresi√≥n:
Si X no tiene ning√∫n tipo de relaci√≥n lineal con Y , entonces œÅ = 0, en cuyo caso Œ≤1 =
la recta es simplemente

‚àö
‚àö V arY œÅ
V arX

=0y

yÃÇi = Œ≤0 + Œ≤1 xi
= yÃÑ.
Es decir, si X no tiene ning√∫n tipo de relaci√≥n lineal con Y , entonces la mejor predicci√≥n que podemos
dar por el m√©todo de m√≠nimos cuadrados es la media. Adem√°s, en ese caso

SSE =

n
X

(yi ‚àí yÃÇi )

2

i=1

=

n
X

2

(yi ‚àí yÃÑ) = SSyy ,

i=1

es decir, SSE es el total de la variaci√≥n de los valores de Y . Est√° claro que esta es la peor de las
situaciones posibles de cara a la precisi√≥n.
Si la relaci√≥n lineal entre X e Y es total, entonces œÅ = 1, en cuyo caso Œ≤1 =

202

‚àö
‚àö V arY .
V arX

Adem√°s, si la

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

relaci√≥n lineal es total, y = yÃÇx , de manera que

SSE =

n
X

2

(yi ‚àí yÃÇi ) = 0.

i=1

Esta, desde luego, es la mejor de las situaciones posibles.
La idea de la medida que vamos a utilizar es cuanticar en qu√© medida estamos m√°s cerca o m√°s lejos de
estas dos situaciones. Dado que SSE , que es la medida del error de la recta de regresi√≥n, puede ir de 0 (mejor
situaci√≥n posible) a SSyy (peor situaci√≥n posible), tan s√≥lo tenemos que relativizar en una escala c√≥moda una
medida de este error.
Se dene el

coeciente de determinaci√≥n lineal como
r2 = 1 ‚àí

SSE
.
SSyy

N√≥tese que la notaci√≥n es r al cuadrado, ya que, en efecto, en una regresi√≥n lineal simple coincide con el
coeciente de correlaci√≥n lineal al cuadrado.
Por lo tanto, la interpretaci√≥n de r2 es la medida en que X contribuye a la explicaci√≥n de Y en una escala de
0 a 1, donde el 0 indica que el error es el total de la variaci√≥n de los valores de Y y el 1 es la precisi√≥n total,
el error 0. La medida suele darse en porcentaje. Dicho de otra forma:

Aproximadamente 100 √ó r2 % de la variaci√≥n total de los valores de
pueden ser explicada mediante la recta de regresi√≥n de Y dada X .

Y

respecto de su media

Ejemplo. En el ejemplo de la absorci√≥n explicada por el tiempo de exposici√≥n, r2 = 0.892, de manera
que podemos decir que el 89 % de la variaci√≥n total de los valores del porcentaje de absorci√≥n puede ser
explicada mediante la recta de m√≠nimos cuadrados dado el tiempo de exposici√≥n. Es evidente que es un
porcentaje importante, que proporcionar√° predicciones relativamente ables.

10.7. Predicci√≥n y estimaci√≥n a partir del modelo
Recordemos que en el modelo ajustado de la recta de regresi√≥n,

yÃÇx = Œ≤ÃÇ0 + Œ≤ÃÇ1 x
y, por otro lado,

E [Y /X=x ] = Œ≤0 + Œ≤1 x,
luego yÃÇx puede interpretarse de dos formas:
1. Como

predicci√≥n del valor que tomar√° Y

Prof. Dr. Antonio Jos√© S√°ez Castillo

cuando X = x.

203

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

2. Como

estimaci√≥n del valor medio de Y

para el valor X = x, es decir, de E [Y /X=x ].

Ambas cantidades est√°n sujetas a incertidumbre, que ser√° tanto mayor cuanto m√°s variabilidad tenga Y, y/o
peor sea el ajuste mediante la recta de regresi√≥n.
Lo que vamos a ver en esta secci√≥n para concluir el tema es c√≥mo establecer

regiones de conanza

para estas

predicciones de los valores de Y y para las estimaciones de los valores medios de Y dados valores de X . Estos
resultados requieren que se veriquen los supuestos adicionales sobre los errores dados en la secci√≥n 10.3.
Podemos garantizar con un (1 ‚àí Œ±) √ó 100 % de conanza que cuando X = x, el valor medio de Y se encuentra
en el intervalo

Ô£´

s

Ô£≠yÃÇx ‚àí t1‚àíŒ±/2;n‚àí2 √ó se

2

1
(x ‚àí xÃÑ)
+
, yÃÇx + t1‚àíŒ±/2;n‚àí2 √ó se
n
SSxx

s

2

Ô£∂

1
(x ‚àí xÃÑ) Ô£∏
+
,
n
SSxx

es decir, podemos garantizar que

Ô£Æ

Ô£´

s

P Ô£∞E[Y /X=x ] ‚àà Ô£≠yÃÇx ‚àì t1‚àíŒ±/2;n‚àí2 √ó se

Ô£∂
Ô£π
(x ‚àí xÃÑ)2 Ô£∏
1
+
|X=x Ô£ª = 1 ‚àí Œ±.
n
SSxx

Asimismo, podemos garantizar con un (1 ‚àí Œ±)√ó100 % de conanza que cuando X = x, el valor Y se encuentra
en el intervalo
Ô£´

s

Ô£≠yÃÇx ‚àí t1‚àíŒ±/2;n‚àí2 √ó se

2

1
(x ‚àí xÃÑ)
1+ +
, yÃÇx + t1‚àíŒ±/2;n‚àí2 √ó se
n
SSxx

Ô£∂
2
1
(x ‚àí xÃÑ) Ô£∏
1+ +
,
n
SSxx

s

es decir, podemos garantizar que

Ô£Æ

Ô£´

P Ô£∞Y ‚àà Ô£≠yÃÇx ‚àì t1‚àíŒ±/2;n‚àí2 √ó se

s

Ô£∂
Ô£π
1
(x ‚àí xÃÑ)2 Ô£∏
1+ +
|X=x Ô£ª = 1 ‚àí Œ±
n
SSxx

Nota. No debemos olvidar que los modelos de regresi√≥n que podemos estimar lo son a partir de los datos
de una muestra de valores de X e Y . A partir de estos modelos podemos obtener, como acabamos de
recordar, predicciones y estimaciones para valores dados de X. Dado que el modelo se basa precisamente
en

esos valores de la muestra, no es conveniente hacer predicciones y estimaciones para valores de X

que se encuentren fuera del rango de valores de X en la muestra.

Ejemplo. En la Figura 10.7 aparece la recta de regresi√≥n para los datos del ejemplo sobre la absorci√≥n
del compuesto junto con l√≠neas que contienen los intervalos de conanza al 95 % para las predicciones y
las estimaciones asociadas a los distintos valores de X .

204

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

x

110

105

Resistencia

100

observed
fit
conf int
pred int

95

90

85

80

50

60

x

70

80

90

100

Velocidad

Figura 10.7: Recta de regresi√≥n con intervalos de conanza al 95 % para las predicciones (franjas m√°s exteriores) y para las estimaciones (franjas interiores) en el ejemplo de la absorci√≥n.

Obs√©rvese que la amplitud de los intervalos se hace mayor en los valores m√°s extremos de X . Es decir,
los errores en las estimaciones y en las predicciones son mayores en estos valores m√°s extremos. Esto
debe ser un motivo a a√±adir al comentario anterior para no hacer estimaciones ni predicciones fuera del
rango de valores de X en la muestra.
Por otra parte, nos plante√°bamos al comienzo de cap√≠tulo que ser√≠a de inter√©s estimar el porcentaje de
absorci√≥n que tendr√° alguien que se someta a un tiempo de exposici√≥n al compuesto de 8 horas. Eso es
una predicci√≥n, as√≠ que como estimaci√≥n puntual daremos

yÃÇ8 = 46.82 + 1.60 √ó 8 = 59.59
y como intervalo de predicci√≥n al 95 %,

Ô£´

s

Ô£≠yÃÇx ‚àì t1‚àíŒ±/2;n‚àí2 √ó se

s
Ô£∂ Ô£´
Ô£∂
2
1
(x ‚àí xÃÑ)2 Ô£∏ Ô£≠
1 (8 ‚àí 12) Ô£∏
1+ +
= 59.59 ‚àì 2.36 √ó 5.73 1 + +
= (45.17, 74.00) .
n
SSxx
9
744

Por el contrario, imaginemos que los trabajadores de una empresa van a estar sometidos todos ellos a
un tiempo de exposici√≥n de 8 horas. En ese caso, no tiene sentido que nos planteemos una predicci√≥n
para saber cu√°l va a ser su porcentaje de absorci√≥n, ya que cada uno de ellos tendr√° un porcentaje
distinto; lo que s√≠ tiene sentido es que nos planteemos cu√°l va a ser el porcentaje medio de absorci√≥n de
los trabajadores sometidos a 8 horas de exposici√≥n al compuesto. Esto es un ejemplo de la estimaci√≥n
de un valor promedio. La estimaci√≥n puntual es la misma que en la predicci√≥n, es decir, 59.59, pero el
intervalo de conanza al 95 % es

Ô£´
Ô£≠yÃÇx ‚àì t1‚àíŒ±/2;n‚àí2 √ó se

s

xÃÑ)2

Ô£∂

Ô£´

1
(x ‚àí
Ô£∏ = Ô£≠59.59 ‚àì 2.36 √ó 5.73
+
n
SSxx

Prof. Dr. Antonio Jos√© S√°ez Castillo

s

2

Ô£∂

1 (8 ‚àí 12) Ô£∏
= (54.66, 64.52) .
+
9
744

205

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

10.8. Diagnosis del modelo
Todo lo relacionado con inferencia sobre el modelo de regresi√≥n se ha basado en el cumplimiento de los
supuestos descritos en el apartado 10.3. Como ya comentamos, en la medida en que todos o algunos de estos
supuestos no se den, las conclusiones que se extraigan en la inferencia sobre el modelo podr√≠an no ser v√°lidas.
Es por ello que es necesario comprobar estos supuestos mediante herramientas de diagn√≥stico. Aqu√≠ vamos a
ver s√≥lo las m√°s b√°sicas, vinculadas al an√°lisis de los residuos y a la gr√°ca de residuos frente a los valores
ajustados.

10.8.1. Normalidad de los residuos
Entre los supuestos del modelo consideramos que los residuos, es decir,

i = yi ‚àí yÃÇi
siguen una distribuci√≥n normal.
Ni que decir tiene que comprobar esta hip√≥tesis en trivial: bastar√° con calcular los residuos, ajustarles una
distribuci√≥n normal y realizar un contraste de bondad de ajuste mediante, por ejemplo, el test de KolmogorovSmirno.

10.8.2. Gr√°ca de residuos frente a valores ajustados
El resto de supuestos se reeren a la varianza constante de los residuos, a su media cero y a su independencia.
Una de las herramientas diagn√≥sticas m√°s simples para estas hip√≥tesis es la llamada gr√°ca
a valores ajustados.

de residuos frente

Se trata de representar en unos ejes cartesianos:

1. En el eje X, los valores yÃÇi de la muestra.
2. En el eje Y, los residuos, i = yi ‚àí yÃÇi .
Habitualmente, se le a√±ade a esta gr√°ca la recta de regresi√≥n de la nube de puntos resultante.
Vamos a ir viendo c√≥mo debe ser esta gr√°ca en el caso de que se cumplan cada uno de los supuestos:
1. Si la media de los residuos es cero, la nube de puntos de la gr√°ca debe hacernos pensar en una recta de
regresi√≥n horizontal situada en el cero, indicando que sea cual sea el valor yÃÇi , la media de los residuos
es cero.
2. Si los errores son independientes, no debe observarse ning√∫n patr√≥n en la gr√°ca, es decir, ning√∫n efecto
en ella que haga pensar en alg√∫n tipo de relaci√≥n entre yÃÇi y i .
3. Si los errores tienen una varianza constante (se habla entonces de

homocedasticidad), la dispersi√≥n

vertical de los puntos de la gr√°ca no debe variar seg√∫n var√≠e el eje X. En caso contrario, se habla de

heterocedasticidad.

Una √∫ltima observaci√≥n: si se dan todas las condiciones que acabamos de mencionar sobre la gr√°ca de
residuos frente a valores ajustados, entonces es

probable,

pero no se tiene la seguridad, de que los supuestos

del modelo sean ciertos.

206

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

5

‚àí5

0

4

‚àí15 ‚àí10

Residuals

5

Residuals vs Fitted

2

50

55

60

65

70

75

80

85

Fitted values
lm(Porcentaje.Absorbido ~ Tiempo)
Figura 10.8: Gr√°ca de valores ajustados vs residuos en el ejemplo de la absorci√≥n

Ejemplo.

Por √∫ltima vez vamos a considerar el ejemplo de la absorci√≥n. En la Figura 10.8 aparece el

gr√°co de residuos vs valores ajustados y podemos ver que a primer vista parece que se dan las condiciones
requeridas:
1. Los puntos se sit√∫an en torno al eje Y = 0, indicando que la media de los residuos parece ser cero.
2. No se observan patrones en los residuos.
3. No se observa mayor variabilidad en algunas partes del gr√°co. Hay que tener en cuenta que son
muy pocos datos para sacar conclusiones.

Prof. Dr. Antonio Jos√© S√°ez Castillo

207

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

208

Prof. Dr. Antonio Jos√© S√°ez Castillo

Parte IV
Procesos aleatorios

209

Cap√≠tulo 11
Procesos aleatorios

The best material model of a cat is another, or preferably the same, cat.
Norbert Wiener,

Resumen.

Philosophy of Science

(1945) (with A. Rosenblueth)

Los procesos aleatorios suponen el √∫ltimo paso en la utilizaci√≥n de modelos matem√°ticos para

describir fen√≥menos reales no determin√≠sticos: concretamente, se trata de fen√≥menos aleatorios que dependen
del tiempo. Se describen principalmente en t√©rminos de sus medias y sus covarianzas. En este cap√≠tulo se
incluyen adem√°s algunos de los ejemplos m√°s comunes de tipos de procesos y su comportamiento cuando se
transmiten a trav√©s de sistemas lineales invariantes en el tiempo.

Palabras clave.

Procesos aleatorios, funci√≥n media, funci√≥n de autocorrelaci√≥n, funci√≥n de autocovarian-

za, procesos estacionarios, procesos gaussianos, proceso de Poisson, sistemas lineales, densidad espectral de
potencia.

11.1. Introducci√≥n
En muchos experimentos de tipo aleatorio el resultado es una funci√≥n del tiempo (o del espacio).
Por ejemplo,
en sistemas de reconocimiento de voz las decisiones se toman sobre la base de una onda que reproduce
las caracter√≠sticas de la voz del interlocutor, pero la forma en que el mismo interlocutor dice una misma
palabra sufre ligeras variaciones cada vez que lo hace;
en un sistema de cola, por ejemplo, en un servidor de telecomunicaciones, el n√∫mero de clientes en el
sistema a la espera de ser atendidos evoluciona con el tiempo y est√° sujeto a condiciones tales que su
comportamiento es

impredecible ;

en un sistema de comunicaci√≥n t√≠pico, la se√±al de entrada es una onda que evoluciona con el tiempo
y que se introduce en un canal donde es contaminada por un ruido aleatorio, de tal manera que es
imposible separar cu√°l es el mensaje original con absoluta
...
211

certeza.

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Desde un punto de vista matem√°tico, todos estos ejemplos tienen en com√∫n que el fen√≥meno puede ser visto
como unas funciones que dependen del tiempo, pero que son desconocidas a priori, porque dependen del
azar.

En este contexto vamos a denir el concepto de proceso aleatorio. Nuestro objetivo, como en cap√≠tulos

anteriores dedicados a variables y vectores aleatorios, es describir desde un punto de vista estad√≠stico el
fen√≥meno, proporcionando medidas de posici√≥n, medidas sobre la variabilidad, etc.

11.1.1. Denici√≥n
Consideremos un experimento aleatorio sobre un espacio muestral ‚Ñ¶. Supongamos que para cada resultado
posible, A, tenemos una observaci√≥n del fen√≥meno dada por una funci√≥n real de variable real, x (t, A), con

t ‚àà I ‚äÇ R. Habitualmente, t representa al tiempo, pero tambi√©n puede referirse a otras magnitudes f√≠sicas.
Para cada A vamos a denominar a x (t, A)

realizaci√≥n o funci√≥n muestral.

Obs√©rvese que para cada t0 ‚àà I , X (t, ¬∑) es una variable aleatoria. Pues bien, al conjunto

{X (t, A) : t ‚àà I, A ‚àà ‚Ñ¶}
lo denominamos

proceso aleatorio (en adelante p.a.) o estoc√°stico.

Si recordamos las deniciones de variable aleatoria y vector aleatorio, podemos ver en qu√© sentido est√°n
relacionados los conceptos de variable, vector y proceso aleatorio. Concretamente, si ‚Ñ¶ es un espacio muestral,
una variable aleatoria es una funci√≥n

X:‚Ñ¶‚ÜíR
que a cada suceso posible le asigna

un n√∫mero real. Por su parte, un vector aleatorio es b√°sicamente una

funci√≥n

X : ‚Ñ¶ ‚Üí RN
que a cada suceso posible le asigna

un vector real.

Finalmente, un proceso aleatorio es b√°sicamente una

funci√≥n

X : ‚Ñ¶ ‚Üí {funciones reales de vble real}
que a cada suceso posible le asigna

una funci√≥n real.

De cara a escribir de ahora en adelante un p.a., lo notaremos normalmente, por ejemplo, como X (t), obviando
as√≠ la variable que hace referencia al elemento del espacio muestral al que va asociada la funci√≥n muestral.
Este convenio es el mismo que nos lleva a escribir X reri√©ndonos a una v.a. o a un vector.

11.1.2. Tipos de procesos aleatorios
El tiempo es una magnitud f√≠sica intr√≠nsecamente continua, es decir, que puede tomar cualquier valor de los
n√∫meros reales. Sin embargo, no siempre es posible observar las cosas

en cada instante del tiempo.

Por eso,

en el √°mbito de los procesos (no s√≥lo estoc√°sticos) es importante preguntarse si el fen√≥meno que representa
el proceso es observado

212

en cada instante

o s√≥lo

en momentos concretos del tiempo.
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 11.1: Representaci√≥n de un proceso aleatorio.

Dado un espacio muestral ‚Ñ¶ y un p.a. denido en √©l,

{X (t, A) : t ‚àà I, A ‚àà ‚Ñ¶} ,
se dice que el proceso es un

p.a. en tiempo discreto si I

es un conjunto numerable.

En el caso de procesos en tiempo discreto se suele escribir Xn o X [n] reri√©ndonos a la notaci√≥n m√°s general

X (n). Por otra parte, el conjunto I normalmente es el conjunto de los enteros o de los enteros positivos,
aunque tambi√©n puede ser un subconjunto de √©stos.
En algunos libros los procesos en tiempo discreto tambi√©n son denominados

secuencias aleatorias.

Dado un espacio muestral ‚Ñ¶ y un p.a. denido en √©l,

{X (t, A) : t ‚àà I, A ‚àà ‚Ñ¶} ,
se dice que el proceso es un

p.a. en tiempo continuo si I

es un intervalo.

En el caso de procesos en tiempo continuo, I es normalmente el conjunto de los reales positivos o un subconjunto de √©stos.
Si nos damos cuenta, esta primera clasicaci√≥n de los p.a. la hemos hecho en funci√≥n del car√°cter discreto
o continuo del tiempo, es decir, del conjunto I . Existe otra clasicaci√≥n posible en funci√≥n de c√≥mo son las
variables aleatorias del proceso, discretas o continuas. Sin embargo, ambos tipos de procesos, con variables
discretas o con variables continuas, pueden estudiarse casi siempre de forma conjunta. Por ello s√≥lo distinProf. Dr. Antonio Jos√© S√°ez Castillo

213

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 11.2: Distintas funciones muestrales de un proceso aleatorio.

Figura 11.3: Distintas funciones muestrales de un proceso.
guiremos p.a. con variables discretas y p.a. con variables continuas si es necesario. En este sentido, cuando
nos reramos a la funci√≥n masa (si el p.a. es de variables discretas) o a la funci√≥n de densidad (si el p.a. es
de variables continuas), hablaremos en general de funci√≥n de densidad.

Ejemplo.

Sea Œæ una variable aleatoria uniforme en (‚àí1, 1). Denimos el proceso en tiempo continuo

X (t, Œæ) como
X (t, Œæ) = Œæ cos (2œÄt) .
Sus funciones muestrales son ondas sinusoidales de amplitud aleatoria en (‚àí1, 1) (Figura 11.2).

Ejemplo.

Sea Œ∏ una variable aleatoria uniforme en (‚àíœÄ, œÄ). Denimos el proceso en tiempo continuo

X (t, œÄ) como
X (t, œÄ) = cos (2œÄt + Œ∏) .
Sus funciones muestrales son versiones desplazadas aleatoriamente de cos (2œÄt) (Figura 11.3).

214

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

11.2. Descripci√≥n de un proceso aleatorio
11.2.1. Descripci√≥n estad√≠stica mediante distribuciones multidimensionales
En general, para especicar c√≥mo es un p.a. de forma precisa es necesario caracterizar la distribuci√≥n de
probabilidad de cualquier subconjunto de variables del proceso. Es decir, si X (t) es un p.a., es necesario
conocer cu√°l es la distribuci√≥n de cualquier vector del tipo

(X (t1 ) , ..., X (tk )) ,
para todo k > 0, (t1 , ..., tk ) ‚äÇ I , mediante su funci√≥n de distribuci√≥n conjunta

FX(t1 ),...,X(tk ) (x1 , ..., xk )
o mediante su funci√≥n de densidad (o masa) conjunta

fX(t1 ),...,X(tk ) (x1 , ..., xk ) .
Sin embargo, no siempre es f√°cil conocer todas las posibles distribuciones de todos los posibles vectores de
variables del proceso. Por ello, para tener una descripci√≥n m√°s sencilla aunque puede que incompleta del
proceso, se acude a las medias, a las varianzas y a las covarianzas de sus variables.

11.2.2. Funci√≥n media y funciones de autocorrelaci√≥n y autocovarianza
Sea un p.a. X (t). Se dene la

funci√≥n media o simplemente la media de X (t) como
ÀÜ

‚àû

XÃÑ (t) = xÃÑ (t) = E [X (t)] =

xfX(t) (x) dx,
‚àí‚àû

para cada t ‚àà I.
N√≥tese que, como su nombre indica, se trata de una funci√≥n determin√≠stica. No tiene ninguna componente
aleatoria. N√≥tese tambi√©n que aunque se est√° escribiendo el s√≠mbolo integral, podr√≠amos estar reri√©ndonos
a una variable discreta, en cuyo caso se tratar√≠a de una suma.

Se dene la

funci√≥n de autocovarianza

o simplemente la

autocovarianza de X (t) como

CX (t, s) = Cov [X (t) , X (s)] = E [(X (t) ‚àí mX (t)) (X (s) ‚àí mX (s))]
ÀÜ ‚àûÀÜ ‚àû
=
(x1 ‚àí xÃÑ (t)) (x2 ‚àí xÃÑ (s)) fX(t),X(s) (x1 , x2 ) dx2 dx1
‚àí‚àû

‚àí‚àû

Prof. Dr. Antonio Jos√© S√°ez Castillo

215

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Se dene la

funci√≥n de autocorrelaci√≥n

o simplemente la

ÀÜ

ÀÜ

‚àû

autocorrelaci√≥n de X (t) como

‚àû

RX (t, s) = E [X (t) ¬∑ X (s)] =

x1 x2 fX(t),X(s) (x1 , x2 ) dx2 dx1
‚àí‚àû

‚àí‚àû

N√≥tese, de cara al c√°lculo, que la diferencia entre ambas funciones tan s√≥lo es el producto de las medias1 .

CX (t, s) = RX (t, s) ‚àí mX (t) ¬∑ mX (s) .
De hecho, si el proceso est√°

centrado en media,

es decir, si su media es constantemente cero, ambas

funciones coinciden.
Por otra parte, la varianza de las variables del proceso puede obtenerse como

V ar (X (t)) = CX (t, t) .
La interpretaci√≥n de la funci√≥n de autocovarianza CX (t, s) es la de una funci√≥n que proporciona una medida
de la interdependencia lineal entre dos v.a. del proceso, X (t) y X (s), que distan œÑ = s ‚àí t unidades de
tiempo. De hecho, ya sabemos que podr√≠amos analizar esta relaci√≥n mediante el coeciente de correlaci√≥n
lineal

œÅX (t, s) = p

CX (t, s)
CX (t, t) CX (s, s)

.

Aparentemente es esperable que tanto m√°s r√°pidamente cambie el proceso, m√°s decrezca la autocorrelaci√≥n
conforme aumenta œÑ , aunque por ejemplo, los procesos peri√≥dicos no cumplen esa propiedad.
En el campo de la teor√≠a de la se√±al aletatoria, a partir de la funci√≥n de autocorrelaci√≥n se puede distinguir
una se√±al cuyos valores cambian muy r√°pidamente frente a una se√±al con variaciones m√°s suaves. En el primer
caso, la funci√≥n de autocorrelaci√≥n y de autocovarianza en instantes t y t + œÑ decrecer√°n lentamente con œÑ ,
mientras que en el segundo, ese descenso ser√° mucho m√°s r√°pido. En otras palabras, cuando la autocorrelaci√≥n
(o la autocovarianza) es alta, entre dos instantes cercanos del proceso tendremos valorer similares, pero cuando
es baja, podremos tener fuertes diferencias entre valores cercanos en el tiempo.
La gran importancia de estas funciones asociadas a un proceso, media y autocovarianza (o autocorrelaci√≥n),
es por tanto que aportan toda la informaci√≥n acerca de la relaci√≥n lineal que existe entre dos v.a. cualesquiera
del proceso. Como hemos dicho, en la pr√°ctica, resulta extremadamente complicado conocer completamente
la distribuci√≥n de un proceso y, cuando esto ocurre, no siempre es sencillo utilizar las t√©cnicas del c√°lculo
de probabilidades para el tratamiento de estos procesos. Sin embargo, tan s√≥lo con la informaci√≥n dada por
la funci√≥n media y la funci√≥n de autocorrelaci√≥n pueden ofrecerse resultados muy relevantes acerca de los
procesos, tal y como hemos visto en el caso de variables y vectores aleatorios.

Ejemplo. La se√±al recibida por un receptor AM de radio es una se√±al sinusoidal con fase aleatoria, dada
por X (t) = A ¬∑ cos (2œÄfc t + Œû) , donde A y fc son constantes y Œû es una v.a. uniforme en (‚àíœÄ, œÄ) .
1 Esta

f√≥rmula es la misma que cuando ve√≠amos la covarianza entre dos variables, calculable como la media del producto menos

el producto de las medias.

216

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

En ese caso,

ÀÜ

œÄ

A cos (2œÄfc t + Œæ)

E [X (t)] =
‚àíœÄ

1
A
Œæ=œÄ
dŒæ =
[sin (2œÄfc t + Œæ)]Œæ=‚àíœÄ
2œÄ
2œÄ

A
(sin (2œÄfc t) cos (œÄ) + cos (2œÄfc t) sin (œÄ) ‚àí sin (2œÄfc t) cos (‚àíœÄ) ‚àí cos (2œÄfc t) sin (‚àíœÄ))
2œÄ
A
=
[0 + 0] = 0.
2œÄ

=



RX (t, t + œÑ ) = E [X (t + œÑ ) X (t)] = E A2 cos (2œÄfc t + 2œÄfc œÑ + Œû) cos (2œÄfc t + Œû)
=

A2
A2
E [cos (4œÄfc t + 2œÄfc œÑ + 2Œû)] +
E [cos (2œÄfc œÑ )]
2
2

A2
=
2

ÀÜ

œÄ

‚àíœÄ

1
A2
cos (4œÄfc t + 2œÄfc œÑ + 2Œæ) dŒæ +
cos (2œÄfc œÑ )
2œÄ
2

A2
A2
A2
=
¬∑0+
cos (2œÄfc œÑ ) =
cos (2œÄfc œÑ ) .
2
2
2
Por tanto,

CX (t, t + œÑ ) = RX (t, t + œÑ ) ‚àí mX (t) mX (t + œÑ ) =

A2
cos (2œÄfc œÑ ) .
2

11.3. Tipos m√°s comunes de procesos aleatorios
En este apartado denimos propiedades que pueden ser vericadas por algunos procesos aleatorios y que les
coneren caracter√≠sticas especiales en las aplicaciones pr√°cticas.

11.3.1. Procesos independientes
Sea un p.a. X (t). Si para cada n instantes de tiempo, t1 , ..., tn , las v.a. del proceso en esos instantes son
independientes, es decir,

fX(t1 ),...,X(tn ) (x1 , ..., xn ) = fX(t1 ) (x1 ) ¬∑ ... ¬∑ fX(tn ) (xn ) ,
se dice que el proceso es

independiente.

La interpretaci√≥n de este tipo de procesos es la de aquellos en donde el valor de la v.a. que es el proceso en
un momento dado no tiene nada que ver con el valor del proceso en cualquier otro instante. Desde un punto
de vista f√≠sico estos procesos son muy

ca√≥ticos

y se asocian en la pr√°ctica a ruidos que no guardan en un

momento dado ninguna relaci√≥n consigo mismos en momentos adyacentes.
Prof. Dr. Antonio Jos√© S√°ez Castillo

217

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

4

3

2

1

0

‚àí1

‚àí2

‚àí3

‚àí4

0

1

2

3

4

5

6

7

8

9

10

Figura 11.4: Funci√≥n muestral de un proceso independiente formado por v.a gaussianas de media cero y
varianza uno.

11.3.2. Procesos con incrementos independientes
Sea un p.a. X (t). Se dice que tiene incrementos independientes si cualquier conjunto de N v.a. del proceso,

X (t1 ) , X (t2 ) , ..., X (tN ), con t1 < t2 < ... < tN son tales que los incrementos
X (t1 ) , X (t2 ) ‚àí X (t1 ) , ..., X (tN ) ‚àí X (tN ‚àí1 )
son independientes entre s√≠.

11.3.3. Procesos de Markov
No debemos perder de vista la complejidad que implica la descripci√≥n estad√≠stica de un proceso aleatorio.
Pensemos por ejemplo que un proceso ha evolucionado hasta un instante t y se conoce esa evoluci√≥n; es decir,
se conoce el valor X (s) = xs para todo s ‚â§ t. Si se desea describir la posici√≥n del proceso en un instante
posterior a t, t + ‚àÜ, ser√≠a necesario calcular la distribuci√≥n condicionada

X (t + ‚àÜ) | {X (s) = xs para todo s ‚â§ t} .
Esto, en general, es bastante complejo.
Adem√°s, ¬ætiene sentido pensar que la evoluci√≥n del proceso en el instante t + ‚àÜ se vea afectada por toda
la historia del proceso, desde el instante inicial s = 0 hasta el √∫ltimo instante de esa historia s = t? Parece
l√≥gico pensar que la evoluci√≥n del proceso tenga en cuenta la historia m√°s reciente de √©ste, pero no toda
la historia. Esta hipotesis se ve avalada por los perles m√°s habituales de las funciones de autocorrelaci√≥n,
donde observamos que la relaci√≥n entre variables del proceso suele decrecer en la mayor√≠a de las ocasiones
conforme aumenta la distancia en el tiempo entre las mismas.
Los procesos de Markov son un caso donde esto ocurre. Se trata de procesos que evolucionan de manera que
en cada instante

218

olvidan

todo su pasado y s√≥lo tienen en cuenta para su evoluci√≥n futura el instante m√°s
Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

reciente, m√°s actual. En el siguiente sentido:
Un proceso X (t) se dice

markoviano o de Markov

si para cualesquiera t1 < ... < tn < tn+1 instantes

consecutivos de tiempo se verica

fX(tn+1 )|X(t1 )=x1 ,...,X(tn )=xn (xn+1 ) = fX(tn+1 )|X(tn )=xn (xn+1 ) .
Esta denici√≥n se suele enunciar coloquialmente diciendo que un proceso de Markov es

aquel cuyo futuro no

depende del pasado sino tan s√≥lo del presente.

11.3.4. Procesos d√©bilmente estacionarios
Una de las propiedades m√°s usuales en los procesos estoc√°sticos consiste en una cierta estabilidad en sus
medias y en sus covaranzas, en el sentido en que vamos a describir a continuaci√≥n.

X (t) es un proceso

d√©bilmente estacionario si

mX (t) es independiente de t y
C (t, s) (o R (t, s)) depende tan s√≥lo de s ‚àí t, en cuyo caso se nota C (s ‚àí t) (√≥ R (s ‚àí t)).
Es importante destacar que la primera de las condiciones es irrelevante, ya que siempre se puede centrar en
media un proceso para que √©sta sea cero, constante. Es decir, en la pr√°ctica es indiferente estudiar un proceso

X (t) con funci√≥n media ¬µX (t) que estudiar el proceso Y (t) = X (t) ‚àí ¬µX (t), con media cero.
La propiedad m√°s exigente y realmente importante es la segunda. Viene a decir que la relaci√≥n entre variables
aleatorias del proceso s√≥lo depende de la distancia en el tiempo que las separa.

Nota.

Vamos a hacer una puntualizaci√≥n muy importante respecto a la notaci√≥n que emplearemos en

adelante. Acabamos de ver que si un proceso es d√©bilmente estacionario, sus funciones de autocovarianza
y de autocorrelaci√≥n, C (s, t) y R (s, t) no dependen en realidad de s y de t, sino tan s√≥lo de t ‚àí s. Por
eso introducimos la notaci√≥n

C (t, s) ‚â° C (s ‚àí t)
R (t, s) = R (s ‚àí t) .
Por lo tanto, ¬æqu√© queremos decir si escribimos directamente C (œÑ ) o R (œÑ )? Que tenemos un p.a. d√©bilmente estacionario y que hablamos de

C (œÑ ) = C (t, t + œÑ )
R (œÑ ) = R (t, t + œÑ ) .

Una medida importante asociada a un proceso d√©bilmente estacionario es la

potencia
promedio
,
h
i

da como la media del cuadrado de √©ste en cada instante t, es decir RX (0) = E |X (t)|

2

deni-

. M√°s adelante

observaremos con detenimiento esta medida.
Prof. Dr. Antonio Jos√© S√°ez Castillo

219

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Por otra parte, la peculiaridad que dene a los procesos d√©bilmente estacionarios le conere a su funci√≥n
de autocorrelaci√≥n y autocovarianza dos propiedades interesantes: sea X (t) un proceso estacionario (d√©bil).
Entonces, si notamos RX (œÑ ) = E [X (t) X (t + œÑ )] para todo t, su funci√≥n de autocorrelaci√≥n y por CX (œÑ ) a
su funci√≥n de autocovarianza:

1. Ambas son funciones pares, es decir, RX (‚àíœÑ ) = RX (œÑ ) y CX (‚àíœÑ ) = CX (œÑ ).

2. |RX (œÑ )| ‚â§ RX (0) y |CX (œÑ )| ‚â§ CX (0) = œÉ 2 para todo œÑ.

Ejemplo. En el ejemplo del oscilador vimos que la se√±al recibida por un receptor AM de radio es una
se√±al sinusoidal con fase aleatoria, dada por X (t) = A ¬∑ cos (2œÄfc t + Œû) , donde A y fc son constantes y

Œû es una v.a. uniforme en (‚àíœÄ, œÄ) tiene por funci√≥n media
E [X (t)] = 0
y por funci√≥n de autocorrelaci√≥n

RX (t, t + œÑ ) =

A2
cos (2œÄfc œÑ ) .
2

De esta forma, podemos ver que el proceso es d√©bilmente estacionario.

Ejemplo. Un proceso binomial es un proceso con funci√≥n de autocovarianza
C (m, n) = mƒ±ÃÅn (m, n) p (1 ‚àí p) ,
que no depende s√≥lo de m ‚àí n. Por lo tanto no es d√©bilmente estacionario.

Ejemplo.

Vamos a considerar un proceso en tiempo discreto e independiente, Xn , con media cero y

varianza constante e igual a œÉ 2 . Vamos a considerar tambi√©n otro proceso que en cada instante de
tiempo considera la media de X en ese instante y el anterior, es decir,

Yn =

Xn + Xn‚àí1
.
2

En primer lugar, dado que E [Xn ] = 0 para todo n, lo mismo ocurre con Yn , es decir,


E [Yn ] = E

220


Xn + Xn‚àí1
= 0.
2

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Por otra parte,

CY (n, n + m) = RY (n, n + m) ‚àí 0 = E [Y (n) Y (n + m)]


Xn + Xn‚àí1 Xn+m + Xn+m‚àí1
=E
2
2
1
= E [(Xn + Xn‚àí1 ) (Xn+m + Xn+m‚àí1 )]
4
1
= (E [Xn Xn+m ] + E [Xn Xn+m‚àí1 ] + E [Xn‚àí1 Xn+m ] + E [Xn‚àí1 Xn+m‚àí1 ])
4
Ahora debemos tener en cuenta que

CX (n, m) = RX (n, m) =

Ô£±
Ô£≤0

si n 6= m

Ô£≥œÉ 2

si n = m

,

ya que Xn es un proceso independiente. Por lo tanto,

Ô£±

1
Ô£¥
Ô£¥
œÉ2 + 0 + 0 + œÉ2
Ô£¥
4
Ô£¥
Ô£¥
Ô£¥
Ô£≤ 1 0 + œÉ 2 + 0 + 0
CY (n, n + m) = 4

1
Ô£¥
Ô£¥
0 + 0 + œÉ2 + 0
Ô£¥
4
Ô£¥
Ô£¥
Ô£¥
Ô£≥0
Ô£±
1 2
Ô£¥
Ô£¥
Ô£¥ 2 œÉ si m = 0
Ô£≤
= 14 œÉ 2 si m = ¬±1
Ô£¥
Ô£¥
Ô£¥
Ô£≥0
en otro caso

si m = 0
si m = 1
si m = ‚àí1
en otro caso

Podemos decir, por tanto, que el proceso Yn tambi√©n es d√©bilmente estacionario, porque su media es
constante (cero) y CY (n, n + m) no depende de n sino tan s√≥lo de m.

11.3.5. Procesos erg√≥dicos
Si nos damos cuenta, estamos describiendo los procesos aleatorios a partir de promedios estad√≠sticos, principalmente a partir de la media de cada una de sus variables y de sus correlaciones. Vamos a centrarnos en
procesos d√©bilmente estacionarios. En ese caso, los promedios estad√≠sticos m√°s relevantes ser√≠an la media,

ÀÜ

‚àû

E [X (t)] = mX (t) = mX =

xfX(t) (x) dx
‚àí‚àû

y la autocorrelaci√≥n entre dos variables que disten œÑ unidades de tiempo,

ÀÜ

‚àû

RX (œÑ ) = E [X (t) X (t + œÑ )] =

x1 x2 fX(t)X(t+œÑ ) (x1 , x2 ) dx1 dx2 .
‚àí‚àû

Hasta ahora quiz√° no lo hab√≠amos pensado, pero m√°s all√° de los t√≠picos ejemplos, ¬æc√≥mo podr√≠amos tratar de
calcular o estimar al menos estas cantidades? Si aplicamos lo que hemos aprendido hasta ahora, estimar√≠amos,
por ejemplo, la media con la media muestral, pero para ello necesitar√≠amos una muestra muy grande de
Prof. Dr. Antonio Jos√© S√°ez Castillo

221

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

funciones muestrales del proceso, y eso no siempre ocurre. De hecho, no es nada rara la situaci√≥n en la que,
en realidad, s√≥lo es posible observar una √∫nica funci√≥n muestral del proceso.
Ahora bien, dada una √∫nica funci√≥n muestral de un proceso, x (t), en esa funci√≥n hay muchos datos, tantos
como instantes de tiempo t hayamos sido capaces de observar. ¬æNo podr√≠a ocurrir que utiliz√°ramos todos esos
datos que hay en x (t)para estimar las medias y las autocorrelaciones? Por ejemplo, si tenemos observada la
se√±al x (t) en un mont√≥n de valores t1 , ...tn , ¬æqu√© tendr√° que ver

x (t1 ) + ...x (tn )
n
con la media del proceso mX ? De hecho, si n es muy grande y corresponde a un intervalo de observaci√≥n

[‚àíT, T ], tendr√≠amos que
x (t1 ) + ... + x (tn )
1
'
n
2T
Ahora no es una integral sobre los valores de x (integral

ÀÜ

T

x (t) dt.
‚àíT

estad√≠stica )

sino sobre el tiempo.

En el caso de la autocorrelaci√≥n pasar√≠a igual, tendr√≠amos que podr√≠amos observar un mont√≥n de pares de
valores de la se√±al en los instantes t1 , ..., tn y t1 + œÑ, ..., tn + œÑ en el intervalo [‚àíT, T ] y con ellos podr√≠amos
estimar

1
2T

ÀÜ

T

x (t) x (t + œÑ ) dt '
‚àíT

x (t1 ) x (t1 + œÑ ) + ... + x (tn ) x (tn + œÑ )
.
n

Lo que no sabemos, en general, es si esa integral tiene algo que ver con RX (œÑ ), que es una integral estad√≠stica.
Pues bien, se dice que un proceso estacionario es

erg√≥dico cuando las funciones que entra√±an valores espe-

rados a lo largo de las realizaciones (integrales o promedios

estad√≠sticos )

pueden obtenerse tambi√©n a partir

de una sola funci√≥n muestral x (t). Es decir, que una sola realizaci√≥n es representativa de todo el proceso.
M√°s concretamente, un proceso ser√° erg√≥dico en media y en autocorrelaci√≥n si

limT ‚Üí‚àû
y

1
limT ‚Üí‚àû
2T

ÀÜ

1
2T

ÀÜ

T

x (t) dt = mX
‚àíT

T

x (t) x (t + œÑ ) dt = RX (œÑ ) .
‚àíT

11.4. Ejemplos de procesos aleatorios
11.4.1. Ruidos blancos
En telecomunicaciones los ruidos son se√±ales que se adhieren a la se√±al enviada en cualquier proceso de
comunicaci√≥n, de tal manera que uno de los objetivos fundamentales en este tipo de procesos es, dada la
se√±al resultante de sumar la se√±al enviada, X (t), y el ruido del canal, N (t), es decir, dada Y (t) = X (t)+N (t),
saber

ltrar

esta se√±al para estimar cu√°l es el verdadero valor de X (t).

En este apartado nos referimos brevemente a un modelo gastante com√∫n para los fen√≥menos de ruido, llamado
ruido blanco.

222

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Un

ruido blanco es un proceso N (t) centrado, d√©bilmente estacionario e incorrelado con varianza

N0
2 .

Por

tanto, su funci√≥n de autocovarianza (y autocorrelaci√≥n) ser√°

CN (t, t + œÑ ) =

Ô£±
Ô£≤ N0

si œÑ = 0

Ô£≥0

en otro caso

2

.

Utilizando la llamada funci√≥n impulso, dada por

Ô£±
Ô£≤1 si t = 0
Œ¥ (t) =
,
Ô£≥0 en otro caso
esta funci√≥n de autocovarianza puede escribirse como

CN (œÑ ) =

N0
Œ¥ (œÑ ) .
2

La justicaci√≥n de que este sea un modelo habitual para los ruidos, considerando que los valores del ruido
est√°n incorrelados unos con otros, es que suelen ser debidos a fen√≥menos completamente aleatorios y ca√≥ticos,
por lo que no es esperable que exista relaci√≥n entre valores del ruido, ni siquiera cuando √©stos son muy cercanos
en el tiempo.

11.4.2. Procesos gaussianos
Hasta ahora hemos denido y estudiado familias muy gen√©ricas de procesos (independientes, estacionarios,
...). En esta secci√≥n vamos a considerar m√°s concretamente la conocida como familia de procesos aleatorios
gaussianos, que constituye, sin duda, la m√°s importante de entre las que se utilizan en Telecomunicaciones y
en cualquier otro √°mbito de aplicaci√≥n de la Estad√≠stica.
Un p.a. X (t) se dice

proceso gaussiano si cualquier colecci√≥n de variables del proceso tiene distribuci√≥n

conjuntamente gaussiana. Es decir, si cualquier colecci√≥n X (t1 ) , ..., X (tn ) tiene funci√≥n de densidad conjunta

fX(t1 ),...,X(tn ) (x1 , ..., xn ) = p



1
0
‚àí1
exp
‚àí
(x
‚àí
¬µ)
¬∑
C
¬∑
(x
‚àí
¬µ)
,
n
2
(2œÄ) det (C)
1

donde
0

x = (x1 , ..., xn ) ,
0

¬µ = (E [X (t1 )] , ..., E [X (tn )]) ,
C = (Ci,j )i,j=1,..,n ,
Cij = Cov [X (ti ) , X (tj )] .
N√≥tese que un proceso gaussiano est√° completamente descrito una vez que se conocen su funci√≥n media y su
autocovarianza o su autocorrelaci√≥n.
Prof. Dr. Antonio Jos√© S√°ez Castillo

223

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Existen dos razones fundamentales por las que, como hemos comentado, los procesos gaussianos son la familia
de procesos m√°s relevante:
Por una parte, las propiedades anal√≠ticas que verican los hacen f√°cilmente manejables, como veremos
a continuaci√≥n.
Por otra parte, estos procesos han demostrado ser un excelente modelo matem√°tico para gran n√∫mero
de experimentos o fen√≥menos reales (resultado amparado en el Teorema Central del L√≠mite).

Ejemplo. Es muy habitual considerar que los ruidos blancos son gaussianos. En ese caso, si consideramos
ruidos blancos gaussianos, sus variables no s√≥lo son incorreladas, sino que tambi√©n son independientes.

Ejemplo.

Sea un proceso gaussiano X (t) d√©bilmente estacionario con E [X (t)] = 4 y autocorrelaci√≥n

RX (œÑ ) = 25e‚àí3|œÑ | + 16. Obs√©rvese que la autocorrelaci√≥n (y la autocovarianza) decrece r√°pidamente con
el paso del tiempo.
Si deseamos caracterizar la distribuci√≥n de probabilidad de tres v.a. del proceso, observadas en los
instantes t0 , t1 = t0 +

1
2

y t2 = t1 +

1
2

= t0 + 1, necesitamos las medias, E [X (ti )] = 4 y la matriz de

covarianzas, dada a partir de CX (œÑ ) = 25e‚àí3|œÑ | .

Ô£´

25

Ô£¨
CX(t0 ),X(t1 ),X(t2 ) = Ô£≠ 25e‚àí3/2
25e‚àí6/2

25e‚àí3/2
25
25e‚àí3/2

25e‚àí6/2

Ô£∂

Ô£∑
25e‚àí3/2 Ô£∏ .
25

Algunas propiedades de inter√©s de los procesos gaussianos:
Un proceso gaussiano es independiente si y s√≥lo si C (ti , tj ) = 0 para todo i 6= j.
Sea X (t) un proceso gaussiano. Este proceso es markoviano si y s√≥lo si

CX (t1 , t3 ) =

CX (t1 , t2 ) ¬∑ CX (t2 , t3 )
,
CX (t2 , t2 )

para cualesquiera t1 < t2 < t3 .
Un proceso X (t) gaussiano, centrado, con incrementos independientes y estacionarios es de Markov.

11.4.3. Procesos de Poisson
El proceso de Poisson es un modelo para procesos de la vida real que cuentan ocurrencias de un suceso a lo
largo del tiempo, denominados por ello

procesos de recuento.

Algunos de los ejemplos m√°s comunes en el campo de las Telecomunicaciones son el proceso que cuenta el
n√∫mero de llamadas recibidas en una centralita telef√≥nica o el que cuenta el n√∫mero de visitas a una p√°gina
WEB. En otros √°mbitos, como la F√≠sica, estos procesos pueden servir, por ejemplo, para contabilizar el
n√∫mero de part√≠culas emitidas por un cuerpo.

224

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

En todas estas aplicaciones, el proceso tendr√≠a la expresi√≥n

N (t) =

‚àû
X

u (t ‚àí T [n]) ,

n=1

donde T [n] es un proceso en tiempo discreto que representa el momento de la n‚àí√©sima llegada que cuenta
el proceso y

Ô£±
Ô£≤0 si t < t
0
u (t ‚àí t0 ) =
Ô£≥1 si t ‚â• t
0

es la funci√≥n umbral.
El

proceso de Poisson de par√°metro Œª es el proceso N (t) =

P‚àû

n=1

u (t ‚àí T [n]) para el cual la v.a. T [n]

es una suma de n exponenciales independientes del mismo par√°metro Œª, lo que genera una distribuci√≥n de
Erlang de par√°metros n y Œª, con funci√≥n de densidad
n‚àí1

fT [n] (t) =
Alternativamente, puede decirse que

llegadas,

(Œªt)
Œªe‚àíŒªt u (t) .
(n ‚àí 1)!

el proceso de Poisson es aqu√©l en el que los tiempos entre
Œ• [n] = T [n] ‚àí T [n ‚àí 1] ,

siguen siempre distribuciones exponenciales independientesa del mismo par√°metro, esto es
fŒ•[n] (t) = Œªe‚àíŒªt u (t) .

a Obs√©rvese por tanto que el proceso T [n] tiene incrementos independientes.

Ejemplo.

En la Figura 11.6 se muestran funciones muestrales de un proceso de Poisson de par√°metro

Œª = 1. Vamos a interpretar la funci√≥n muestral de la izquierda pensando, por ejemplo, que representa
el n√∫mero de visitas a una p√°gina WEB: se observa que poco depu√©s de los tres minutos se han dado 3
visitas; despu√©s pasan casi 5 minutos sin ninguna visita; a continuaci√≥n se producen un buen n√∫mero de
visitas en poco tiempo; ...
Si observamos tan s√≥lo el eje del tiempo, podr√≠amos se√±alar los instantes en que se producen las llegadas.
Sabemos que esos incrementos en el tiempo desde que se produce una llegada hasta la siguiente siguen
una distribuci√≥n exponencial, en este caso de par√°metro 1.
Vamos a describir algunas de las propiedades m√°s interesantes de los procesos de Poisson:
Sea N (t) un proceso de Poisson de par√°metro Œª. Entonces, para todo t se tiene que N (t) ‚Üí P (Œªt).
La media de un proceso de Poisson de par√°metro Œª es ¬µN (t) = Œªt. Por tanto, el proceso de Poisson no
es estacionario.
Sea N (t) un proceso de Poisson de par√°metro Œª. Entonces, el proceso tiene incrementos independientes
Prof. Dr. Antonio Jos√© S√°ez Castillo

225

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

Figura 11.5: Representaci√≥n gr√°ca de una funci√≥n muestral de un p.a. de Poisson.
y para cualesquiera t1 < t2 , el incremento N (t2 )‚àíN (t1 ) sigue una distribuci√≥n de Poisson de par√°metro

Œª (t2 ‚àí t1 ).
Sea N (t) un proceso de Poisson de par√°metro Œª. Entonces

CN (t1 , t2 ) = Œª mƒ±ÃÅn (t1 , t2 ) .
Sea N (t) un proceso de Poisson de par√°metro Œª. Entonces, para cualesquiera t1 < ... < tk ,

fN (t1 ),...,N (tk ) (n1 , ..., nk )
Ô£±
nk ‚àínk‚àí1
Ô£≤ ‚àíŒ±1 Œ±n1 1 ‚àíŒ±2 Œ±2n2 ‚àín1
‚àíŒ±k Œ±2
e
¬∑
e
¬∑
...
¬∑
e
n1 !
(n2 ‚àín1 )!
(nk ‚àínk‚àí1 )! si n1 ‚â§ ... ‚â§ nk ,
=
Ô£≥
0 en otro caso
donde Œ±i = Œª (ti ‚àí ti‚àí1 ) .
El proceso de Poisson es de Markov.
Sean N1 (t) p.a. de Poisson de par√°metro Œª1 , N2 (t) p.a. de Poisson de par√°metro Œª2 , ambos independientes. Entonces, N1 (t) + N2 (t) es un p.a. de Poisson de par√°metro Œª1 + Œª2 . Esta propiedad se conoce
como

propiedad aditiva.

Sea N (t) un p.a. de Poisson de par√°metro Œª. Supongamos que de todos los eventos que cuenta el
proceso, s√≥lo consideramos una parte de ellos; concretamente los que presentan una caracter√≠stica que
tiene probabilidad p entre todos los eventos. En ese caso, si notamos por Np (t) al proceso que cuenta

226

Prof. Dr. Antonio Jos√© S√°ez Castillo

Apuntes de Estad√≠stica para Ingenieros

Figura 11.6: Funciones muestrales de un proceso de Poisson de par√°metro 1.
los eventos con la caracter√≠stica dada, dicho proceso es de Poisson de par√°metro Œª ¬∑ p. Esta propiedad
se conoce como

propiedad de descomposici√≥n.

El tiempo W que transcurre desde un instante arbitrario t0 hasta la siguiente discontinuidad de un
proceso de Poisson de par√°metro Œª es una variable aleatoria exponencial de par√°metro Œª, independientemente de la elecci√≥n del punto t0 . Esta propiedad aparentemente parad√≥jica se conoce como

propiedad de no memoria del proceso de Poisson. Obs√©rvese que, en realidad, esta propiedad de no
memoria lo es de la distribuci√≥n exponencial.

Ejemplo.

Es frecuente considerar que el proceso que cuenta el n√∫mero de part√≠culas emitidas por un

material radiactivo es un proceso de Poisson. Vamos a suponer por tanto, que estamos observando el
comportamiento de un determinado material del que se conoce que emite a raz√≥n de Œª part√≠culas por
segundo.
Supongamos que se observa el proceso que cuenta el n√∫mero de part√≠culas emitidas desde un instante

t hasta el instante t + T0 . Si en ese intervalo de tiempo se supera un umbral de N0 part√≠culas, deber√≠a
sonar una se√±al de alarma. En ese caso, la probabilidad de que la alarma suene es
‚àû
X

P [N (t + T0 ) ‚àí N (t) > N0 ] =

k=N0 +1

k

e‚àíŒªT0

N

k

0
X
(ŒªT0 )
(ŒªT0 )
=1‚àí
e‚àíŒªT0
,
k!
k!

k=0

ya que N (t + T0 ) ‚àí N (t) ‚Üí P (ŒªT0 ).

Ejemplo.

El n√∫mero de visitas a la p√°gina WEB de una empresa que desea vender sus productos a

trav√©s de INTERNET es adecuadamente descrito mediante un proceso de Poisson. Sabiendo que durante
una hora se reciben un promedio de 5 visitas,

Prof. Dr. Antonio Jos√© S√°ez Castillo

227

Dpto de Estad√≠stica e I.O. Universidad de Ja√©n

1. ¬æcu√°l es la probabilidad de que no se reciba ninguna visita en media hora?
0

P [N (0.5) = 0] = e‚àí5√ó0.5

(5 √ó 0.5)
= 8.2085 √ó 10‚àí2 ,
0!

apenas un 8 % de probabilidad.
2. ¬æCu√°l es el promedio de visitas en 5 horas a la WEB? E [N (5)] = 5 √ó 5 = 25 visitas.
3. La empresa absorbe otra empresa del sector y opta por establecer un enlace directamente desde la
p√°gina de su lial a la propia, garantiz√°ndose que todos los clientes de la lial visitan su p√°gina.
Si el promedio de clientes que visitaban la p√°gina de la lial era de 2 clientes a la hora, ¬æcu√°l es la
probabilidad de que tras la fusi√≥n no se reciba ninguna visita en 10 minutos?
Al hacerse con los clientes de la otra empresa (notemos por M (t) al proceso de Poisson que contaba
sus visitas, de par√°metro Œª = 2 visitas/hora), lo que ha ocurrido es que ahora el n√∫mero de visitas
a la WEB de la empresa es la suma de ambos procesos: T (t) = N (t) + M (t) .
Suponiendo que los procesos de Poisson que contaban las visitas a ambas empresas fueran independientes, se tiene que T (t), en virtud de la propiedad aditiva del proceso de Poisson, es tambi√©n
un proceso de Poisson, de par√°metro Œª = 5 + 2 = 7 visitas/hora. Por tanto,


  

1 0
1
‚àí7√ó 16 7 √ó 6
P T
=0 =e
= 0.3114,
6
0!
una probabilidad del 31 %.

228

Prof. Dr. Antonio Jos√© S√°ez Castillo

Bibliograf√≠a

[Canavos, G. C. (1988)] Canavos, G. C. (1988). Probabilidad y Estad√≠stica. Aplicaciones y M√©todos.
McGraw-Hill.
[DeVore, J. L. (2004)] DeVore, J. L. (2004). Probabilidad y estad√≠stica para ingenier√≠a y ciencias (6¬™ edici√≥n).
Thomson.
[Johnson, R. A. (1997)] Johnson, R. A. (1997). Probabilidad y estad√≠stica para Ingenieros (5¬™ edici√≥n). Prentice Hall.
[Leon-Garcia, A.] Leon-Garcia, A. (1994). Probability and Random Processes for Electrical Engineering (2nd
edition). Addison-Wesley.
[Lipschutz, S. & Schiller, J. (2000)] Lipschutz, S. & Schiller, J. (2000). Introducci√≥n a la Probabilidad y la
Estad√≠stica. McGraw-Hill.
[Mendenhal, W & Sincich, T. (1997)] Mendenhal, W & Sincich, T. (1997). Probabilidad y Estad√≠stica para
Ingenier√≠a y Ciencias (4¬™ edici√≥n). Prentice Hall.
[Montgomery, D. C. & Runger, G. C. (2002)] Montgomery, D. C. & Runger, G. C. (2002). Probabilidad y
estad√≠stica aplicadas a la Ingenier√≠a (2¬™ edici√≥n). Wiley.
[Navidi, W. (2006)] Navidi, W. (2006). Estad√≠stica para ingenieros y cient√≠cos. McGraw-Hill.
[Ross, S. M. (2005)] Ross, S. M. (2005). Introducci√≥n a la Estad√≠stica. Editorial Revert√©.
[Spiegel et al. (2010)] Spiegel, M. R., Schiller, J. y Srinivasan, R. A. (2010). Probabilidad y estad√≠stica (3¬™
edici√≥n), serie Schaum. McGraw-Hill.
[Walpole, R. E

et al

(1998)] Walpole, R. E., Myers, R. H. & Myers, S. L. (1998). Probabilidad y Estad√≠stica

para Ingenieros (6¬™ edici√≥n). Prentice Hall.

229

√çndice alfab√©tico
ANOVA, 168170

Distribuci√≥n normal, 86
Distribuci√≥n normal multivariante, 120, 219

Bonferroni, m√©todo de, 171, 172

Distribuci√≥n t de Student, 130, 158, 161164, 194,
195, 200, 201

Coeciente de asimetr√≠a, 31

Distribuci√≥n uniforme, 82

Coeciente de correlaci√≥n lineal, 112, 195199, 212

Distribuciones condicionadas, 104

Coeciente de variaci√≥n, 30, 37, 38
Contraste de hip√≥tesis, 134, 149152

Error tipo I, 151153, 158, 171

Contraste para el cociente de varianzas, 167

Error tipo II, 152, 158
Contraste para la diferencia de medias, 159, 160, 162 Espacio muestral, 4345, 48, 50, 53, 61, 62, 137
Contraste para la diferencia de proporciones, 166
Estad√≠stico de contraste, 150153, 155, 157, 159, 161,
Contraste para la media, 156, 158
164, 166168, 170, 173, 181, 184, 185, 198
Contraste para la varianza, 167

Estimador puntual, 134, 175, 176

Contraste para proporci√≥n, 164

Funci√≥n de autocorrelaci√≥n, 212, 215

Covarianza, 112

Funci√≥n de autocovarianza, 211, 215

Cuantil, 27, 92, 93

Funci√≥n de densidad, 7578, 8184, 86, 88, 91, 92,

Datos cualitativos, 20

127, 129, 136, 137, 139

Datos cuantitativos, 21, 22, 25, 34

Funci√≥n de densidad conjunta, 99

de cola pesada, 32

Funci√≥n de distribuci√≥n, 7678, 83, 88, 93, 179181

Desviaci√≥n t√≠pica o estandar, 2931, 37, 64, 80, 88, Funci√≥n masa conjunta, 99
128, 129, 145, 157
Funci√≥n masa de probabilidad, 62, 63, 68, 70, 71, 74,
Diagrama de barras, 22, 23, 25, 31
81, 92, 127, 139
Diagrama de cajas y bigotes, 35, 36, 38

Funci√≥n media, 211

Diagrama de sectores, 20, 21

Funci√≥n muestral, 208

Diagramas de barras, 2024

Histograma, 2225, 28, 30, 31, 3437, 7375, 77, 90,

Distribuci√≥n binomial, 65, 66, 69, 87, 91, 138

91, 136, 137

Distribuci√≥n binomial negativa, 71, 72, 139
Distribuci√≥n œá2 , 129
Distribuci√≥n œá2 , 85, 130, 146, 167, 170, 177, 178, 184,
185
Distribuci√≥n de Poisson, 68, 83, 87, 222
Distribuci√≥n exponencial, 8284, 145, 181, 221, 223
Distribuci√≥n F de Snedecor, 130, 131, 170

Incorrelaci√≥n, 112
Independencia de sucesos, 4850, 52, 53, 68, 181
Independencia estad√≠stica, 213, 214
Insesgadez, 134137, 148
Intervalos de conanza, 134, 142148, 200

Distribuci√≥n Gamma, 84, 85, 129, 138, 179, 221

M√©todo de los momentos, 138142, 175, 178, 181

Distribuci√≥n geom√©trica, 70, 71, 139, 178

M√©todo de m√°xima verosimilitud, 139142, 148, 175,
181, 190

Distribuci√≥n marginal, 101
230

Apuntes de Estad√≠stica para Ingenieros

Matriz de correlaciones, 118
Matriz de varianzas-covarianzas, 118
Media, 25, 64, 135, 156

Variable aleatoria, 61, 62, 65, 87, 127129, 138, 139,
142, 150, 189
Variable aleatoria continua, 73, 76, 78

Media muestral, 25, 26, 2831, 34, 64, 81, 87, 128, Variable aleatoria discreta, 6264
Varianza muestral, 28, 29, 64, 81, 129, 135, 136, 144,
129, 135, 144146, 150, 156, 169, 217
Media poblacional, 34, 63, 64, 78, 80, 81, 90, 91, 129,
135, 144147, 150, 156, 192, 199, 202
Mediana, 26, 28, 31, 35
Moda, 26, 31

muestra, 15

156, 162, 167, 169
Varianza poblacional, 63, 64, 78, 80, 81, 129, 134136,
138, 144148, 156, 167, 170, 193, 202, 212
Vector aleatorio, 98
Vector de medias, 118

Muestra aleatoria simple, 20, 29, 33, 36, 37, 63, 65,
74, 183, 196, 197
Nivel de conanza, 142144, 148, 151154, 157, 158,
160, 161, 171, 177, 178, 180, 184, 194, 200
Ortogonalidad, 112
p-valor, 153, 154, 156, 158161, 164, 166168, 171
173, 176181, 183, 185, 194
Percentil, 27, 34, 35, 37, 38, 9294
Probabilidad, 41, 42, 45, 47, 48
Probabilidad condicionada, 4850
Proceso aleatorio, 208
Proceso aleatorio en tiempo continuo, 209
Proceso aleatorio en tiempo discreto, 209
Proceso d√©bilmente estacionario, 215
Proceso de Markov, 215, 220
Proceso de Poisson, 221
Proceso erg√≥dico, 218
Proceso gaussiano, 219
Procesos independientes, 213
Recta de regresi√≥n, 191
Ruido blanco, 219
Tabla de frecuencias, 21
Teorema de Bayes, 5355
Teorema de la probabilidad total, 5355
Test chi2 de bondad de ajuste, 176, 178
Test chi2 de independencia, 181
Test de Kolmogorov-Smirno, 179, 191, 192, 196, 198
202
Valores z , 34, 90
Prof. Dr. Antonio Jos√© S√°ez Castillo

231


